#!/usr/bin/env python3
"""
æ¯æ—¥ TTS è®ºæ–‡å¤„ç†ï¼ˆè§£æ + æŠ“å–æ‘˜è¦ + åˆ†ææ‘˜è¦ + ç”Ÿæˆæ—¥æŠ¥ï¼‰
"""

import re
import time
import json
import requests
from pathlib import Path
from datetime import datetime, timedelta

SCRIPT_DIR = Path(__file__).parent
RAW_DIR = SCRIPT_DIR.parent / "raw" / "tts-arxiv-daily"
PROCESSED_DIR = SCRIPT_DIR.parent / "processed"
ABSTRACT_CACHE = PROCESSED_DIR / "abstracts_cache.json"
BY_DATE_DIR = PROCESSED_DIR / "by-date"
DAILY_SUMMARIES_DIR = SCRIPT_DIR.parent / "summaries" / "daily"
DAILY_SUMMARIES_DIR.mkdir(parents=True, exist_ok=True)

KEYWORDS = {
    "zero-shot": ["zero-shot", "zero shot", "few-shot", "voice cloning", "speaker adaptation"],
    "expressive": ["expressive", "emotional", "emotion", "prosody", "intonation", "style", "affect"],
    "streaming": ["real-time", "streaming", "low-latency", "online", "incremental", "fast"],
    "long-context": ["long-context", "long-form", "long audio", "hour", "60-minute", "extended"],
    "multilingual": ["multilingual", "cross-lingual", "language", "dialect"],
    "codec": ["neural codec", "speech codec", "vocoder", "discrete token", "semantic token", "acoustic token"],
    "llm-based": ["LLM", "large language model", "speech language model", "transformer"],
    "editing": ["editing", "modification", "manipulation"],
    "synthesis": ["text-to-speech", "TTS", "speech synthesis", "voice synthesis"],
}

EXCLUDED = [
    "diarization", "speaker diarization", "multi-speaker", "speaker separation",
    "speaker embedding", "speaker verification", "voice spoofing", "anti-spoofing",
    "ASR", "speech recognition", "voice activity detection"
]

def load_cache():
    if ABSTRACT_CACHE.exists():
        with open(ABSTRACT_CACHE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}

def save_cache(cache):
    with open(ABSTRACT_CACHE, 'w', encoding='utf-8') as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)

def load_analysis_cache():
    analysis_path = PROCESSED_DIR / "analysis_cache.json"
    if analysis_path.exists():
        with open(analysis_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}

def fetch_abstracts_batch(arxiv_ids):
    if not arxiv_ids:
        return {}
    params = {"id_list": ",".join(arxiv_ids), "max_results": len(arxiv_ids)}
    try:
        resp = requests.get("https://export.arxiv.org/api/query", params=params, timeout=30)
        resp.raise_for_status()
        results = {}
        entries = re.findall(r'<entry>(.*?)</entry>', resp.text, re.DOTALL)
        for entry in entries:
            id_match = re.search(r'<id[^>]*>([^<]+)</id>', entry)
            summary_match = re.search(r'<summary[^>]*>(.*?)</summary>', entry, re.DOTALL)
            if id_match and summary_match:
                raw_id = id_match.group(1).rstrip('/').split('/')[-1]
                arxiv_id = re.sub(r'v\d+$', '', raw_id)
                abstract = re.sub(r'\s+', ' ', summary_match.group(1).strip())
                results[arxiv_id] = abstract
        return results
    except Exception as e:
        print(f"æ‰¹é‡è·å–å¤±è´¥: {e}")
        return {}

def parse_markdown_table(file_path: Path):
    try:
        lines = file_path.read_text(encoding='utf-8').splitlines()
    except UnicodeDecodeError:
        lines = file_path.read_text(encoding='gbk', errors='replace').splitlines()
    papers = []
    for line in lines:
        line = line.strip()
        if not line.startswith('|') or 'Publish Date' in line or '---' in line:
            continue
        if not re.search(r'\d{4}-\d{2}-\d{2}', line):
            continue
        cols = [c.strip() for c in line.split('|')]
        if len(cols) < 6:
            continue
        date_str = cols[1].replace('**', '').strip()
        title = cols[2].replace('**', '').strip()
        authors = cols[3].replace('**', '').strip()
        pdf_col = cols[4]
        arxiv_id = None
        arxiv_url = None
        arxiv_match = re.search(r'https?://arxiv\.org/abs/([^\s)\]]+)', pdf_col)
        if arxiv_match:
            arxiv_id = arxiv_match.group(1)
            arxiv_url = f"https://arxiv.org/abs/{arxiv_id}"
        paper = {
            "date": date_str,
            "title": title,
            "authors": authors,
            "arxiv_id": arxiv_id,
            "arxiv_url": arxiv_url,
            "raw": f"{title} {authors}"
        }
        papers.append(paper)
    return papers

def get_tags(paper):
    text = paper['title'] + ' ' + paper['authors'] + ' ' + paper['raw']
    text_lower = text.lower()
    tags = []
    for tag, keywords in KEYWORDS.items():
        for kw in keywords:
            if kw.lower() in text_lower:
                tags.append(tag)
                break
    # æ’é™¤
    for ex in EXCLUDED:
        if ex in text_lower:
            return None
    return tags if tags else None

def generate_date_file(date_str, date_papers):
    date_file = BY_DATE_DIR / f"{date_str}.md"
    with date_file.open('w', encoding='utf-8') as f:
        f.write(f"# TTS Papers - {date_str}\n\n")
        f.write(f"Total: {len(date_papers)}\n\n")
        for p in date_papers:
            f.write(f"## {p['title']}\n")
            f.write(f"- **Authors**: {p['authors']}\n")
            if p['arxiv_id']:
                f.write(f"- **arXiv**: [{p['arxiv_id']}]({p['arxiv_url']})\n")
            f.write(f"- **Tags**: {', '.join(p['tags'])}\n\n")
    print(f"ç”Ÿæˆ {date_file.name}: {len(date_papers)} ç¯‡")

def insert_abstracts_to_file(date_file: Path, cache: dict):
    content = date_file.read_text(encoding='utf-8')
    if "**Abstract**:" in content:
        return 0
    lines = content.splitlines(keepends=True)
    new_lines = []
    inserted = set()
    for i, line in enumerate(lines):
        new_lines.append(line)
        if line.strip().startswith("- **Tags**:"):
            arxiv_id = None
            for j in range(i-1, max(0, i-20), -1):
                m = re.search(r'\[([^\]]+)\]\(https?://arxiv\.org/abs/([^)\]]+)\)', lines[j])
                if m:
                    arxiv_id = re.sub(r'v\d+$', '', m.group(2))
                    break
            if arxiv_id and arxiv_id in cache and arxiv_id not in inserted:
                abstract = cache[arxiv_id]
                new_lines.append(f"- **Abstract**: {abstract}\n")
                inserted.add(arxiv_id)
    date_file.write_text(''.join(new_lines), encoding='utf-8')
    return len(inserted)

def analyze_trends(date_str, papers):
    """åˆ†æè¶‹åŠ¿æ•°æ®"""
    trend_data = {}
    
    # è·å–å‰7å¤©çš„æ•°æ®
    base_date = datetime.strptime(date_str, '%Y-%m-%d')
    recent_dates = []
    for i in range(1, 8):
        recent_date = (base_date - timedelta(days=i)).strftime('%Y-%m-%d')
        recent_dates.append(recent_date)
    
    # æ£€æŸ¥æ¯å¤©çš„è®ºæ–‡æ•°é‡
    paper_counts = {}
    for d in recent_dates + [date_str]:
        date_file = BY_DATE_DIR / f"{d}.md"
        if date_file.exists():
            content = date_file.read_text(encoding='utf-8')
            # è®¡ç®—è®ºæ–‡æ•°é‡ï¼ˆç®€å•æ–¹æ³•ï¼Œå®é™…åº”è¯¥ä½¿ç”¨æ›´ç²¾ç¡®çš„æ–¹æ³•ï¼‰
            paper_counts[d] = content.count("## ")
    
    if len(paper_counts) > 1:
        # è®¡ç®—å¢é•¿ç‡
        current_count = paper_counts.get(date_str, 0)
        previous_count = paper_counts.get(recent_dates[0], 0)
        if previous_count > 0:
            growth_rate = ((current_count - previous_count) / previous_count) * 100
            trend_data['growth_rate'] = f"{growth_rate:.1f}%"
        else:
            trend_data['growth_rate'] = "N/A"
        
        # åˆ†æçƒ­é—¨ä¸»é¢˜
        all_tags = {}
        for d in recent_dates + [date_str]:
            date_file = BY_DATE_DIR / f"{d}.md"
            if date_file.exists():
                content = date_file.read_text(encoding='utf-8')
                # æå–æ ‡ç­¾ï¼ˆç®€åŒ–æ–¹æ³•ï¼Œå®é™…åº”è¯¥ä½¿ç”¨æ›´å¤æ‚çš„æ–¹æ³•ï¼‰
                tags = re.findall(r'`([^`]+)`', content)
                for tag in tags:
                    all_tags[tag] = all_tags.get(tag, 0) + 1
        
        sorted_tags = sorted(all_tags.items(), key=lambda x: x[1], reverse=True)
        hot_topics = [f"{tag} ({count})".strip() for tag, count in sorted_tags[:3]]
        trend_data['hot_topics'] = hot_topics
        
        # æŠ€æœ¯è¶‹åŠ¿
        technology_trends = []
        for d in recent_dates + [date_str]:
            date_file = BY_DATE_DIR / f"{d}.md"
            if date_file.exists():
                content = date_file.read_text(encoding='utf-8')
                # æ£€æŸ¥å…³é”®æŠ€æœ¯è¯
                if "zero-shot" in content.lower():
                    technology_trends.append("zero-shot æŠ€æœ¯çƒ­åº¦æ¥è¿‘")
                if "streaming" in content.lower():
                    technology_trends.append("streaming å®æ—¶å¤„ç†æŠ€æœ¯æ¨è¿›")
                if "llm-based" in content.lower():
                    technology_trends.append("LLM-based æ–¹æ³•æ¶Œç°")
        trend_data['technology_trends'] = technology_trends
        
        # æ½œåœ¨æŒ‘æˆ˜
        challenges = []
        for d in recent_dates + [date_str]:
            date_file = BY_DATE_DIR / f"{d}.md"
            if date_file.exists():
                content = date_file.read_text(encoding='utf-8')
                # æ£€æŸ¥å…³é”®æŒ‘æˆ˜
                if "limitations" in content.lower():
                    challenges.append("æŠ€æœ¯æŒ‘æˆ˜ä»å­˜åœ¨")
                if "evaluation" in content.lower():
                    challenges.append("è¯„ä¼°æ–¹æ³•éœ€è¦ä¼˜åŒ–")
        trend_data['challenges'] = challenges
    
    return trend_data if trend_data else None

def generate_daily_report(date_str, papers):
    """ç”Ÿæˆä¸­æ–‡æ—¥æŠ¥ï¼ˆå¸¦åˆ†ææ‘˜è¦ï¼‰"""
    if not papers:
        return
    total = len(papers)
    topic_counts = {}
    for p in papers:
        for tag in p['tags']:
            topic_counts[tag] = topic_counts.get(tag, 0) + 1
    sorted_topics = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)

    priority_topics = {"zero-shot", "streaming", "llm-based", "long-context", "expressive"}
    highlights = [p for p in papers if any(t in priority_topics for t in p['tags'])]
    highlights.sort(key=lambda x: x['title'])
    
    # æŒ‰è¯„åˆ†æ’åºæ‰€æœ‰è®ºæ–‡
    all_papers_sorted = sorted(papers, key=lambda x: (x.get('analysis') or {}).get('rating', 0), reverse=True)

    lines = []
    lines.append(f"# TTS è®ºæ–‡æ—¥æŠ¥")
    lines.append(f"**æ—¥æœŸ**: {date_str}")
    lines.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    lines.append("")
    lines.append("## æ¦‚è§ˆ")
    lines.append(f"- **è®ºæ–‡æ•°**: {total}")
    lines.append("- **ä¸»é¢˜åˆ†å¸ƒ**:")
    for topic, count in sorted_topics:
        lines.append(f"  - `{topic}`: {count}")
    lines.append("")
    
    # ç»Ÿè®¡ä¿¡æ¯
    lines.append("## ç»Ÿè®¡ä¿¡æ¯")
    lines.append("")
    lines.append(f"- **å¹³å‡è¯„åˆ†**: {sum((p.get('analysis') or {}).get('rating', 0) for p in papers) / max(len(papers), 1):.1f}/10")
    lines.append(f"- **é«˜è¯„åˆ†è®ºæ–‡**: {len([p for p in papers if (p.get('analysis') or {}).get('rating', 0) >= 7])}")
    lines.append(f"- **ä¸­ç­‰è¯„åˆ†è®ºæ–‡**: {len([p for p in papers if 5 <= (p.get('analysis') or {}).get('rating', 0) < 7])}")
    lines.append(f"- **ä½è¯„åˆ†è®ºæ–‡**: {len([p for p in papers if (p.get('analysis') or {}).get('rating', 0) < 5])}")
    lines.append("")
    
    # æŠ€æœ¯å¤æ‚åº¦åˆ†æ
    complexity_stats = {}
    for p in papers:
        complexity = (p.get('analysis') or {}).get('technical_complexity', 'unknown')
        complexity_stats[complexity] = complexity_stats.get(complexity, 0) + 1
    lines.append("## æŠ€æœ¯å¤æ‚åº¦åˆ†å¸ƒ")
    lines.append("")
    for complexity, count in complexity_stats.items():
        lines.append(f"- **{complexity}**: {count}")
    lines.append("")

    lines.append("## é‡ç‚¹è®ºæ–‡ï¼ˆè¯¦ç»†åˆ†æï¼‰")
    lines.append("")
    for p in highlights:
        lines.append(f"### ğŸ“„ {p['title']}")
        lines.append("")
        lines.append(f"- **ä½œè€…**: {p['authors']}")
        lines.append(f"- **arXiv**: [{p['arxiv_id']}]({p['arxiv_url']})")
        lines.append(f"- **æ ‡ç­¾**: {', '.join(p['tags'])}")
        lines.append("")
        
        analysis = p.get('analysis')
        if analysis:
            # è¯¦ç»†åˆ†æéƒ¨åˆ†
            lines.append("#### ğŸ¯ TLDR")
            lines.append(f"{analysis.get('tldr', 'N/A')}")
            lines.append("")
            
            lines.append("#### ğŸ” æ ¸å¿ƒè´¡çŒ®")
            lines.append(f"{analysis.get('core_contribution', 'N/A')}")
            lines.append("")
            
            lines.append("#### ğŸ› ï¸ æŠ€æœ¯æ–¹æ³•")
            lines.append(f"{analysis.get('technical_approach', 'N/A')}")
            lines.append("")
            
            lines.append("#### ğŸ’¡ å…³é”®åˆ›æ–°")
            if analysis.get('key_innovations'):
                for innovation in analysis['key_innovations']:
                    lines.append(f"- {innovation}")
                lines.append("")
            
            lines.append("#### ğŸ“Š å…³é”®å‘ç°")
            lines.append(f"{analysis.get('key_findings', 'N/A')}")
            lines.append("")
            
            lines.append("#### âš™ï¸ æŠ€æœ¯ä¼˜åŠ¿")
            lines.append(f"{analysis.get('technical_strengths', 'N/A')}")
            lines.append("")
            
            lines.append("#### âš ï¸ å±€é™æ€§")
            lines.append(f"{analysis.get('limitations', 'N/A')}")
            lines.append("")
            
            lines.append("#### ğŸš€ æœªæ¥å·¥ä½œ")
            lines.append(f"{analysis.get('future_work', 'N/A')}")
            lines.append("")
            
            lines.append("#### ğŸ“ˆ å®é™…åº”ç”¨")
            lines.append(f"{analysis.get('practical_applications', 'N/A')}")
            lines.append("")
            
            lines.append("#### ğŸ”— ç›¸å…³å·¥ä½œ")
            lines.append(f"{analysis.get('related_work', 'N/A')}")
            lines.append("")
            
            lines.append("#### ğŸ¯ è¯„ä¼°")
            lines.append(f"**è¯„åˆ†**: {analysis.get('rating', 0)}/10")
            lines.append(f"**è¯„ä¼°ç»“æœ**: {analysis.get('evaluation', 'N/A')}")
            lines.append("")
            
            lines.append("#### ğŸ”§ æŠ€æœ¯å¤æ‚åº¦")
            lines.append(f"**å¤æ‚åº¦**: {analysis.get('technical_complexity', 'N/A')}")
            lines.append("")
        else:
            if p.get('abstract'):
                lines.append(f"#### ğŸ“ æ‘˜è¦")
                lines.append(f"{p['abstract']}")
                lines.append("")
        
        lines.append("---")
        lines.append("")

    lines.append("## æ‰€æœ‰è®ºæ–‡åˆ—è¡¨")
    lines.append("")
    for i, p in enumerate(all_papers_sorted, 1):
        analysis = p.get('analysis') or {}
        rating = analysis.get('rating', 0)
        complexity = analysis.get('technical_complexity', 'N/A')
        
        lines.append(f"### {i}. {p['title']}")
        lines.append(f"- **ä½œè€…**: {p['authors']}")
        lines.append(f"- **arXiv**: [{p['arxiv_id']}]({p['arxiv_url']})")
        lines.append(f"- **è¯„åˆ†**: {rating}/10")
        lines.append(f"- **å¤æ‚åº¦**: {complexity}")
        lines.append(f"- **æ ‡ç­¾**: {', '.join(p['tags'])}")
        
        if rating >= 7:  # åªæ˜¾ç¤ºé«˜è¯„åˆ†è®ºæ–‡çš„è¯¦ç»†åˆ†æ
            if analysis.get('tldr'):
                lines.append(f"- **TLDR**: {analysis['tldr']}")
            if analysis.get('key_findings'):
                lines.append(f"- **å…³é”®å‘ç°**: {analysis['key_findings']}")
        
        lines.append("")

    # è¶‹åŠ¿åˆ†æ
    trend_analysis = analyze_trends(date_str, papers)
    if trend_analysis:
        lines.append("## è¶‹åŠ¿åˆ†æ")
        lines.append("")
        lines.append("### ğŸ”¥ çƒ­é—¨ä¸»é¢˜")
        if trend_analysis.get('hot_topics'):
            for topic in trend_analysis['hot_topics']:
                lines.append(f"- {topic}")
        lines.append("")
        
        lines.append("### ğŸ“ˆ æŠ€æœ¯è¶‹åŠ¿")
        if trend_analysis.get('technology_trends'):
            for trend in trend_analysis['technology_trends']:
                lines.append(f"- {trend}")
        lines.append("")
        
        lines.append("### âš¡ å¢é•¿ç‡")
        lines.append(f"- è¾ƒå‰æ—¥å¢é•¿: {trend_analysis.get('growth_rate', 'N/A')}")
        lines.append("")
        
        lines.append("### ğŸ¯ æ½œåœ¨æŒ‘æˆ˜")
        if trend_analysis.get('challenges'):
            for challenge in trend_analysis['challenges']:
                lines.append(f"- {challenge}")
        lines.append("")

    lines.append("---")
    lines.append("")
    lines.append(f"*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*")
    lines.append("*æ•°æ®æ¥æº: analysis_cache.json*")
    lines.append("")
    
    report = "\n".join(lines)
    out_file = DAILY_SUMMARIES_DIR / f"{date_str}.md"
    out_file.write_text(report, encoding='utf-8')
    print(f"æ—¥æŠ¥å·²ç”Ÿæˆ: {out_file}")

def main():
    if len(sys.argv) > 1:
        target_date = sys.argv[1]
    else:
        # é»˜è®¤å¤„ç†æ˜¨å¤©çš„è®ºæ–‡
        target_date = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
    print(f"=== å¤„ç† {target_date} çš„ TTS è®ºæ–‡ ===")

    raw_readme = RAW_DIR / "README.md"
    if not raw_readme.exists():
        print(f"é”™è¯¯: {raw_readme} ä¸å­˜åœ¨")
        sys.exit(1)

    # è§£æ README
    papers = parse_markdown_table(raw_readme)
    print(f"è§£æåˆ° {len(papers)} ç¯‡è®ºæ–‡")
    tagged_papers = []
    for paper in papers:
        tags = get_tags(paper)
        if tags:
            paper['tags'] = tags
            tagged_papers.append(paper)
    print(f"TTS ç›¸å…³: {len(tagged_papers)} ç¯‡")

    # æŒ‰æ—¥æœŸåˆ†ç»„
    papers_by_date = {}
    for p in tagged_papers:
        d = p['date']
        papers_by_date.setdefault(d, []).append(p)

    # å¦‚æœæŒ‡å®šæ—¥æœŸæ²¡æœ‰è®ºæ–‡ï¼Œä½¿ç”¨æœ€æ–°çš„æœ‰è®ºæ–‡çš„æ—¥æœŸ
    if target_date not in papers_by_date:
        if papers_by_date:
            # è·å–æœ€æ–°çš„æ—¥æœŸï¼ˆæŒ‰æ—¥æœŸå­—ç¬¦ä¸²æ’åºï¼Œå–æœ€å¤§çš„ï¼‰
            latest_date = max(papers_by_date.keys())
            print(f"ç›®æ ‡æ—¥æœŸ {target_date} æ²¡æœ‰è®ºæ–‡ï¼Œå°†ä½¿ç”¨æœ€æ–°æ—¥æœŸ {latest_date}")
            target_date = latest_date
        else:
            print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•TTSç›¸å…³è®ºæ–‡")
            return
    date_papers = papers_by_date[target_date]

    # ç”Ÿæˆæ—¥æœŸæ–‡ä»¶
    generate_date_file(target_date, date_papers)

    # åŠ è½½ç¼“å­˜å¹¶å¤„ç†æ‘˜è¦
    cache = load_cache()
    date_file = BY_DATE_DIR / f"{target_date}.md"
    content = date_file.read_text(encoding='utf-8')
    arxiv_ids = set()
    for m in re.finditer(r'\[([^\]]+)\]\(https?://arxiv\.org/abs/([^)\]]+)\)', content):
        arxiv_id = re.sub(r'v\d+$', '', m.group(2))
        arxiv_ids.add(arxiv_id)
    need_fetch = [i for i in arxiv_ids if i not in cache]
    if need_fetch:
        print(f"éœ€è¦æŠ“å– {len(need_fetch)} æ¡æ–°æ‘˜è¦")
        for i in range(0, len(need_fetch), 200):
            batch = need_fetch[i:i+200]
            results = fetch_abstracts_batch(batch)
            cache.update(results)
            save_cache(cache)
            print(f"  æ‰¹æ¬¡ {i//200 + 1}: {len(results)} ç¯‡")
            if i + 200 < len(need_fetch):
                time.sleep(3)
    else:
        print("æ‰€æœ‰æ‘˜è¦å·²ç¼“å­˜")

    # æ’å…¥æ‘˜è¦åˆ°æ–‡ä»¶
    inserted = insert_abstracts_to_file(date_file, cache)
    print(f"æ’å…¥ {inserted} æ¡æ‘˜è¦")

    # ä¸º date_papers æ·»åŠ  abstract å­—æ®µ
    for p in date_papers:
        aid = p.get('arxiv_id')
        p['abstract'] = cache.get(aid, "") if aid else ""

    # åŠ è½½åˆ†æç¼“å­˜å¹¶åˆå¹¶
    analysis_cache = load_analysis_cache()
    for p in date_papers:
        aid = p.get('arxiv_id')
        if aid and aid in analysis_cache:
            p['analysis'] = analysis_cache[aid]
        else:
            p['analysis'] = None

    # ç”Ÿæˆæ—¥æŠ¥
    generate_daily_report(target_date, date_papers)
    print(f"=== {target_date} å¤„ç†å®Œæˆ ===")

if __name__ == "__main__":
    import sys
    from datetime import datetime
    main()
