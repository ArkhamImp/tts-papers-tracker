# TTS 论文月报
**月份**: 2026-01
**生成时间**: 2026-02-03 17:58

## 概览
- **论文总数**: 55
- **主题分布**:
  - `合成`: 22
  - `其他`: 17
  - `表现力`: 12
  - `多语言`: 9
  - `零样本`: 7
  - `LLM 基础`: 4
  - `编解码器`: 3
  - `编辑`: 2
  - `流式`: 2

## 重点论文（分析摘要）
- **EmoShift: Lightweight Activation Steering for Enhanced Emotion-Aware Speech Synthesis** (2026-01-30)
  - **作者**: Li Zhou et.al.
  - **arXiv**: [2601.22873](https://arxiv.org/abs/2601.22873)
  - **标签**: expressive, synthesis
  - **TLDR**: EmoShift is a lightweight TTS framework that uses a small trainable EmoSteer layer to learn emotion-specific steering vectors for precise and controllable emotional speech synthesis.
  - **核心贡献**: Introducing the EmoSteer layer, a parameter-efficient activation-steering module that learns a latent offset vector for each target emotion to precisely control expression without extensive retraining.
  - **方法**: A lightweight framework built on a pre-trained TTS model (likely LLM-based). It adds a small EmoSteer layer that learns a steering vector for each emotion category. This vector is applied to the model's output embeddings to shift the speech generation towards the target emotional style, using only ~10M trainable parameters via efficient fine-tuning.
  - **关键发现**: Outperformed zero-shot and fully fine-tuned baselines in objective (e.g., emotion classification accuracy) and subjective (naturalness, expressiveness, similarity) evaluations. The EmoSteer layer effectively captures emotion-specific characteristics and shows potential for controlling emotional intensity.
  - **评估**: medium (评分: 7/10)

- **Evaluating and Rewarding LALMs for Expressive Role-Play TTS via Mean Continuation Log-Probability** (2026-01-30)
  - **作者**: Yong Ren et.al.
  - **arXiv**: [2601.22661](https://arxiv.org/abs/2601.22661)
  - **标签**: expressive, synthesis
  - **TLDR**: The paper proposes Mean Continuation Log-Probability (MCLP) as a novel metric and reinforcement learning reward to improve the stylistic consistency of Large Audio Language Models in expressive role-play text-to-speech.
  - **核心贡献**: Introducing MCLP, a metric derived from a pre-trained LALM's in-context learning capability, to objectively quantify and optimize speaking style consistency in role-play TTS, addressing a key evaluation bottleneck.
  - **方法**: 1) Formulate MCLP: Use a frozen, pre-trained LALM to compute the average log-probability of the ground-truth speech continuation given the generated speech and role-play context (character/scene). 2) Use MCLP as a reward signal in reinforcement learning (PPO) to fine-tune the LALM-based TTS model for better style alignment. 3) Construct a new RP-TTS dataset with detailed annotations to support training and evaluation.
  - **关键发现**: Models fine-tuned with the MCLP reward significantly outperformed strong LALM baselines (e.g., VoiceCraft, AudioPaLM) in both objective metrics (MCLP, speaker similarity) and subjective human evaluations (naturalness, style consistency, role adherence). The MCLP metric showed high correlation with human judgments.
  - **评估**: strong (评分: 8/10)

- **Erasing Your Voice Before It's Heard: Training-free Speaker Unlearning for Zero-shot Text-to-Speech** (2026-01-28)
  - **作者**: Myungjin Lee et.al.
  - **arXiv**: [2601.20481](https://arxiv.org/abs/2601.20481)
  - **标签**: zero-shot, synthesis

- **ASR for Affective Speech: Investigating Impact of Emotion and Speech Generative Strategy** (2026-01-28)
  - **作者**: Ya-Tse Wu et.al.
  - **arXiv**: [2601.20319](https://arxiv.org/abs/2601.20319)
  - **标签**: expressive

- **T-Mimi: A Transformer-based Mimi Decoder for Real-Time On-Phone TTS** (2026-01-27)
  - **作者**: Haibin Wu et.al.
  - **arXiv**: [2601.20094](https://arxiv.org/abs/2601.20094)
  - **标签**: streaming, llm-based, synthesis

- **Phonological Tokenizer: Prosody-Aware Phonetic Token via Multi-Objective Fine-Tuning with Differentiable K-Means** (2026-01-27)
  - **作者**: Kentaro Onda et.al.
  - **arXiv**: [2601.19781](https://arxiv.org/abs/2601.19781)
  - **标签**: expressive

- **SonoEdit: Null-Space Constrained Knowledge Editing for Pronunciation Correction in LLM-Based TTS** (2026-01-23)
  - **作者**: Ayush Pratap Singh et.al.
  - **arXiv**: [2601.17086](https://arxiv.org/abs/2601.17086)
  - **标签**: llm-based, editing, synthesis

- **Timbre-Aware LLM-based Direct Speech-to-Speech Translation Extendable to Multiple Language Pairs** (2026-01-22)
  - **作者**: Lalaram Arya et.al.
  - **arXiv**: [2601.16023](https://arxiv.org/abs/2601.16023)
  - **标签**: multilingual, llm-based

- **DeepASMR: LLM-Based Zero-Shot ASMR Speech Generation for Anyone of Any Voice** (2026-01-22)
  - **作者**: Leying Zhang et.al.
  - **arXiv**: [2601.15596](https://arxiv.org/abs/2601.15596)
  - **标签**: zero-shot, llm-based

- **Prosody-Guided Harmonic Attention for Phase-Coherent Neural Vocoding in the Complex Spectrum** (2026-01-20)
  - **作者**: Mohammed Salah Al-Radhi et.al.
  - **arXiv**: [2601.14472](https://arxiv.org/abs/2601.14472)
  - **标签**: expressive

- **Lombard Speech Synthesis for Any Voice with Controllable Style Embeddings** (2026-01-19)
  - **作者**: Seymanur Akti et.al.
  - **arXiv**: [2601.12966](https://arxiv.org/abs/2601.12966)
  - **标签**: expressive, synthesis

- **A Unified Neural Codec Language Model for Selective Editable Text to Speech Generation** (2026-01-18)
  - **作者**: Hanchen Pei et.al.
  - **arXiv**: [2601.12480](https://arxiv.org/abs/2601.12480)
  - **标签**: multilingual, codec

- **ParaMETA: Towards Learning Disentangled Paralinguistic Speaking Styles Representations from Speech** (2026-01-18)
  - **作者**: Haowei Lou et.al.
  - **arXiv**: [2601.12289](https://arxiv.org/abs/2601.12289)
  - **标签**: expressive

- **Confidence-based Filtering for Speech Dataset Curation with Generative Speech Enhancement Using Discrete Tokens** (2026-01-18)
  - **作者**: Kazuki Yamauchi et.al.
  - **arXiv**: [2601.12254](https://arxiv.org/abs/2601.12254)
  - **标签**: codec

- **FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning** (2026-01-16)
  - **作者**: Tanyu Chen et.al.
  - **arXiv**: [2601.11141](https://arxiv.org/abs/2601.11141)
  - **标签**: zero-shot, streaming

- **DSA-Tokenizer: Disentangled Semantic-Acoustic Tokenization via Flow Matching-based Hierarchical Fusion** (2026-01-15)
  - **作者**: Hanlin Zhang et.al.
  - **arXiv**: [2601.09239](https://arxiv.org/abs/2601.09239)
  - **标签**: codec

- **SPAM: Style Prompt Adherence Metric for Prompt-based TTS** (2026-01-09)
  - **作者**: Chanhee Cho et.al.
  - **arXiv**: [2601.05554](https://arxiv.org/abs/2601.05554)
  - **标签**: expressive, synthesis

- **CosyEdit: Unlocking End-to-End Speech Editing Capability from Zero-Shot Text-to-Speech Models** (2026-01-08)
  - **作者**: Junyang Chen et.al.
  - **arXiv**: [2601.05329](https://arxiv.org/abs/2601.05329)
  - **标签**: zero-shot, editing, synthesis

- **FlexiVoice: Enabling Flexible Style Control in Zero-Shot TTS with Natural Language Instructions** (2026-01-08)
  - **作者**: Dekun Chen et.al.
  - **arXiv**: [2601.04656](https://arxiv.org/abs/2601.04656)
  - **标签**: zero-shot, expressive, multilingual, synthesis

- **ReStyle-TTS: Relative and Continuous Style Control for Zero-Shot Speech Synthesis** (2026-01-07)
  - **作者**: Haitao Li et.al.
  - **arXiv**: [2601.03632](https://arxiv.org/abs/2601.03632)
  - **标签**: zero-shot, expressive, synthesis

## 完整列表（按日期）
### 2026-01-30
- **EmoShift: Lightweight Activation Steering for Enhanced Emotion-Aware Speech Synthesis**
  - **作者**: Li Zhou et.al.
  - **arXiv**: [2601.22873](https://arxiv.org/abs/2601.22873)
  - **TLDR**: EmoShift is a lightweight TTS framework that uses a small trainable EmoSteer layer to learn emotion-specific steering vectors for precise and controllable emotional speech synthesis.
  - **核心贡献**: Introducing the EmoSteer layer, a parameter-efficient activation-steering module that learns a latent offset vector for each target emotion to precisely control expression without extensive retraining.
  - **关键发现**: Outperformed zero-shot and fully fine-tuned baselines in objective (e.g., emotion classification accuracy) and subjective (naturalness, expressiveness, similarity) evaluations. The EmoSteer layer effectively captures emotion-specific characteristics and shows potential for controlling emotional intensity.

- **Evaluating and Rewarding LALMs for Expressive Role-Play TTS via Mean Continuation Log-Probability**
  - **作者**: Yong Ren et.al.
  - **arXiv**: [2601.22661](https://arxiv.org/abs/2601.22661)
  - **TLDR**: The paper proposes Mean Continuation Log-Probability (MCLP) as a novel metric and reinforcement learning reward to improve the stylistic consistency of Large Audio Language Models in expressive role-play text-to-speech.
  - **核心贡献**: Introducing MCLP, a metric derived from a pre-trained LALM's in-context learning capability, to objectively quantify and optimize speaking style consistency in role-play TTS, addressing a key evaluation bottleneck.
  - **关键发现**: Models fine-tuned with the MCLP reward significantly outperformed strong LALM baselines (e.g., VoiceCraft, AudioPaLM) in both objective metrics (MCLP, speaker similarity) and subjective human evaluations (naturalness, style consistency, role adherence). The MCLP metric showed high correlation with human judgments.

- **Now You Hear Me: Audio Narrative Attacks Against Large Audio-Language Models**
  - **作者**: Ye Yu et.al.
  - **arXiv**: [2601.23255](https://arxiv.org/abs/2601.23255)
  - **TLDR**: The paper demonstrates a highly effective audio jailbreak attack that embeds disallowed instructions within narrative-style synthetic speech to bypass safety mechanisms in large audio-language models.
  - **核心贡献**: Introducing and characterizing a new class of vulnerability—audio narrative attacks—that exploits the modality gap between text and speech safety alignment in large audio-language models.
  - **关键发现**: The attack achieved a 98.26% success rate on Gemini 2.0 Flash, substantially outperforming text-only baselines; it showed high effectiveness across multiple state-of-the-art models, highlighting a critical vulnerability in speech-based interfaces.


### 2026-01-29
- **Speech Quality-Based Localization of Low-Quality Speech and Text-to-Speech Synthesis Artefacts**
  - **作者**: Michael Kuhlmann et.al.
  - **arXiv**: [2601.21886](https://arxiv.org/abs/2601.21886)

- **Unit-Based Agent for Semi-Cascaded Full-Duplex Dialogue Systems**
  - **作者**: Haoyuan Yu et.al.
  - **arXiv**: [2601.20230](https://arxiv.org/abs/2601.20230)


### 2026-01-28
- **ASR for Affective Speech: Investigating Impact of Emotion and Speech Generative Strategy**
  - **作者**: Ya-Tse Wu et.al.
  - **arXiv**: [2601.20319](https://arxiv.org/abs/2601.20319)

- **Audio Deepfake Detection in the Age of Advanced Text-to-Speech models**
  - **作者**: Robin Singh et.al.
  - **arXiv**: [2601.20510](https://arxiv.org/abs/2601.20510)

- **Erasing Your Voice Before It's Heard: Training-free Speaker Unlearning for Zero-shot Text-to-Speech**
  - **作者**: Myungjin Lee et.al.
  - **arXiv**: [2601.20481](https://arxiv.org/abs/2601.20481)


### 2026-01-27
- **Phonological Tokenizer: Prosody-Aware Phonetic Token via Multi-Objective Fine-Tuning with Differentiable K-Means**
  - **作者**: Kentaro Onda et.al.
  - **arXiv**: [2601.19781](https://arxiv.org/abs/2601.19781)

- **Rethinking Discrete Speech Representation Tokens for Accent Generation**
  - **作者**: Jinzuomu Zhong et.al.
  - **arXiv**: [2601.19786](https://arxiv.org/abs/2601.19786)

- **T-Mimi: A Transformer-based Mimi Decoder for Real-Time On-Phone TTS**
  - **作者**: Haibin Wu et.al.
  - **arXiv**: [2601.20094](https://arxiv.org/abs/2601.20094)


### 2026-01-26
- **UrgentMOS: Unified Multi-Metric and Preference Learning for Robust Speech Quality Assessment**
  - **作者**: Wei Wang et.al.
  - **arXiv**: [2601.18438](https://arxiv.org/abs/2601.18438)


### 2026-01-25
- **AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation**
  - **作者**: Dongjie Cheng et.al.
  - **arXiv**: [2601.17761](https://arxiv.org/abs/2601.17761)

- **Quran-MD: A Fine-Grained Multilingual Multimodal Dataset of the Quran**
  - **作者**: Muhammad Umar Salman et.al.
  - **arXiv**: [2601.17880](https://arxiv.org/abs/2601.17880)


### 2026-01-23
- **SonoEdit: Null-Space Constrained Knowledge Editing for Pronunciation Correction in LLM-Based TTS**
  - **作者**: Ayush Pratap Singh et.al.
  - **arXiv**: [2601.17086](https://arxiv.org/abs/2601.17086)


### 2026-01-22
- **DeepASMR: LLM-Based Zero-Shot ASMR Speech Generation for Anyone of Any Voice**
  - **作者**: Leying Zhang et.al.
  - **arXiv**: [2601.15596](https://arxiv.org/abs/2601.15596)

- **Qwen3-TTS Technical Report**
  - **作者**: Hangrui Hu et.al.
  - **arXiv**: [2601.15621](https://arxiv.org/abs/2601.15621)

- **Timbre-Aware LLM-based Direct Speech-to-Speech Translation Extendable to Multiple Language Pairs**
  - **作者**: Lalaram Arya et.al.
  - **arXiv**: [2601.16023](https://arxiv.org/abs/2601.16023)


### 2026-01-20
- **Habibi: Laying the Open-Source Foundation of Unified-Dialectal Arabic Speech Synthesis**
  - **作者**: Yushen Chen et.al.
  - **arXiv**: [2601.13802](https://arxiv.org/abs/2601.13802)

- **HoverAI: An Embodied Aerial Agent for Natural Human-Drone Interaction**
  - **作者**: Yuhua Jin et.al.
  - **arXiv**: [2601.13801](https://arxiv.org/abs/2601.13801)

- **Prosody-Guided Harmonic Attention for Phase-Coherent Neural Vocoding in the Complex Spectrum**
  - **作者**: Mohammed Salah Al-Radhi et.al.
  - **arXiv**: [2601.14472](https://arxiv.org/abs/2601.14472)

- **Synthetic Singers: A Review of Deep-Learning-based Singing Voice Synthesis Approaches**
  - **作者**: Changhao Pan et.al.
  - **arXiv**: [2601.13910](https://arxiv.org/abs/2601.13910)


### 2026-01-19
- **Lombard Speech Synthesis for Any Voice with Controllable Style Embeddings**
  - **作者**: Seymanur Akti et.al.
  - **arXiv**: [2601.12966](https://arxiv.org/abs/2601.12966)


### 2026-01-18
- **A Unified Neural Codec Language Model for Selective Editable Text to Speech Generation**
  - **作者**: Hanchen Pei et.al.
  - **arXiv**: [2601.12480](https://arxiv.org/abs/2601.12480)

- **Confidence-based Filtering for Speech Dataset Curation with Generative Speech Enhancement Using Discrete Tokens**
  - **作者**: Kazuki Yamauchi et.al.
  - **arXiv**: [2601.12254](https://arxiv.org/abs/2601.12254)

- **ParaMETA: Towards Learning Disentangled Paralinguistic Speaking Styles Representations from Speech**
  - **作者**: Haowei Lou et.al.
  - **arXiv**: [2601.12289](https://arxiv.org/abs/2601.12289)


### 2026-01-16
- **F-Actor: Controllable Conversational Behaviour in Full-Duplex Models**
  - **作者**: Maike Züfle et.al.
  - **arXiv**: [2601.11329](https://arxiv.org/abs/2601.11329)

- **FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning**
  - **作者**: Tanyu Chen et.al.
  - **arXiv**: [2601.11141](https://arxiv.org/abs/2601.11141)

- **WenetSpeech-Wu: Datasets, Benchmarks, and Models for a Unified Chinese Wu Dialect Speech Processing Ecosystem**
  - **作者**: Chengyou Wang et.al.
  - **arXiv**: [2601.11027](https://arxiv.org/abs/2601.11027)


### 2026-01-15
- **DSA-Tokenizer: Disentangled Semantic-Acoustic Tokenization via Flow Matching-based Hierarchical Fusion**
  - **作者**: Hanlin Zhang et.al.
  - **arXiv**: [2601.09239](https://arxiv.org/abs/2601.09239)

- **ESDD2: Environment-Aware Speech and Sound Deepfake Detection Challenge Evaluation Plan**
  - **作者**: Xueping Zhang et.al.
  - **arXiv**: [2601.07303](https://arxiv.org/abs/2601.07303)

- **VoiceSculptor: Your Voice, Designed By You**
  - **作者**: Jingbin Hu et.al.
  - **arXiv**: [2601.10629](https://arxiv.org/abs/2601.10629)


### 2026-01-14
- **Afri-MCQA: Multimodal Cultural Question Answering for African Languages**
  - **作者**: Atnafu Lambebo Tonja et.al.
  - **arXiv**: [2601.05699](https://arxiv.org/abs/2601.05699)


### 2026-01-13
- **Decoding Order Matters in Autoregressive Speech Synthesis**
  - **作者**: Minghui Zhao et.al.
  - **arXiv**: [2601.08450](https://arxiv.org/abs/2601.08450)


### 2026-01-12
- **FOCAL: A Novel Benchmarking Technique for Multi-modal Agents**
  - **作者**: Aditya Choudhary et.al.
  - **arXiv**: [2601.07367](https://arxiv.org/abs/2601.07367)


### 2026-01-11
- **Bridging Attribution and Open-Set Detection using Graph-Augmented Instance Learning in Synthetic Speech**
  - **作者**: Mohd Mujtaba Akhtar et.al.
  - **arXiv**: [2601.07064](https://arxiv.org/abs/2601.07064)


### 2026-01-10
- **Lightweight Resolution-Aware Audio Deepfake Detection via Cross-Scale Attention and Consistency Learning**
  - **作者**: K. A. Shahriar et.al.
  - **arXiv**: [2601.06560](https://arxiv.org/abs/2601.06560)


### 2026-01-09
- **IndexTTS 2.5 Technical Report**
  - **作者**: Yunpei Li et.al.
  - **arXiv**: [2601.03888](https://arxiv.org/abs/2601.03888)

- **Pantagruel: Unified Self-Supervised Encoders for French Text and Speech**
  - **作者**: Phuong-Hang Le et.al.
  - **arXiv**: [2601.05911](https://arxiv.org/abs/2601.05911)

- **SPAM: Style Prompt Adherence Metric for Prompt-based TTS**
  - **作者**: Chanhee Cho et.al.
  - **arXiv**: [2601.05554](https://arxiv.org/abs/2601.05554)


### 2026-01-08
- **CosyEdit: Unlocking End-to-End Speech Editing Capability from Zero-Shot Text-to-Speech Models**
  - **作者**: Junyang Chen et.al.
  - **arXiv**: [2601.05329](https://arxiv.org/abs/2601.05329)

- **FlexiVoice: Enabling Flexible Style Control in Zero-Shot TTS with Natural Language Instructions**
  - **作者**: Dekun Chen et.al.
  - **arXiv**: [2601.04656](https://arxiv.org/abs/2601.04656)


### 2026-01-07
- **ReStyle-TTS: Relative and Continuous Style Control for Zero-Shot Speech Synthesis**
  - **作者**: Haitao Li et.al.
  - **arXiv**: [2601.03632](https://arxiv.org/abs/2601.03632)

- **SpeakerSleuth: Evaluating Large Audio-Language Models as Judges for Multi-turn Speaker Consistency**
  - **作者**: Jonggeun Lee et.al.
  - **arXiv**: [2601.04029](https://arxiv.org/abs/2601.04029)


### 2026-01-06
- **Segment-Aware Conditioning for Training-Free Intra-Utterance Emotion and Duration Control in Text-to-Speech**
  - **作者**: Qifan Liang et.al.
  - **arXiv**: [2601.03170](https://arxiv.org/abs/2601.03170)

- **Tigrinya Number Verbalization: Rules, Algorithm, and Implementation**
  - **作者**: Fitsum Gaim et.al.
  - **arXiv**: [2601.03403](https://arxiv.org/abs/2601.03403)

- **Vclip: Face-based Speaker Generation by Face-voice Association Learning**
  - **作者**: Yao Shi et.al.
  - **arXiv**: [2601.02753](https://arxiv.org/abs/2601.02753)

- **Vulnerabilities of Audio-Based Biometric Authentication Systems Against Deepfake Speech Synthesis**
  - **作者**: Mengze Hong et.al.
  - **arXiv**: [2601.02914](https://arxiv.org/abs/2601.02914)

- **XLSR-MamBo: Scaling the Hybrid Mamba-Attention Backbone for Audio Deepfake Detection**
  - **作者**: Kwok-Ho Ng et.al.
  - **arXiv**: [2601.02944](https://arxiv.org/abs/2601.02944)


### 2026-01-05
- **Towards Prosodically Informed Mizo TTS without Explicit Tone Markings**
  - **作者**: Abhijit Mohanta et.al.
  - **arXiv**: [2601.02073](https://arxiv.org/abs/2601.02073)

- **VocalBridge: Latent Diffusion-Bridge Purification for Defeating Perturbation-Based Voiceprint Defenses**
  - **作者**: Maryam Abbasihafshejani et.al.
  - **arXiv**: [2601.02444](https://arxiv.org/abs/2601.02444)


### 2026-01-04
- **MM-Sonate: Multimodal Controllable Audio-Video Generation with Zero-Shot Voice Cloning**
  - **作者**: Chunyu Qiang et.al.
  - **arXiv**: [2601.01568](https://arxiv.org/abs/2601.01568)

- **OV-InstructTTS: Towards Open-Vocabulary Instruct Text-to-Speech**
  - **作者**: Yong Ren et.al.
  - **arXiv**: [2601.01459](https://arxiv.org/abs/2601.01459)


### 2026-01-01
- **DepFlow: Disentangled Speech Generation to Mitigate Semantic Bias in Depression Detection**
  - **作者**: Yuxin Li et.al.
  - **arXiv**: [2601.00303](https://arxiv.org/abs/2601.00303)

- **Latent Flow Matching for Expressive Singing Voice Synthesis**
  - **作者**: Minhyeok Yun et.al.
  - **arXiv**: [2601.00217](https://arxiv.org/abs/2601.00217)

