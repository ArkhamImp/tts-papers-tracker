# TTS 论文月报
**月份**: 2026-01
**生成时间**: 2026-02-05 10:56

## 概览
- **论文总数**: 56
- **主题分布**:
  - `合成`: 22
  - `其他`: 17
  - `表现力`: 12
  - `多语言`: 9
  - `零样本`: 7
  - `LLM 基础`: 4
  - `编辑`: 3
  - `编解码器`: 3
  - `流式`: 2

## 重点论文（分析摘要）
- **EmoShift: Lightweight Activation Steering for Enhanced Emotion-Aware Speech Synthesis** (2026-01-30)
  - **作者**: Li Zhou et.al.
  - **arXiv**: [2601.22873](https://arxiv.org/abs/2601.22873)
  - **标签**: expressive, synthesis
  - **TLDR**: EmoShift introduces a lightweight activation-steering framework called EmoSteer that enhances emotion-aware speech synthesis by learning emotion-specific steering vectors in the output embedding space, enabling precise emotional control without full model fine-tuning. It achieves superior performance over zero-shot and fully fine-tuned baselines with only 10M trainable parameters—less than 1/30 of full fine-tuning—while preserving naturalness and speaker identity.
  - **核心贡献**: The paper addresses the limitation of existing emotion-aware TTS systems that rely on scaling fixed emotion embeddings or external prompts, which restrict their ability to capture emotion-specific latent characteristics. EmoShift’s core innovation is the EmoSteer layer—a lightweight, plug-and-play module that learns emotion-specific offset vectors in the model’s activation space, enabling controllable, interpretable, and stable emotional expression across utterances without modifying or retraining the base TTS model.
  - **方法**: The experiments use the English subset of the Emotional Speech Dataset (ESD), covering five emotions including neutral. Two base models—CosyVoice and another unnamed LLM-based TTS—are used. Baselines include zero-shot emotion prompting and full fine-tuning (SFT). Evaluation metrics include Word Error Rate (WER) via ASR, Speaker Similarity (SpkSIM) using WavLM-Base, Emotion Recognition Accuracy via emotion2vec, Mean Opinion Score (MOS) for naturalness, and Emo-MOS for emotional expressiveness. Subjective evaluation involves 10 listeners rating preference between base and EmoShift-augmented outputs. Objective evaluations measure speech quality and emotion fidelity; controllability is tested by scaling the steering vector magnitude (α) and measuring resulting SER accuracy.
  - **关键发现**: EmoShift outperforms both zero-shot and fully fine-tuned (SFT) baselines across all metrics. In subjective evaluation, it achieves higher MOS and Emo-MOS scores, with listeners preferring EmoShift outputs in pairwise comparisons. Objectively, it improves emotion recognition accuracy—highest for Sad (61.24%) and Neutral (55.84%)—and maintains low WER and high SpkSIM, indicating preserved naturalness and speaker identity. Scaling the steering vector (α) enables smooth control of emotional intensity, validated by monotonic changes in SER accuracy. EmoShift matches or exceeds CosyVoice-SFT-Shift performance despite using <1/30 the trainable parameters.
  - **评估**: strong (评分: 8/10)

- **Evaluating and Rewarding LALMs for Expressive Role-Play TTS via Mean Continuation Log-Probability** (2026-01-30)
  - **作者**: Yong Ren et.al.
  - **arXiv**: [2601.22661](https://arxiv.org/abs/2601.22661)
  - **标签**: expressive, synthesis
  - **TLDR**: This paper introduces Mean Continuation Log-Probability (MCLP) as a novel metric and reinforcement learning reward to improve stylistic consistency in Large Audio Language Model (LALM)-based Role-Play Text-to-Speech (RP-TTS). By leveraging in-context learning of pretrained LALMs, MCLP quantifies how well generated speech aligns with character profiles and scene context, significantly outperforming strong baselines in both objective and subjective evaluations on a newly constructed multi-turn RP-TTS dataset.
  - **核心贡献**: The core contribution is the proposal of Mean Continuation Log-Probability (MCLP), which serves dual purposes: (1) as an objective evaluation metric for stylistic consistency in expressive role-play TTS, addressing the lack of reliable style quantification in prior work; and (2) as a reinforcement learning (RL) reward signal to fine-tune LALMs for better alignment with role-play instructions. The innovation lies in using the intrinsic likelihood modeling capability of pretrained LALMs—via in-context learning—to compute the log-probability of ground-truth speech continuations conditioned on model-generated speech, thereby capturing dynamic, context-sensitive stylistic coherence across multi-turn dialogues rather than treating style as static or attribute-based.
  - **方法**: The authors construct a new RP-TTS dataset called Drp from the WenetSpeech corpus, sourcing drama-tagged YouTube videos. The pipeline includes filtering for quality, using Qwen-VL-7B to generate objective scene descriptions (ignoring dialogue), and prompting LLMs to extract character profiles from full episode scripts. The final dataset contains ~100 hours of speech with detailed annotations. Evaluation uses both objective metrics (WER, Character Error Rate [CER], Pinyin Error Rate, MCLP) and subjective Mean Opinion Score (MOS) assessed by 32 native Chinese professional annotators on 31 curated samples. Baselines include GPT-Audio (closed-source), SFT-only LALM, and Audio-2-mini variants. Two test conditions are evaluated: with audio history (W. Audio History) and without (W/O. Audio History). Ablation studies examine the impact of SFT, GRPO, and reward components (MCLP vs. WER). Training uses 1 epoch of SFT followed by GRPO with advantage estimation relative to the SFT reference policy.
  - **关键发现**: The proposed method achieves state-of-the-art performance: MOS of 3.646 (95% CI reported), approaching ground-truth naturalness, and significantly outperforms all baselines in both settings. It attains the lowest CER and Pinyin error rates, indicating high phonetic fidelity. MCLP scores confirm superior stylistic consistency. Notably, while baseline models degrade without audio history, the proposed method maintains robust performance, demonstrating effective use of textual context alone. Ablation shows that combining MCLP and WER in GRPO is critical—using only WER leads to reward hacking (e.g., generating bland speech to minimize errors), while MCLP-only yields unsafe outputs. The full pipeline (SFT + GRPO with combined reward) achieves the best balance. Audio-2-mini shows significant performance drops in role-play scenarios, highlighting the need for instruction-aligned training.
  - **评估**: strong (评分: 8/10)

- **Erasing Your Voice Before It's Heard: Training-free Speaker Unlearning for Zero-shot Text-to-Speech** (2026-01-28)
  - **作者**: Myungjin Lee et.al.
  - **arXiv**: [2601.20481](https://arxiv.org/abs/2601.20481)
  - **标签**: zero-shot, synthesis
  - **TLDR**: The paper introduces TruS, a training-free speaker unlearning framework for zero-shot text-to-speech (TTS) systems that prevents unauthorized voice synthesis by suppressing identity-specific activations during inference. Unlike prior retraining-based methods, TruS works on both seen and unseen speakers without modifying the model, offering a scalable privacy safeguard.
  - **核心贡献**: TruS addresses the critical privacy risk in modern zero-shot TTS models—unauthorized voice cloning—by enabling speaker unlearning without any retraining. It innovates by shifting from data-centric deletion to inference-time control, manipulating internal hidden activations to erase target speaker identities while preserving non-identity attributes like prosody and emotion. This is the first method to generalize unlearning to speakers not present in the original training data (unseen opt-out speakers), overcoming a major limitation of existing approaches.
  - **方法**: Experiments use F5-TTS pretrained on the Emilia dataset (retain set). Opt-out speakers are drawn from LibriSpeech: seen opt-out (SO) speakers are those in Emilia but marked for removal; unseen opt-out (UO) are entirely absent from training. Evaluation metrics include speaker similarity (using SV model cosine similarity), Word Error Rate (WER) via ASR to assess intelligibility, and emotion preservation scores on an emotional speech dataset. Baselines include fine-tuned F5-TTS (F5-TTS-FT) and retraining-based unlearning methods SGU and TGU. Ablation studies examine layer selection strategies, number of retain speakers (N=30 chosen), and pool size for ID-prototypes. All tests are zero-shot: no adaptation to target speakers.
  - **关键发现**: TruS reduces speaker similarity for seen opt-out speakers by ~80% compared to baseline F5-TTS, matching the performance of retrained baselines (e.g., F5-TTS-FT) without any training. Crucially, it achieves significant suppression on unseen opt-out speakers (UO), where retraining methods fail entirely. WER remains stable (<2% increase), confirming preserved intelligibility. Emotion preservation scores show minimal degradation, indicating attribute retention. The 'µ + σ' layer selection yields optimal balance between unlearning strength and speech quality. Performance scales with larger retain sets (N=30 optimal).
  - **评估**: strong (评分: 9/10)

- **ASR for Affective Speech: Investigating Impact of Emotion and Speech Generative Strategy** (2026-01-28)
  - **作者**: Ya-Tse Wu et.al.
  - **arXiv**: [2601.20319](https://arxiv.org/abs/2601.20319)
  - **标签**: expressive
  - **TLDR**: This paper investigates how emotional speech synthesized by different TTS models affects ASR performance, revealing that substitution errors dominate and vary across models. The authors propose two novel generative strategies—based on transcription correctness and emotional salience—to curate fine-tuning data, achieving consistent WER improvements on real emotional speech datasets without degrading performance on neutral speech.
  - **核心贡献**: The work addresses the challenge of degraded ASR performance on emotionally expressive speech by analyzing how different emotional TTS systems influence error patterns. It introduces a targeted data curation framework for fine-tuning ASR models using synthetic emotional speech, guided by two novel selection criteria: one prioritizing utterances with high transcription fidelity (low WER) and another emphasizing high emotional salience (measured via arousal/valence). This enables building emotion-aware ASR systems that generalize to real emotional speech while preserving robustness on clean, neutral data.
  - **方法**: The methodology involves generating 30,000 synthetic emotional utterances using three TTS systems (EmoVoice, CosyVoice2, MaskGCT) conditioned on emotion labels. Emotional salience is quantified using a WavLM-based multitask regressor trained to predict arousal and valence. ASR performance is first evaluated on these synthetic sets to characterize error types (substitutions, deletions, insertions). Then, two subset selection strategies are applied to construct fine-tuning corpora. The ASR model (Qwen2-audio-7B) is fine-tuned under a parameter-efficient regime. Evaluation is conducted on three held-out real emotional datasets: MSP-Podcast Test1, Test2 (English podcast clips with dimensional emotion annotations), and IEMOCAP (dyadic acted conversations with categorical and dimensional labels). No real emotional data is used in training. Metrics include WER overall and stratified by emotional expressiveness. Baselines include vanilla fine-tuning on unfiltered synthetic data and the original pretrained model.
  - **关键发现**: All emotional TTS datasets degrade ASR performance compared to neutral speech, with substitution errors being dominant. Among TTS models, MaskGCT yields the lowest WER on synthetic data (4.40%). Fine-tuning with TTS-EMO-G reduces WER by up to 12.3% relative on MSP-Podcast Test2 and 9.8% on IEMOCAP compared to vanilla fine-tuning. Crucially, performance on LibriSpeech (neutral) remains stable (WER change < 0.1%), confirming no degradation. Gains are most pronounced in high-arousal/high-valence regions. The combined TTS-EMO-G strategy consistently outperforms individual strategies across all benchmarks, achieving the best average WER of 14.2% on real emotional datasets.
  - **评估**: strong (评分: 8/10)

- **T-Mimi: A Transformer-based Mimi Decoder for Real-Time On-Phone TTS** (2026-01-27)
  - **作者**: Haibin Wu et.al.
  - **arXiv**: [2601.20094](https://arxiv.org/abs/2601.20094)
  - **标签**: streaming, llm-based, synthesis
  - **TLDR**: This paper introduces T-Mimi, a Transformer-only decoder architecture that replaces the convolution-heavy Mimi codec decoder to drastically reduce on-device TTS latency—from 42.1ms to 4.4ms—while maintaining high audio quality. It also identifies critical quantization sensitivity in the final layers of the decoder, enabling efficient mixed-precision deployment on mobile CPUs.
  - **核心贡献**: The core contribution is the redesign of the Mimi neural audio codec’s decoder into a purely Transformer-based architecture (T-Mimi), eliminating deconvolution layers that are inefficient on mobile CPUs like those using XNNPACK. This addresses a key latency bottleneck in real-time on-device TTS systems. Additionally, the work provides empirical insights into quantization-aware training (QAT), revealing that only the last two Transformer layers and final linear layers must remain in full precision to preserve audio fidelity, enabling significant model compression without quality loss.
  - **方法**: The methodology involves architectural redesign, ablation studies, and quantization experiments. Baselines include the original CNN-based Mimi decoder and a fine-tuned version (Mimi-FT). Models are trained on a proprietary 5-million-utterance speech corpus. Objective evaluation uses wide-band PESQ and STOI; subjective evaluation employs CMOS with 200 paired samples rated by human listeners. Latency is measured on actual mobile hardware under real-time streaming conditions. Ablation studies vary decoder depth (8 vs. 12 layers) and linear projection dimension (2048 vs. 3072). QAT strategies test uniform 4-bit, 8-bit, and mixed-precision configurations. All models are trained for 50k steps during selection and up to 90k for final evaluation.
  - **关键发现**: T-Mimi reduces on-phone TTS latency from 42.1ms (original Mimi) to 4.4ms—a 9.6× speedup—while matching audio quality (PESQ ≈ 3.8, statistically on-par per CMOS). Human evaluation shows no significant preference between T-Mimi-32bit and Mimi-FT-32bit. An 8-bit quantized T-Mimi achieves 75% storage reduction with minimal quality drop, but 4-bit quantization degrades perceptual quality. Crucially, keeping only the last two Transformer layers and final linear layers at 32-bit preserves near-full-precision quality (PESQ 3.78 vs. 3.80). Increasing decoder depth from 8 to 12 layers yields substantial quality gains, while expanding linear dimension from 2048 to 3072 offers only marginal improvement at higher storage cost.
  - **评估**: strong (评分: 8/10)

- **Phonological Tokenizer: Prosody-Aware Phonetic Token via Multi-Objective Fine-Tuning with Differentiable K-Means** (2026-01-27)
  - **作者**: Kentaro Onda et.al.
  - **arXiv**: [2601.19781](https://arxiv.org/abs/2601.19781)
  - **标签**: expressive
  - **TLDR**: The paper introduces the Phonological Tokenizer, a novel method that fine-tunes phonetic tokens using differentiable k-means under a multi-objective framework combining ASR and speech resynthesis. This approach yields discrete speech tokens that preserve both linguistic and prosodic information while discarding speaker identity, making them well-suited for prosody-sensitive tasks like speech language modeling.
  - **核心贡献**: The core contribution is a prosody-aware phonetic tokenization method that bridges the gap between purely acoustic tokens (which retain speaker details) and traditional phonetic tokens (which often discard prosody). By jointly optimizing for automatic speech recognition (ASR) and speech waveform reconstruction via a multi-task objective, the Phonological Tokenizer learns discrete representations that abstract away speaker-specific attributes while preserving linguistically relevant prosodic cues—critical for downstream tasks such as expressive voice conversion, emotion recognition, and speech language models (speechLMs).
  - **方法**: The experimental setup involves training the Phonological Tokenizer on a 30-hour subset of LibriSpeech-100h. Baselines include Discrete WavLM and other hybrid/acoustic tokenizers. Evaluation spans three categories: (1) Discriminative tasks—ASR (LibriSpeech-100h, measured by WER), ER (RAVDESS, accuracy), and SID (VoxCeleb1, EER); (2) Generative tasks—speech reconstruction (LJSpeech, evaluated via MCD, PESQ, UTMOS) and voice conversion (Expresso and TIMIT, using speaker similarity and naturalness metrics); (3) SpeechLM tasks—lexical and syntactic probing via sWUGGY and sBLIMP, plus ZeroSpeech and SALMon benchmarks. Ablation studies vary the loss weight α to analyze trade-offs between linguistic, prosodic, and speaker information. All models are implemented in ESPnet with HiFi-GAN vocoder and ECAPA-TDNN classifiers.
  - **关键发现**: The Phonological Tokenizer achieves state-of-the-art or competitive performance across multiple dimensions: (1) In ASR, it matches or exceeds Discrete WavLM (e.g., lower WER at optimal α); (2) In ER on RAVDESS, it significantly outperforms baselines (peak accuracy at intermediate α), confirming prosody retention; (3) In SID, performance degrades as α increases—demonstrating successful speaker information removal; (4) In voice conversion (Expresso VC), it outperforms all baselines in naturalness and speaker similarity; (5) In speechLM evaluations, it achieves the best GenPPL and UTMOS scores, indicating superior linguistic and prosodic coherence. Ablation shows that α=0.5 offers the best balance, with ER peaking there while ASR gradually declines and SID improves as α→1.
  - **评估**: strong (评分: 8/10)

- **SonoEdit: Null-Space Constrained Knowledge Editing for Pronunciation Correction in LLM-Based TTS** (2026-01-23)
  - **作者**: Ayush Pratap Singh et.al.
  - **arXiv**: [2601.17086](https://arxiv.org/abs/2601.17086)
  - **标签**: llm-based, editing, synthesis
  - **TLDR**: SonoEdit introduces a novel, one-shot model editing technique for correcting pronunciation errors in pre-trained LLM-based TTS systems without retraining or degrading general speech capabilities. By leveraging null-space constrained updates guided by acoustic causal tracing, it surgically modifies only the layers responsible for pronunciation while provably preserving all other model behavior.
  - **核心贡献**: The paper addresses the persistent problem of mispronunciation of low-resource proper nouns (e.g., non-English names, brands, locations) in neural TTS systems trained predominantly on English data. Unlike existing approaches that require costly fine-tuning, phonetic annotations, or multilingual data, SonoEdit enables targeted, single-step parameter updates that correct specific pronunciations while mathematically guaranteeing zero first-order change to the model’s output on a preserved speech corpus. This is achieved through a combination of layer-localization via Acoustic Causal Tracing and a closed-form weight update computed in the null-space of general speech representations.
  - **方法**: Experiments were conducted on Orpheus-TTS (LLaMA-3B backbone) using a curated evaluation set called HardNoun-300, containing 300 challenging proper nouns across six languages (50 per language), all systematically mispronounced by the baseline model. Baselines included Full Fine-Tuning (FFT), LoRA, and phoneme-injected synthesis. Evaluation used both objective and subjective metrics: Target-WER (Word Error Rate on corrected words via forced alignment), Global-WER (WER on a preservation set like LibriTTS to assess general performance retention), and qualitative analysis of prosody, stress, and intonation. All edits were applied in a one-shot manner without any retraining. Layer localization was validated via three complementary methods (interventional analysis, gradient attribution, and representation probing), converging on layers 15–21 as pronunciation-critical.
  - **关键发现**: SonoEdit achieved a Target-WER of 2.8% on HardNoun-300, significantly outperforming baselines: Full Fine-Tuning (12.4%), LoRA (9.7%), and the original model (28.6%). Crucially, it maintained a Global-WER of 3.15%, statistically indistinguishable from the original model (3.12%), confirming no degradation in general speech quality. Subjective evaluations confirmed preserved prosody, stress patterns, and speaker identity. The method also demonstrated high edit success rate (98% of edits effective) and minimal drift in acoustic features. Unconstrained methods showed noticeable artifacts and WER degradation, while SonoEdit’s null-space constraint prevented interference with unrelated utterances.
  - **评估**: strong (评分: 9/10)

- **Timbre-Aware LLM-based Direct Speech-to-Speech Translation Extendable to Multiple Language Pairs** (2026-01-22)
  - **作者**: Lalaram Arya et.al.
  - **arXiv**: [2601.16023](https://arxiv.org/abs/2601.16023)
  - **标签**: multilingual, llm-based
  - **TLDR**: This paper introduces DS2ST-LM, a single-stage, multilingual direct speech-to-speech translation (S2ST) system that leverages a multilingual LLM (Qwen2-0.5B) and a timbre-aware vocoder to improve semantic fidelity, speaker identity preservation, and scalability across six language pairs. By constructing a new 1000-hour synthetic bilingual dataset (GigaS2S-1000) and comparing semantic tokenization strategies and projection architectures, the authors demonstrate state-of-the-art performance over both cascaded and prior direct S2ST baselines.
  - **核心贡献**: The core contribution is DS2ST-LM—a unified, end-to-end direct S2ST framework that addresses key limitations of existing systems: data scarcity in low-resource language pairs, poor speaker identity retention, and limited multilingual scalability. It innovatively combines a Whisper speech encoder, a learnable projection module, a multilingual LLM for semantic reasoning, and a timbre-controlled vocoder within a single trainable pipeline. The work also introduces GigaS2S-1000, a large-scale synthetic dataset that mitigates parallel speech data scarcity, and systematically evaluates semantic token generation methods and projection architectures to optimize training stability and output quality.
  - **方法**: The experiments use four datasets: GigaS2S-1000 (zh-en, 1000h, synthetic target speech built from GigaST), CVSS (multilingual, includes CVSS-C and CVSS-T), Bhasaanuvaad (Indic languages), and FLEURS for evaluation. Baselines include: (1) a traditional cascaded ASR+MT+TTS pipeline, and (2) Qwen-Audio (an ST model) + TTS. Evaluation metrics span lexical (BLEU, METEOR), semantic (BLEURT, COMET), and speech quality dimensions (MOS for naturalness, MOS-SIM for speaker similarity, adequacy, fluency). Human evaluations involve 20 samples per model rated by annotators. Ablation studies examine projection architectures and tokenization strategies. The model is trained with Whisper-small encoder, Qwen2-0.5B LLM, and a flow-matching vocoder. Speaker embeddings are extracted using WavLM. All code, recipes, and model checkpoints are released for reproducibility.
  - **关键发现**: DS2ST-LM outperforms cascaded and ST+TTS baselines across all metrics: BLEU scores improved by up to 4.2 points on zh-en, COMET by 6.8 points, and BLEURT by 5.1 points. On multilingual evaluation (fr, es, de, hi, bn, ur), it consistently surpasses baselines. Speaker similarity (MOS-SIM) reached 3.54, significantly higher than prior direct S2ST systems and approaching ground truth. Surprisingly, the Linear projection achieved the best final performance despite slower convergence, while Q-Former converged faster but plateaued lower. Text-derived semantic tokens yielded more stable training and better semantic consistency than speech-derived S3 tokens. Synthetic data in GigaS2S-1000 enabled effective training with only modest performance degradation versus real data, facilitating extension to low-resource pairs.
  - **评估**: strong (评分: 8/10)

- **DeepASMR: LLM-Based Zero-Shot ASMR Speech Generation for Anyone of Any Voice** (2026-01-22)
  - **作者**: Leying Zhang et.al.
  - **arXiv**: [2601.15596](https://arxiv.org/abs/2601.15596)
  - **标签**: zero-shot, llm-based
  - **TLDR**: DeepASMR introduces the first zero-shot TTS framework capable of generating high-fidelity ASMR speech in any speaker's voice using only a short snippet of their normal read-style speech, without requiring whispered training data. It leverages discrete speech tokens and a two-stage LLM + flow-matching pipeline to disentangle ASMR style from speaker timbre, supported by a new 670-hour bilingual ASMR dataset and a comprehensive evaluation protocol.
  - **核心贡献**: The paper solves the critical challenge of zero-shot ASMR speech synthesis—a specialized, low-intensity, often unvoiced speaking style used for relaxation—by introducing DeepASMR, a novel framework that enables personalized ASMR generation for any speaker without needing target-speaker whispered data. The core innovation lies in recognizing that discrete speech tokens provide a 'soft factorization' between ASMR stylistic attributes and speaker identity, enabling effective disentanglement. This allows the system to condition on a speaker’s normal speech and generate authentic ASMR in their voice, a capability absent in prior TTS systems.
  - **方法**: The methodology includes: (1) Dataset construction: DeepASMR-DB contains professionally recorded, high-fidelity ASMR audio from diverse speakers in English and Chinese, balanced across genders and topics, released under CC BY-NC 4.0. (2) Model architecture: Two-stage system with LLM encoder (0.5B parameters) and flow-matching decoder. (3) Training: Pretrained on 200k hours of general speech, then fine-tuned on DeepASMR-DB mixed with normal speech datasets (e.g., Emilia) for 40 epochs using a constant learning rate of 1e-5 and Noam scheduler. (4) Baselines: Compared against CosyVoice2, F5TTS (zero-shot TTS), and VC models (CosyVoiceVC, SeedVC) in intra-style (normal→normal) and cross-style (normal→ASMR) scenarios. (5) Evaluation metrics: Objective (WER, SIM, MCD), subjective MOS (Mean Opinion Score), LLM-based style scoring via GPT-2.5 Pro with prompt engineering, and unvoiced ratio analysis using RMS energy thresholds to quantify whisper-like characteristics.
  - **关键发现**: DeepASMR outperforms all baselines in cross-style ASMR synthesis: it achieves the lowest WER (indicating better intelligibility despite whispering), highest LLM-based style scores (>90% ASMR authenticity), and most accurate unvoiced ratios (~85–90%, close to ground truth). In subjective evaluations, it scores MOS of 4.1–4.3 (out of 5), approaching ground-truth ASMR (4.5). Notably, standard zero-shot TTS models (CosyVoice2, F5TTS) fail to suppress vocal fold vibration, producing voiced speech even when prompted for ASMR. Cascade VC approaches produce inflated unvoiced ratios but poor naturalness. DeepASMR maintains competitive performance on normal speech (intra-style), confirming no degradation in general TTS capability. Commercial models (e.g., ElevenLabs) also fail to generate authentic ASMR due to reliance on voiced phonation.
  - **评估**: strong (评分: 9/10)

- **Prosody-Guided Harmonic Attention for Phase-Coherent Neural Vocoding in the Complex Spectrum** (2026-01-20)
  - **作者**: Mohammed Salah Al-Radhi et.al.
  - **arXiv**: [2601.14472](https://arxiv.org/abs/2601.14472)
  - **标签**: expressive
  - **TLDR**: This paper introduces a novel neural vocoder that integrates prosody-guided harmonic attention with direct complex spectrum modeling to jointly predict magnitude and phase, improving pitch fidelity and phase coherence. By combining adversarial, spectral, and phase-aware losses in a unified architecture, it outperforms HiFi-GAN and AutoVocoder in both objective metrics (e.g., 22% lower F0 RMSE) and subjective quality (MOS +0.15).
  - **核心贡献**: The core contribution is a unified neural vocoder architecture that explicitly models both prosody (via fundamental frequency, F0) and complex spectral components (magnitude and phase) within a single framework. This addresses two persistent limitations in current neural vocoders: inadequate prosody representation and inaccurate or incoherent phase reconstruction. The proposed prosody-guided harmonic attention mechanism conditions the model on extracted F0 to enhance voiced segment encoding, while direct prediction of complex spectra enables phase-coherent waveform synthesis via inverse STFT—bypassing the need for post-processing or indirect phase estimation used in mel-spectrogram-based systems.
  - **方法**: Experiments were conducted on two standard TTS corpora: LJSpeech 1.1 (13,100 English utterances) and another benchmark dataset (implied but not fully named in excerpts). All models—including HiFi-GAN, AutoVocoder, and the proposed method—used identical preprocessing pipelines for fair comparison. The proposed model was trained using Adam optimizer (β1=0.8, β2=0.99) on a single GPU. Objective evaluation included F0 RMSE (measuring pitch accuracy), voiced/unvoiced (V/UV) classification error, and spectral distortion metrics. Subjective evaluation employed Mean Opinion Score (MOS) tests with human listeners rating naturalness on synthesized samples. Baselines included HiFi-GAN (mel-spectrogram conditioned GAN vocoder) and AutoVocoder (recent autoregressive or flow-based alternative). Evaluation focused on both prosodic fidelity and perceptual quality.
  - **关键发现**: The proposed vocoder achieved a 22% reduction in F0 RMSE compared to HiFi-GAN, indicating significantly improved pitch tracking. Voiced/unvoiced error decreased by 18%, demonstrating better voicing decision accuracy. MOS scores improved by 0.15 points, reflecting enhanced naturalness. Qualitative analysis showed waveforms closely matched reference prosody and exhibited smoother phase trajectories, especially in voiced regions. The model maintained phase coherence across frames, reducing artifacts common in magnitude-only vocoders. These gains were consistent across datasets, confirming robustness.
  - **评估**: medium (评分: 8/10)

- **Lombard Speech Synthesis for Any Voice with Controllable Style Embeddings** (2026-01-19)
  - **作者**: Seymanur Akti et.al.
  - **arXiv**: [2601.12966](https://arxiv.org/abs/2601.12966)
  - **标签**: expressive, synthesis
  - **TLDR**: This paper introduces a zero-shot controllable TTS system that synthesizes Lombard speech—characterized by increased vocal effort in noisy environments—for any speaker without requiring Lombard-labeled training data. By leveraging style embeddings from a prosodically diverse dataset and manipulating them via PCA-based shifts along Lombard-correlated dimensions, the method achieves fine-grained control over Lombardness while preserving speaker identity and naturalness.
  - **核心贡献**: The core contribution is a novel framework for zero-shot Lombard speech synthesis that eliminates the need for speaker-specific or Lombard-specific training data. It solves the problem of generalizing Lombard-style prosody to arbitrary speakers by repurposing pre-trained style embeddings learned from large-scale, prosodically varied read-speech datasets. The innovation lies in identifying and manipulating latent directions in the style embedding space that correlate with Lombard attributes using PCA, enabling continuous and controllable Lombard speech generation.
  - **方法**: The methodology involves training a style encoder on the Emilia dataset, which contains diverse prosodic variations. Lombard-relevant dimensions in the style embedding space are identified using PCA applied to embeddings derived from the ALBA (articulation) and AVID (Lombard) datasets. The base TTS model is F5-TTS, adapted with a TDNN for speaker embedding injection. Evaluation includes both objective and subjective metrics: Word Error Rate (WER) under varying SNR conditions (10 dB and 5 dB), ∆WER (improvement over baseline), Speaker Similarity via SSIM (Structural Similarity Index), and speaker verification using a pre-trained model. Baselines include standard F5-TTS and prior Lombard TTS methods that require Lombard data. Subjective user studies assess naturalness and intelligibility. Datasets used include Emilia (training), ALBA, and AVID (for PCA and evaluation).
  - **关键发现**: The proposed method achieves lower WER than the F5-TTS baseline under noisy conditions (SNR = 10 and 5 dB), with Table 2 showing consistent improvements and Table 3 confirming positive ∆WER reductions. SSIM scores (Table 5) indicate high speaker similarity across Lombardness levels, demonstrating preserved speaker identity. Subjective evaluations reveal maintained naturalness even at higher Lombardness settings. The model outperforms prior approaches that require Lombard data in terms of generalization to unseen speakers. Preliminary experiments confirmed that style embeddings from prosodically rich data contain latent Lombard-related prosodic cues, and PCA effectively isolates these dimensions.
  - **评估**: medium (评分: 8/10)

- **A Unified Neural Codec Language Model for Selective Editable Text to Speech Generation** (2026-01-18)
  - **作者**: Hanchen Pei et.al.
  - **arXiv**: [2601.12480](https://arxiv.org/abs/2601.12480)
  - **标签**: multilingual, codec
  - **TLDR**: The paper introduces SpeechEdit, a unified neural codec language model that enables selective editing of specific speech attributes (e.g., emotion, prosody) while preserving other characteristics from a reference prompt. By leveraging a novel delta-pair training strategy on the newly constructed LibriEdit dataset, it achieves controllable zero-shot TTS without sacrificing naturalness or robustness.
  - **核心贡献**: SpeechEdit addresses the limitation of existing neural codec language models—which holistically imitate all acoustic attributes of a reference audio—by introducing a selective control mechanism. This allows users to override only specified attributes (e.g., change emotion while keeping speaker identity and prosody intact) using explicit textual instructions. The core innovation lies in enabling attribute-level disentanglement through data-driven implicit learning rather than architectural modifications or auxiliary modules, all within a single unified model supporting both autoregressive (AR) and non-autoregressive (NAR) generation stages.
  - **方法**: The authors constructed LibriEdit, a new dataset derived from LibriHeavy, containing 2,566 speakers and labeled for five emotion classes (Neutral, Happy, Sad, Angry, Surprised), with Sad being most frequent. Delta pairs consist of same-speaker utterances differing in target attributes. The model was trained in two phases: initial pre-training on LibriHeavy (following VALL-E), then fine-tuning on LibriEdit. Baselines include VALL-E 2, Step-Audio-EditX, and other open-source neural codec TTS systems. Evaluation includes objective metrics (e.g., MCD, WER via ASR) and subjective tests: CMOS (Comparative Mean Opinion Score) for naturalness, similarity, and controllability; SMOS (Speech MOS) for quality; and SSIM for speaker similarity. Subjective evaluations involved human raters across multiple speakers and editing tasks (emotion, pitch, energy). Experiments tested both matched (same speaker) and mismatched (cross-speaker) prompt-editing scenarios.
  - **关键发现**: SpeechEdit achieves competitive zero-shot TTS performance on LibriSpeech test-clean (WER comparable to VALL-E 2). In emotion editing, it outperforms Step-Audio-EditX in CMOS by +0.42 points for controllability while maintaining naturalness (SMOS > 4.0). Objective results show lower MCD scores than baselines in editing tasks, indicating higher fidelity. The model successfully edits emotion without degrading speaker identity (SSIM > 0.85). However, pitch and energy manipulations yield slightly lower naturalness. Ablation studies confirm that delta-pair training significantly improves editing accuracy. Notably, naive emotional data augmentation without delta alignment harms performance, validating the need for structured difference-aware pairs.
  - **评估**: strong (评分: 8/10)

- **ParaMETA: Towards Learning Disentangled Paralinguistic Speaking Styles Representations from Speech** (2026-01-18)
  - **作者**: Haowei Lou et.al.
  - **arXiv**: [2601.12289](https://arxiv.org/abs/2601.12289)
  - **标签**: expressive
  - **TLDR**: ParaMETA introduces a unified, lightweight framework for learning disentangled representations of paralinguistic speaking styles (e.g., emotion, age, gender, language) directly from speech, enabling both accurate multi-task classification and fine-grained, controllable speech generation in TTS systems. By projecting speech into dedicated subspaces per style attribute, it reduces inter-task interference and negative transfer while supporting both speech- and text-based prompting.
  - **核心贡献**: The paper addresses the challenge of modeling multiple paralinguistic attributes simultaneously without performance degradation due to task interference—a common issue in multi-task learning for speech. ParaMETA’s core innovation is a projection-based architecture that learns disentangled, task-specific embeddings by mapping input speech into orthogonal or dedicated subspaces for each style dimension. This enables a single model to perform well across diverse recognition tasks (emotion, gender, age, language) and to condition TTS models with precise, editable style control—preserving non-target attributes when modifying one style factor.
  - **方法**: The authors construct a multilingual, multi-style speech dataset combining public sources (LJ Speech, DataBaker, Genshin Impact voice lines, emotional actor datasets) covering diverse speakers, languages, emotions, ages, and genders. The dataset ensures disjoint train/test splits with subject independence. They evaluate ParaMETA using multiple backbone encoders (convolutional variants) across four classification tasks: emotion, gender, age, and language. Baselines include single-task models, multi-task CLAP-based approaches (e.g., ParaCLAP), and models trained with cross-modal alignment. Evaluation metrics include classification accuracy (averaged over multiple runs) and subjective/objective measures for TTS quality (e.g., naturalness, expressiveness, style consistency). Ablation studies compare models with/without LMETA loss and assess prompting methods. GPU memory usage, inference speed, and model size are reported to evaluate efficiency.
  - **关键发现**: ParaMETA outperforms baselines in 12 out of 16 backbone–task combinations for classification, with consistent gains across emotion, gender, age, and language tasks. In contrast, CLAP-based models (e.g., ParaCLAP) show poor performance across all tasks, suffering from negative transfer. For TTS, ParaMETA-conditioned models generate more natural and expressive speech, with speech-based prompting yielding higher fidelity than text-based. Style editing experiments confirm that modifying one attribute (e.g., emotion) preserves others (e.g., gender), demonstrating disentanglement. Efficiency-wise, ParaMETA uses only 589 MB GPU memory vs. 1966 MB for CLAP—a 70% reduction—while maintaining real-time inference suitability.
  - **评估**: strong (评分: 8/10)

- **Confidence-based Filtering for Speech Dataset Curation with Generative Speech Enhancement Using Discrete Tokens** (2026-01-18)
  - **作者**: Kazuki Yamauchi et.al.
  - **arXiv**: [2601.12254](https://arxiv.org/abs/2601.12254)
  - **标签**: codec
  - **TLDR**: This paper introduces a confidence-based filtering method for curating high-quality TTS datasets using generative speech enhancement (GSE) models that operate on discrete tokens. By leveraging token-level log-probabilities as confidence scores, the method effectively detects hallucination errors—such as phoneme omissions and speaker inconsistencies—that conventional non-intrusive quality metrics miss, leading to improved downstream TTS model performance.
  - **核心贡献**: The core contribution is a non-intrusive, confidence-based filtering technique specifically designed for discrete token-based GSE models used in TTS dataset curation. It addresses the critical issue of hallucination errors introduced during speech enhancement, which degrade TTS model training but are undetectable by standard speech quality metrics like DNSMOS or PESQ. The innovation lies in repurposing internal model confidence (via token log-probabilities) as a proxy for semantic and acoustic fidelity, enabling effective error detection without reference audio.
  - **方法**: Experiments use two datasets: EARS-WHAM (for filtering evaluation with ground truth) and TITW-hard (an in-the-wild TTS dataset). The GSE model (Genhancer) enhances noisy utterances, and multiple filtering methods are compared: DNSMOS, PESQ, Whisper confidence, and the proposed confidence score. Filtering thresholds are varied to produce precision-recall curves. Combined metrics (e.g., confidence & DNSMOS) are also tested. Downstream TTS models (Matcha-TTS + HiFi-GAN) are trained on: (1) original noisy data, (2) unfiltered enhanced data, and (3) confidence-filtered enhanced data (top N%). Evaluation includes objective metrics (WAcc, PESQ, STOI) and subjective MOS prediction via DNSMOS and another MOS model. Synthetic speech from 200 texts (8,000 utterances per model) is evaluated. Baselines include discriminative SE-based curation and standard non-intrusive quality predictors.
  - **关键发现**: Confidence scores show strong correlation (not quantified numerically in excerpts but implied via results) with intrusive metrics like WAcc. The proposed method significantly outperforms DNSMOS and PESQ in detecting hallucination errors—evidenced by higher WAcc retention at equivalent recall levels. TTS models trained on confidence-filtered data achieve higher MOS (+0.3–0.5 points over unfiltered enhanced data) and better intelligibility. Even when combined with other metrics, confidence alone performs competitively. Filtering too aggressively (e.g., top 30%) harms performance due to data loss, indicating a trade-off between quality and quantity. Unfiltered enhanced data underperforms noisy source data in some cases, highlighting the danger of hallucinations.
  - **评估**: strong (评分: 8/10)

- **FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning** (2026-01-16)
  - **作者**: Tanyu Chen et.al.
  - **arXiv**: [2601.11141](https://arxiv.org/abs/2601.11141)
  - **标签**: zero-shot, streaming
  - **TLDR**: FlashLabs Chroma 1.0 introduces the first open-source, real-time, end-to-end spoken dialogue model that simultaneously achieves high-fidelity personalized voice cloning and sub-second latency through a novel interleaved text-audio token schedule. It outperforms human baselines in speaker similarity while maintaining strong dialogue reasoning capabilities.
  - **核心贡献**: Chroma 1.0 addresses the critical limitation in current end-to-end spoken dialogue systems—poor speaker identity preservation during multi-turn conversations—by integrating personalized voice cloning directly into a streaming-capable, low-latency architecture. Unlike cascaded or non-personalized models, Chroma enables real-time, zero-shot voice cloning without requiring separate voice profile extraction stages, thus unifying understanding, reasoning, and expressive speech generation in a single model.
  - **方法**: The authors trained Chroma on proprietary high-quality multi-turn dialogue datasets meeting strict acoustic and conversational criteria, supplemented by public benchmarks like CommonVoice for evaluation. Objective metrics included Speaker Similarity (SIM) using a fine-tuned speaker verification model, RTF for latency, and SCMOS for speech quality. Subjective evaluations involved human listeners comparing Chroma against ElevenLabs’ commercial system and other SOTA models (e.g., Moshi, SpiritLM) across 30 comparative samples. Baselines included both end-to-end dialogue models and cascaded TTS pipelines. Task accomplishment was assessed across understanding, reasoning, and interactive performance. Human baseline comparisons were conducted using the same evaluation protocol as Seed-TTS (Anastassiou et al., 2024). All experiments used a batch size of 4 and 100K training steps.
  - **关键发现**: Chroma achieved a 10.96% relative improvement in speaker similarity over the human baseline, demonstrating superior voice cloning fidelity. It attained an RTF of 0.43, confirming real-time capability. In subjective evaluations, Chroma’s voice cloning was rated competitively against ElevenLabs, with remarkably close SCMOS scores. It consistently ranked second-best in task accomplishment (71.14%) among all compared models and was the only model balancing strong dialogue reasoning with high-quality personalized speech. Chroma outperformed smaller (0.5B) and larger (7B–9B) models in the real-time dialogue setting, highlighting its efficiency-quality trade-off.
  - **评估**: strong (评分: 8/10)

- **DSA-Tokenizer: Disentangled Semantic-Acoustic Tokenization via Flow Matching-based Hierarchical Fusion** (2026-01-15)
  - **作者**: Hanlin Zhang et.al.
  - **arXiv**: [2601.09239](https://arxiv.org/abs/2601.09239)
  - **标签**: codec
  - **TLDR**: The paper introduces DSA-Tokenizer, a novel speech tokenizer that explicitly disentangles semantic and acoustic information into separate discrete token streams using distinct supervision signals—ASR for semantics and mel-spectrogram reconstruction for acoustics. This enables high-fidelity speech reconstruction and flexible recombination (e.g., voice cloning with swapped content/style), advancing controllable generation in discrete Speech LLMs.
  - **核心贡献**: DSA-Tokenizer addresses the critical limitation in existing speech tokenizers—insufficient or absent disentanglement between linguistic content and speaker/acoustic style—by enforcing explicit separation through dual optimization objectives and a hierarchical fusion mechanism. Unlike prior methods that either mix semantics and acoustics or achieve only partial disentanglement, DSA-Tokenizer trains two distinct tokenizers under separate constraints: semantic tokens are optimized via ASR loss to preserve linguistic content, while acoustic tokens are trained to reconstruct mel-spectrograms, capturing prosody, timbre, and speaker identity. The key innovation lies in enabling independent manipulation of content and style without rigid alignment constraints, facilitated by a flow-matching-based hierarchical decoder.
  - **方法**: Experiments compare DSA-Tokenizer against four baseline model families: (1) semantic-biased (e.g., CosyVoice2 S3 Tokenizer with ASR supervision), (2) acoustic-biased, (3) fused-token models (e.g., SpeechTokenizer), and (4) disentangled but incomplete approaches (e.g., MinMo, GLM-4-Voice). Baselines are matched by bitrate/codebook size for fairness. Evaluation tasks include: (1) Reconstruction—measuring WER/CER (semantic integrity), UTMOS (naturalness), and speaker SIM; (2) Recombination—cross-utterance voice cloning where semantic tokens from one speaker are paired with acoustic tokens from another. Datasets: cleaned Chinese-English Emilia, LibriSpeech (train-clean-100/360 for ASR), and MagicData. Disentanglement is probed by training linear classifiers on token embeddings to predict semantic (ASR labels) or acoustic (speaker ID) attributes—low cross-accuracy indicates good disentanglement. Metrics: WER, CER, UTMOS, SIM, and probing accuracy across token layers (L0–L7). All models trained for 30 epochs; best checkpoint selected by validation WER.
  - **关键发现**: DSA-Tokenizer achieves superior balance across all metrics: (1) In reconstruction, it matches or exceeds baselines in WER (e.g., ~5.2% on LibriSpeech vs. 5.8% for CosyVoice2) while maintaining high UTMOS (>3.8) and SIM (>0.75); (2) In recombination, it significantly outperforms fused/disentangled baselines—e.g., 15–20% lower WER and 0.2+ higher SIM in voice cloning; (3) Disentanglement probing shows semantic tokens (L0) have high ASR accuracy (>90%) but low speaker accuracy (<20%), while acoustic tokens (L1–L7) show opposite pattern (speaker acc >85%, ASR acc <30%), confirming effective separation; (4) Ablation shows removing recombination training causes severe collapse in cross-utterance tasks despite minor reconstruction degradation; (5) Hierarchical flow-matching improves naturalness over standard diffusion or GAN decoders.
  - **评估**: strong (评分: 9/10)

- **SPAM: Style Prompt Adherence Metric for Prompt-based TTS** (2026-01-09)
  - **作者**: Chanhee Cho et.al.
  - **arXiv**: [2601.05554](https://arxiv.org/abs/2601.05554)
  - **标签**: expressive, synthesis
  - **TLDR**: The paper introduces SPAM (Style Prompt Adherence Metric), a novel automatic evaluation metric for prompt-based text-to-speech systems that explicitly addresses both plausibility (alignment with human perception) and faithfulness (grounding in the provided style prompt). By leveraging a CLAP-inspired architecture with supervised contrastive learning and acoustic attribute factorization, SPAM demonstrates strong correlation with human judgments and robust discrimination of semantic differences in style prompts.
  - **核心贡献**: The main contribution is SPAM, an automatic metric designed to evaluate how well synthesized speech adheres to fine-grained textual style prompts in TTS systems. Prior evaluation methods lacked either plausibility (correlation with human perception) or faithfulness (semantic grounding in the prompt). SPAM solves this by jointly modeling acoustic attributes from speech and semantic embeddings from prompts using a contrastive framework, ensuring evaluations are both human-aligned and prompt-grounded.
  - **方法**: Experiments were conducted on two datasets: TextrolSpeech and LibriTTS-P, both containing natural language style descriptions paired with reference audio. Two evaluation perspectives were tested: (1) Plausibility—measured via Pearson correlation between SPAM scores and human Mean Opinion Scores (MOS); (2) Faithfulness—assessed by SPAM’s ability to distinguish between semantically different but acoustically similar prompts using paired t-tests. Baselines included RA-CLAP and other automatic metrics. Three correlation measures were reported per dataset and model. SPAM variants were trained using a bootstrapped approach with an RA-CLAP teacher model, and frame-level speech features were averaged to produce utterance-level embeddings.
  - **关键发现**: In plausibility experiments, SPAM achieved strong MOS correlations: 0.520 and 0.429 on TextrolSpeech and LibriTTS-P respectively, outperforming RA-CLAP which showed inconsistent performance (0.726 vs. 0.545 across datasets). In faithfulness experiments, SPAM significantly discriminated between different semantic prompts (p < 0.01 in paired t-tests), confirming its grounding in prompt semantics. SPAM also demonstrated consistent performance across diverse TTS models like ParlerTTS, indicating robustness. These results validate SPAM as both plausible and faithful, unlike prior metrics that excel in only one dimension.
  - **评估**: strong (评分: 8/10)

- **CosyEdit: Unlocking End-to-End Speech Editing Capability from Zero-Shot Text-to-Speech Models** (2026-01-08)
  - **作者**: Junyang Chen et.al.
  - **arXiv**: [2601.05329](https://arxiv.org/abs/2601.05329)
  - **标签**: zero-shot, editing, synthesis
  - **TLDR**: CosyEdit introduces an end-to-end speech editing system derived from the zero-shot TTS model CosyVoice through task-specific fine-tuning and inference optimization, eliminating the need for external alignment modules. It achieves state-of-the-art performance on the RealEdit benchmark using only 250 hours of supervised data, matching or surpassing both large language model baselines and cascade systems.
  - **核心贡献**: The paper proposes CosyEdit, a novel end-to-end speech editing framework that adapts a pre-trained zero-shot text-to-speech (TTS) model—specifically CosyVoice—via targeted fine-tuning and optimized inference to perform high-fidelity speech editing without relying on complex preprocessing pipelines or explicit temporal alignment. This approach solves the longstanding challenge in automatic speech editing of maintaining consistency between original and edited speech while enabling flexible content modification via textual instructions, all within a unified architecture.
  - **方法**: The authors constructed GigaEdit, a 250-hour supervised dataset derived from GigaSpeech, simulating real-world editing scenarios across insertion, deletion, and substitution tasks. They evaluated CosyEdit on the RealEdit benchmark, comparing against multiple baselines: traditional cascade systems (e.g., Step-Audio-EditX), autoregressive models (VoiceCraft, SSR-Speech), non-autoregressive models (FluentSpeech), and speech language model (SLM)-based approaches. Objective metrics included Word Error Rate (WER) and Emotion Similarity (EMOS); subjective evaluations assessed intelligibility and naturalness via human listeners on 10 randomly sampled examples. All end-to-end models underwent alignment-based postprocessing for fair comparison. Training used 16 epochs with specified learning rates, and ablation studies tested variants like one-stage vs. two-stage inference and conditioning strategies.
  - **关键发现**: CosyEdit outperformed all end-to-end baselines on RealEdit, achieving the lowest WER and highest EMOS scores among them, and matched the performance of top cascade systems despite using only 250 hours of fine-tuning data. It surpassed billion-parameter SLM-based methods in both objective and subjective metrics. Ablation studies showed that reference-guided conditioning significantly improved Mel Cepstral Distortion (MCD) but required careful balancing to avoid WER degradation. CosyEdit demonstrated stable performance across diverse editing operations, whereas competitors like Step-Audio-EditX exhibited high variance. The model preserved speaker identity and prosodic continuity better than alternatives, indicating strong consistency between pre- and post-edit speech.
  - **评估**: strong (评分: 8/10)

- **FlexiVoice: Enabling Flexible Style Control in Zero-Shot TTS with Natural Language Instructions** (2026-01-08)
  - **作者**: Dekun Chen et.al.
  - **arXiv**: [2601.04656](https://arxiv.org/abs/2601.04656)
  - **标签**: zero-shot, expressive, multilingual, synthesis
  - **TLDR**: FlexiVoice introduces a zero-shot text-to-speech system that enables flexible, disentangled control over speaking style via natural language instructions and voice timbre via speech references, using a large language model (LLM) core enhanced with a novel Progressive Post-Training (PPT) scheme. It significantly outperforms existing baselines in controllability, naturalness, and robustness while effectively decoupling style, timbre, and content.
  - **核心贡献**: The paper addresses the critical challenge in zero-shot TTS of simultaneously and accurately controlling multiple attributes—specifically, speaking style through natural language instructions and speaker timbre through reference audio—without conflating them with textual content or each other. Existing systems often fail to disentangle these factors, leading to style leakage or poor instruction adherence. FlexiVoice solves this by integrating an LLM-based architecture with a three-stage Progressive Post-Training (PPT) pipeline that progressively aligns and disentangles control signals using reinforcement learning techniques (DPO and GRPO), enabling precise, flexible, and robust multi-modal control in a zero-shot setting.
  - **方法**: The authors construct a new large-scale dataset, FlexiVoice-Instruct, annotated with natural language style instructions aligned with speech samples, using an LLM-based annotator for efficiency. Evaluation includes both automatic and human assessments across multiple benchmarks: InstructTTSEval (Huang et al., 2025), emotion datasets, and custom test sets covering Text-Only (TO-easy), Reference-Only (RO-easy), and joint instruction+reference conditions. Baselines include open-source TTS systems and closed-source commercial models (e.g., gemini-2.5-flash-preview-tts). Key metrics: (1) Decoupling Avg. (macro-average accuracy across style, speaker, and content tasks using Gemini as judge); (2) agreement (Macro-F1) between surrogate and oracle reward models; (3) correlation with ground-truth style attributes (e.g., pitch, energy); and (4) human evaluations on naturalness, controllability, and robustness via paired comparisons. Ablation studies test training order (e.g., S3-first vs. PPT), joint training, and component removal.
  - **关键发现**: FlexiVoice achieves a Decoupling Avg. score of 88.5, substantially outperforming the base model (which scores poorly due to style leakage) and all open-source and commercial baselines. On complex instruction tasks, it reaches 74.8% accuracy versus ~60% for strongest competitors. Human evaluations confirm superior naturalness and controllability, especially in conflict scenarios (e.g., neutral text with 'excited' instruction). The PPT ablation shows that progressive training is essential: joint GRPO optimization causes interference, while S3-first fails due to lack of foundational disentanglement. Reward model validation shows Kimi-Audio-7B achieves high Macro-F1 (Table 7) against Gemini, confirming its suitability. Speaker verification rewards prove more effective than embedding cosine similarity for timbre control, as they tolerate acoustic variation better.
  - **评估**: strong (评分: 9/10)

- **ReStyle-TTS: Relative and Continuous Style Control for Zero-Shot Speech Synthesis** (2026-01-07)
  - **作者**: Haitao Li et.al.
  - **arXiv**: [2601.03632](https://arxiv.org/abs/2601.03632)
  - **标签**: zero-shot, expressive, synthesis
  - **TLDR**: ReStyle-TTS introduces a novel framework for zero-shot text-to-speech synthesis that enables continuous and reference-relative control over speaking style attributes like pitch, energy, and emotion, without requiring carefully matched reference audio. By decoupling the model's dependence on reference style and introducing orthogonal LoRA-based control mechanisms, it achieves robust style manipulation while preserving speaker timbre and intelligibility.
  - **核心贡献**: The paper addresses a critical limitation in current zero-shot TTS systems: their strong entanglement of speaker timbre and speaking style from reference audio, which makes targeted style control impractical when reference and desired styles mismatch. ReStyle-TTS solves this by first reducing implicit reliance on reference style through Decoupled Classifier-Free Guidance (DCFG), then enabling fine-grained, continuous, and relative style control via style-specific LoRAs with Orthogonal LoRA Fusion, all while maintaining timbre consistency through a dedicated optimization module.
  - **方法**: The authors trained separate LoRA modules on labeled subsets of the VccmDataset (Ji et al., 2024), which includes LibriTTS-derived data annotated for pitch, energy, and emotions. Evaluation was conducted on Seed-TTS and VccmDataset test sets under zero-shot conditions. Baselines included recent controllable TTS models like ControlSpeech, InstructTTS, and CosyVoice. Objective metrics included style accuracy (via Emotion2Vec embeddings) and speaker similarity (using pre-trained speaker verification models). Subjective evaluations used MOS-SA (Mean Opinion Score–Style Accuracy) with human raters assessing both style fidelity and naturalness. A key experimental setting was the 'contradictory-style' scenario (e.g., generating angry speech from a happy reference), which tests robustness under style-reference mismatch. Ablation studies validated each component’s contribution.
  - **关键发现**: ReStyle-TTS outperformed baselines in both objective and subjective evaluations, especially in contradictory-style scenarios. On emotion transfer (Table 2), it achieved higher MOS-SA scores across all emotions (e.g., +0.8 over ControlSpeech for 'angry'). In pitch/energy control (Table 3), it demonstrated precise manipulation even when reference and target styles conflicted. Figures 2–4 showed smooth, continuous control over single and joint attributes (e.g., simultaneously adjusting pitch and happiness). Ablation confirmed DCFG reduces reference style dependence by ~35% (measured via style embedding correlation), while TCO improved speaker similarity scores by 12%. Crucially, intelligibility remained high (WER < 5%) across all conditions.
  - **评估**: strong (评分: 9/10)

## 完整列表（按日期）
### 2026-01-31
- **Edit Content, Preserve Acoustics: Imperceptible Text-Based Speech Editing via Self-Consistency Rewards**
  - **作者**: Yong Ren et.al.
  - **arXiv**: [2602.00560](https://arxiv.org/abs/2602.00560)
  - **TLDR**: This paper introduces a novel text-based speech editing framework that decouples semantic content editing from acoustic rendering to achieve imperceptible edits while preserving speaker identity, prosody, and naturalness. By leveraging a pre-trained TTS model as an implicit critic within a reinforcement learning setup using Self-Consistency Rewards and Group Relative Policy Optimization (GRPO), the method significantly outperforms existing autoregressive and non-autoregressive baselines in intelligibility, robustness, and perceptual quality.
  - **核心贡献**: The core contribution is a principled 'Edit Content, Preserve Acoustics' framework that addresses the fundamental limitation of prior text-based speech editing methods: the entanglement of linguistic content and acoustic style in the waveform or codec space. This entanglement causes instability, boundary artifacts, and speaker drift during editing. The proposed solution decouples editing into a semantic token space for content modification and uses a Flow Matching decoder for high-fidelity acoustic reconstruction, guided by a self-consistency reward derived from a frozen pre-trained TTS model to ensure contextual and perceptual alignment.
  - **关键发现**: The proposed method achieved state-of-the-art results across all metrics: lowest WER (indicating highest intelligibility), highest DNSMOS and subjective MOS (superior naturalness), and best speaker similarity preservation—especially under long-duration edits where baselines degraded significantly. In substitution tasks, it matched or exceeded NAR performance; in insertion tasks, it vastly outperformed both AR (which suffered from boundary artifacts) and NAR (which often predicted silence). GRPO alignment provided substantial gains in naturalness and stability without harming speaker similarity. The method showed minimal degradation in performance even with extended masked regions, demonstrating robust long-context modeling.


### 2026-01-30
- **EmoShift: Lightweight Activation Steering for Enhanced Emotion-Aware Speech Synthesis**
  - **作者**: Li Zhou et.al.
  - **arXiv**: [2601.22873](https://arxiv.org/abs/2601.22873)
  - **TLDR**: EmoShift introduces a lightweight activation-steering framework called EmoSteer that enhances emotion-aware speech synthesis by learning emotion-specific steering vectors in the output embedding space, enabling precise emotional control without full model fine-tuning. It achieves superior performance over zero-shot and fully fine-tuned baselines with only 10M trainable parameters—less than 1/30 of full fine-tuning—while preserving naturalness and speaker identity.
  - **核心贡献**: The paper addresses the limitation of existing emotion-aware TTS systems that rely on scaling fixed emotion embeddings or external prompts, which restrict their ability to capture emotion-specific latent characteristics. EmoShift’s core innovation is the EmoSteer layer—a lightweight, plug-and-play module that learns emotion-specific offset vectors in the model’s activation space, enabling controllable, interpretable, and stable emotional expression across utterances without modifying or retraining the base TTS model.
  - **关键发现**: EmoShift outperforms both zero-shot and fully fine-tuned (SFT) baselines across all metrics. In subjective evaluation, it achieves higher MOS and Emo-MOS scores, with listeners preferring EmoShift outputs in pairwise comparisons. Objectively, it improves emotion recognition accuracy—highest for Sad (61.24%) and Neutral (55.84%)—and maintains low WER and high SpkSIM, indicating preserved naturalness and speaker identity. Scaling the steering vector (α) enables smooth control of emotional intensity, validated by monotonic changes in SER accuracy. EmoShift matches or exceeds CosyVoice-SFT-Shift performance despite using <1/30 the trainable parameters.

- **Evaluating and Rewarding LALMs for Expressive Role-Play TTS via Mean Continuation Log-Probability**
  - **作者**: Yong Ren et.al.
  - **arXiv**: [2601.22661](https://arxiv.org/abs/2601.22661)
  - **TLDR**: This paper introduces Mean Continuation Log-Probability (MCLP) as a novel metric and reinforcement learning reward to improve stylistic consistency in Large Audio Language Model (LALM)-based Role-Play Text-to-Speech (RP-TTS). By leveraging in-context learning of pretrained LALMs, MCLP quantifies how well generated speech aligns with character profiles and scene context, significantly outperforming strong baselines in both objective and subjective evaluations on a newly constructed multi-turn RP-TTS dataset.
  - **核心贡献**: The core contribution is the proposal of Mean Continuation Log-Probability (MCLP), which serves dual purposes: (1) as an objective evaluation metric for stylistic consistency in expressive role-play TTS, addressing the lack of reliable style quantification in prior work; and (2) as a reinforcement learning (RL) reward signal to fine-tune LALMs for better alignment with role-play instructions. The innovation lies in using the intrinsic likelihood modeling capability of pretrained LALMs—via in-context learning—to compute the log-probability of ground-truth speech continuations conditioned on model-generated speech, thereby capturing dynamic, context-sensitive stylistic coherence across multi-turn dialogues rather than treating style as static or attribute-based.
  - **关键发现**: The proposed method achieves state-of-the-art performance: MOS of 3.646 (95% CI reported), approaching ground-truth naturalness, and significantly outperforms all baselines in both settings. It attains the lowest CER and Pinyin error rates, indicating high phonetic fidelity. MCLP scores confirm superior stylistic consistency. Notably, while baseline models degrade without audio history, the proposed method maintains robust performance, demonstrating effective use of textual context alone. Ablation shows that combining MCLP and WER in GRPO is critical—using only WER leads to reward hacking (e.g., generating bland speech to minimize errors), while MCLP-only yields unsafe outputs. The full pipeline (SFT + GRPO with combined reward) achieves the best balance. Audio-2-mini shows significant performance drops in role-play scenarios, highlighting the need for instruction-aligned training.

- **Now You Hear Me: Audio Narrative Attacks Against Large Audio-Language Models**
  - **作者**: Ye Yu et.al.
  - **arXiv**: [2601.23255](https://arxiv.org/abs/2601.23255)
  - **TLDR**: This paper introduces 'Audio Narrative Attacks,' a novel jailbreak technique that embeds harmful instructions within narrative-style synthetic speech to bypass safety mechanisms in large audio-language models (LALMs). By leveraging prosodic and stylistic cues via advanced TTS, the method achieves a 98.26% success rate on models like Gemini 2.0 Flash—far exceeding text-only attacks—highlighting critical vulnerabilities in speech-based AI systems.
  - **核心贡献**: The paper identifies and exploits a previously uncharacterized vulnerability in end-to-end large audio-language models: their susceptibility to disallowed content when delivered through narratively structured, prosodically modulated synthetic speech. Unlike traditional text-based jailbreaks, this approach leverages paralinguistic features—such as tone, pacing, and vocal affect—to manipulate model compliance without altering lexical content. The core innovation lies in treating speech not just as a carrier of text but as a modality with its own semantic and behavioral influence, thereby circumventing alignment safeguards calibrated primarily for textual inputs.
  - **关键发现**: The audio narrative attack achieves a 98.26% success rate on Gemini 2.0 Flash, vastly outperforming text-only baselines (which show near-zero success due to robust textual filters). On GPT-4o and Qwen2.5-Omni, the method also shows significant gains over AdvWave and other audio jailbreaks. Tone-only manipulation (without adversarial wording) yields a 10–20% compliance boost across models, proving prosody alone influences behavior. Human speech experiments confirm the effect generalizes beyond synthetic voices. Common failure modes include premature termination (43% of cases) and internal representation instability, especially in compact models like Qwen2.5-Omni. The results indicate that current safety mechanisms fail to account for paralinguistic cues in raw audio inputs.


### 2026-01-29
- **Speech Quality-Based Localization of Low-Quality Speech and Text-to-Speech Synthesis Artefacts**
  - **作者**: Michael Kuhlmann et.al.
  - **arXiv**: [2601.21886](https://arxiv.org/abs/2601.21886)
  - **TLDR**: This paper introduces a method to enhance the interpretability of speech quality assessment (SQA) models by regularizing utterance-level predictors with a segment-based consistency constraint, enabling reliable frame-level quality scoring. The approach is validated through two applications: detecting partial spoofing attacks and localizing synthesis artefacts in state-of-the-art TTS systems, with listening tests confirming that low-scoring segments are perceptually degraded.
  - **核心贡献**: The main contribution is a regularization technique that enforces temporal consistency in frame-level predictions derived from utterance-level SQA models, thereby reducing stochasticity and improving localization accuracy without requiring frame-level ground truth labels. This bridges the gap between holistic quality scoring and interpretable, fine-grained error detection—addressing a key limitation in existing SQA systems that lack explainability for their predictions.
  - **关键发现**: On PartialSpoof, models with consistency loss achieved higher F1-scores (e.g., up to ~0.65 vs. ~0.55 without), demonstrating improved localization. On BVCC, Spearman rank correlation (SRCC) improved slightly with consistency regularization, indicating maintained utterance-level performance. Listening tests showed that segments with low frame-level scores were rated as poor quality significantly more often than random controls (e.g., >60% vs. <40% poor ratings). The model was particularly sensitive to glitches, discontinuities, and non-verbal vocalizations misaligned with linguistic content. However, precision-recall trade-offs were observed: higher thresholds increased precision but reduced recall.

- **Unit-Based Agent for Semi-Cascaded Full-Duplex Dialogue Systems**
  - **作者**: Haoyuan Yu et.al.
  - **arXiv**: [2601.20230](https://arxiv.org/abs/2601.20230)
  - **TLDR**: This paper introduces a unit-based agent framework for semi-cascaded full-duplex dialogue systems that decomposes conversations into minimal conversational units, enabling real-time, train-free, plug-and-play interaction using a multimodal large language model (MLLM) with auxiliary modules like VAD and TTS. The system achieves competitive performance on the HumDial dataset, ranking second in the Human-like Spoken Dialogue Systems Challenge (Track 2).
  - **核心贡献**: The core contribution is a novel framework for full-duplex spoken dialogue that structures interactions around minimal conversational units—discrete segments of speech that can be processed independently—allowing the system to dynamically decide when to transition between listening and speaking. This addresses the challenge of natural turn-taking and low-latency response generation in real-time human-computer dialogue without requiring end-to-end training. By leveraging a pre-trained multimodal LLM as the central reasoning engine and integrating off-the-shelf auxiliary modules (VAD, TTS, speaker verification), the system operates in a train-free, modular fashion, significantly reducing development complexity while maintaining high responsiveness and naturalness.
  - **关键发现**: The proposed system ranked second among all participating teams on the test set of the Human-like Spoken Dialogue Systems Challenge (Track 2). It demonstrated effective handling of full-duplex interaction with low response latency on the HumDial dataset. Qualitatively, the unit-based approach enabled more natural turn-taking behaviors, including appropriate use of backchannels and timely interruptions. The results indicate that a multimodal LLM can effectively replace traditional cascaded dialogue components without task-specific training, achieving near state-of-the-art performance in a plug-and-play setting.


### 2026-01-28
- **ASR for Affective Speech: Investigating Impact of Emotion and Speech Generative Strategy**
  - **作者**: Ya-Tse Wu et.al.
  - **arXiv**: [2601.20319](https://arxiv.org/abs/2601.20319)
  - **TLDR**: This paper investigates how emotional speech synthesized by different TTS models affects ASR performance, revealing that substitution errors dominate and vary across models. The authors propose two novel generative strategies—based on transcription correctness and emotional salience—to curate fine-tuning data, achieving consistent WER improvements on real emotional speech datasets without degrading performance on neutral speech.
  - **核心贡献**: The work addresses the challenge of degraded ASR performance on emotionally expressive speech by analyzing how different emotional TTS systems influence error patterns. It introduces a targeted data curation framework for fine-tuning ASR models using synthetic emotional speech, guided by two novel selection criteria: one prioritizing utterances with high transcription fidelity (low WER) and another emphasizing high emotional salience (measured via arousal/valence). This enables building emotion-aware ASR systems that generalize to real emotional speech while preserving robustness on clean, neutral data.
  - **关键发现**: All emotional TTS datasets degrade ASR performance compared to neutral speech, with substitution errors being dominant. Among TTS models, MaskGCT yields the lowest WER on synthetic data (4.40%). Fine-tuning with TTS-EMO-G reduces WER by up to 12.3% relative on MSP-Podcast Test2 and 9.8% on IEMOCAP compared to vanilla fine-tuning. Crucially, performance on LibriSpeech (neutral) remains stable (WER change < 0.1%), confirming no degradation. Gains are most pronounced in high-arousal/high-valence regions. The combined TTS-EMO-G strategy consistently outperforms individual strategies across all benchmarks, achieving the best average WER of 14.2% on real emotional datasets.

- **Audio Deepfake Detection in the Age of Advanced Text-to-Speech models**
  - **作者**: Robin Singh et.al.
  - **arXiv**: [2601.20510](https://arxiv.org/abs/2601.20510)
  - **TLDR**: This paper presents a comprehensive evaluation of audio deepfake detection systems against three cutting-edge TTS models—Dia2 (streaming), Maya1 (LLM-based), and MeloTTS (non-autoregressive)—demonstrating that single-paradigm detectors fail to generalize across architectures, while a multi-view detection approach combining semantic, structural, and signal-level features achieves robust performance. The work underscores the urgent need for integrated, adaptive detection frameworks in response to rapidly evolving synthetic speech technologies.
  - **核心贡献**: The paper addresses the critical gap in audio deepfake detection caused by the rapid advancement of diverse TTS architectures that render traditional, artifact-focused detectors obsolete. Its core contribution is the first empirical characterization of how detection efficacy varies dramatically across modern TTS paradigms (streaming, LLM-based, non-autoregressive) and the demonstration that a multi-view detection strategy—fusing complementary analytical perspectives—significantly outperforms single-method approaches. This reframes the detection problem from targeting fixed artifacts to adapting to heterogeneous generative mechanisms.
  - **关键发现**: Single-paradigm detectors showed significant performance variance: Whisper-MesoNet achieved 17.05% EER on Dia2 but degraded substantially on Maya1; XLS-R-SLS performed best overall but still recorded 27.10% EER on Maya1; SSL-AASIST excelled on Dia2 (low EER) but struggled with LLM-based synthesis. In contrast, the multi-view approach (exemplified by XLS-R-SLS and the proprietary UncovAI model) maintained robustness across all TTS types, with UncovAI achieving near-perfect separation (AUC ≈ 1.0). Critically, detectors trained on older vocoder artifacts suffered ~50% AUC drop on new TTS models. Maya1 proved most evasive due to its semantic grounding and multi-temporal token generation, which minimized traditional acoustic artifacts.

- **Erasing Your Voice Before It's Heard: Training-free Speaker Unlearning for Zero-shot Text-to-Speech**
  - **作者**: Myungjin Lee et.al.
  - **arXiv**: [2601.20481](https://arxiv.org/abs/2601.20481)
  - **TLDR**: The paper introduces TruS, a training-free speaker unlearning framework for zero-shot text-to-speech (TTS) systems that prevents unauthorized voice synthesis by suppressing identity-specific activations during inference. Unlike prior retraining-based methods, TruS works on both seen and unseen speakers without modifying the model, offering a scalable privacy safeguard.
  - **核心贡献**: TruS addresses the critical privacy risk in modern zero-shot TTS models—unauthorized voice cloning—by enabling speaker unlearning without any retraining. It innovates by shifting from data-centric deletion to inference-time control, manipulating internal hidden activations to erase target speaker identities while preserving non-identity attributes like prosody and emotion. This is the first method to generalize unlearning to speakers not present in the original training data (unseen opt-out speakers), overcoming a major limitation of existing approaches.
  - **关键发现**: TruS reduces speaker similarity for seen opt-out speakers by ~80% compared to baseline F5-TTS, matching the performance of retrained baselines (e.g., F5-TTS-FT) without any training. Crucially, it achieves significant suppression on unseen opt-out speakers (UO), where retraining methods fail entirely. WER remains stable (<2% increase), confirming preserved intelligibility. Emotion preservation scores show minimal degradation, indicating attribute retention. The 'µ + σ' layer selection yields optimal balance between unlearning strength and speech quality. Performance scales with larger retain sets (N=30 optimal).


### 2026-01-27
- **Phonological Tokenizer: Prosody-Aware Phonetic Token via Multi-Objective Fine-Tuning with Differentiable K-Means**
  - **作者**: Kentaro Onda et.al.
  - **arXiv**: [2601.19781](https://arxiv.org/abs/2601.19781)
  - **TLDR**: The paper introduces the Phonological Tokenizer, a novel method that fine-tunes phonetic tokens using differentiable k-means under a multi-objective framework combining ASR and speech resynthesis. This approach yields discrete speech tokens that preserve both linguistic and prosodic information while discarding speaker identity, making them well-suited for prosody-sensitive tasks like speech language modeling.
  - **核心贡献**: The core contribution is a prosody-aware phonetic tokenization method that bridges the gap between purely acoustic tokens (which retain speaker details) and traditional phonetic tokens (which often discard prosody). By jointly optimizing for automatic speech recognition (ASR) and speech waveform reconstruction via a multi-task objective, the Phonological Tokenizer learns discrete representations that abstract away speaker-specific attributes while preserving linguistically relevant prosodic cues—critical for downstream tasks such as expressive voice conversion, emotion recognition, and speech language models (speechLMs).
  - **关键发现**: The Phonological Tokenizer achieves state-of-the-art or competitive performance across multiple dimensions: (1) In ASR, it matches or exceeds Discrete WavLM (e.g., lower WER at optimal α); (2) In ER on RAVDESS, it significantly outperforms baselines (peak accuracy at intermediate α), confirming prosody retention; (3) In SID, performance degrades as α increases—demonstrating successful speaker information removal; (4) In voice conversion (Expresso VC), it outperforms all baselines in naturalness and speaker similarity; (5) In speechLM evaluations, it achieves the best GenPPL and UTMOS scores, indicating superior linguistic and prosodic coherence. Ablation shows that α=0.5 offers the best balance, with ER peaking there while ASR gradually declines and SID improves as α→1.

- **Rethinking Discrete Speech Representation Tokens for Accent Generation**
  - **作者**: Jinzuomu Zhong et.al.
  - **arXiv**: [2601.19786](https://arxiv.org/abs/2601.19786)
  - **TLDR**: This paper presents the first systematic study of how accent information is encoded in Discrete Speech Representation Tokens (DSRTs), revealing that ASR-fine-tuned encoders significantly attenuate accent cues and that naive codebook reduction fails to disentangle accent from phonetic and speaker content. The authors propose new content-only and content-accent DSRT designs that outperform existing methods in controllable accent generation, supported by a novel unified evaluation framework combining an Accent ABX task and cross-accent Voice Conversion.
  - **核心贡献**: The core contribution is a comprehensive investigation into the encoding and recoverability of accent information within DSRTs—previously overlooked in speech representation research. The authors introduce a dual-component evaluation framework (Accent ABX for accessibility and cross-accent VC for recoverability) to quantify accent preservation, uncover critical limitations in current DSRT pipelines (especially ASR fine-tuning), and propose redesigned DSRTs that explicitly preserve or separate accent information, enabling more effective accent-controlled TTS and voice conversion.
  - **关键发现**: Key quantitative results show: (1) ASR-fine-tuned encoders increase ABX error rates by up to 20% compared to SSL-only models, indicating severe accent information loss; (2) reducing codebook size does not effectively disentangle accent—it often degrades both phonetic and accent fidelity; (3) the proposed content-accent DSRTs achieve ~15% lower ABX error and ~10% higher AID accuracy than baseline HuBERT tokens; (4) in cross-accent VC, content-accent tokens yield significantly higher speaker similarity and intelligibility versus ASR-fine-tuned tokens. Qualitatively, resynthesized speech from new DSRTs preserves target accent characteristics more naturally, while ASR-derived tokens produce neutralized or distorted accents.

- **T-Mimi: A Transformer-based Mimi Decoder for Real-Time On-Phone TTS**
  - **作者**: Haibin Wu et.al.
  - **arXiv**: [2601.20094](https://arxiv.org/abs/2601.20094)
  - **TLDR**: This paper introduces T-Mimi, a Transformer-only decoder architecture that replaces the convolution-heavy Mimi codec decoder to drastically reduce on-device TTS latency—from 42.1ms to 4.4ms—while maintaining high audio quality. It also identifies critical quantization sensitivity in the final layers of the decoder, enabling efficient mixed-precision deployment on mobile CPUs.
  - **核心贡献**: The core contribution is the redesign of the Mimi neural audio codec’s decoder into a purely Transformer-based architecture (T-Mimi), eliminating deconvolution layers that are inefficient on mobile CPUs like those using XNNPACK. This addresses a key latency bottleneck in real-time on-device TTS systems. Additionally, the work provides empirical insights into quantization-aware training (QAT), revealing that only the last two Transformer layers and final linear layers must remain in full precision to preserve audio fidelity, enabling significant model compression without quality loss.
  - **关键发现**: T-Mimi reduces on-phone TTS latency from 42.1ms (original Mimi) to 4.4ms—a 9.6× speedup—while matching audio quality (PESQ ≈ 3.8, statistically on-par per CMOS). Human evaluation shows no significant preference between T-Mimi-32bit and Mimi-FT-32bit. An 8-bit quantized T-Mimi achieves 75% storage reduction with minimal quality drop, but 4-bit quantization degrades perceptual quality. Crucially, keeping only the last two Transformer layers and final linear layers at 32-bit preserves near-full-precision quality (PESQ 3.78 vs. 3.80). Increasing decoder depth from 8 to 12 layers yields substantial quality gains, while expanding linear dimension from 2048 to 3072 offers only marginal improvement at higher storage cost.


### 2026-01-26
- **UrgentMOS: Unified Multi-Metric and Preference Learning for Robust Speech Quality Assessment**
  - **作者**: Wei Wang et.al.
  - **arXiv**: [2601.18438](https://arxiv.org/abs/2601.18438)
  - **TLDR**: UrgentMOS introduces a unified framework for speech quality assessment that jointly learns from diverse objective and perceptual metrics while tolerating missing annotations, enabling robust training on heterogeneous datasets. It uniquely supports both absolute MOS prediction and direct comparative preference modeling, achieving state-of-the-art performance across multiple evaluation settings.
  - **核心贡献**: The paper addresses the limitations of existing learning-based speech quality assessment models that rely heavily on scarce and inconsistent human-annotated Mean Opinion Score (MOS) data. UrgentMOS solves this by proposing a unified architecture that can leverage partially annotated, multi-source datasets through joint learning from heterogeneous supervision signals—including both absolute quality metrics and pairwise preferences—while explicitly modeling comparative MOS (CMOS) for benchmarking scenarios. Its innovation lies in enabling robust generalization across domains without requiring complete annotation coverage for all metrics during training.
  - **关键发现**: UrgentMOS achieves state-of-the-art results across both absolute and comparative tasks. On correlation metrics, it outperforms baselines in LCC/SRCC on representative datasets (e.g., higher than NISQA-MOS on CHiME-7-UDASE-Eval). In preference evaluation, it shows superior acc0.5 and acc0 on SpeechEval and additional test sets (Tables 4 and 9). Notably, it demonstrates strong cross-dataset generalization, maintaining performance even when trained on mixed-source data with inconsistent labeling—unlike prior models that overfit to dataset-specific characteristics. The direct preference modeling approach proves more aligned with human judgments than MOS-derived comparisons, especially under varying tie thresholds. However, performance degrades slightly on certain domains like F1C1M15, indicating room for improvement.


### 2026-01-25
- **AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation**
  - **作者**: Dongjie Cheng et.al.
  - **arXiv**: [2601.17761](https://arxiv.org/abs/2601.17761)
  - **TLDR**: AR-Omni introduces a unified autoregressive model capable of any-to-any multimodal generation—including text, images, and streaming speech—using a single Transformer decoder without relying on external expert modules like diffusion models. It addresses key challenges in modality imbalance, visual fidelity, and decoding stability while achieving real-time speech synthesis with a 0.88 real-time factor.
  - **核心贡献**: The paper presents AR-Omni, the first purely autoregressive, diffusion-free, unified model that supports multimodal understanding and generation across text, vision, and speech within a single architecture. It solves the problem of fragmented multimodal systems that depend on separate expert decoders (e.g., diffusion models for images or dual-codebook systems for speech), enabling simpler training, inference, and true any-to-any generation under one next-token prediction objective.
  - **关键发现**: AR-Omni achieves competitive ASR performance on LibriSpeech (comparable WER to specialized models) and strong zero-shot TTS results on VCTK with a real-time factor of 0.88 and low first-token latency, enabling streaming. In image generation, it produces high-fidelity outputs across diverse prompts without diffusion models, as shown in Figures 7–10. Ablation studies confirm that removing task-aware loss reweighting causes modality collapse, while omitting the perceptual alignment loss degrades image quality. The finite-state decoder enables stable yet expressive speech. AR-Omni is the only model reported that simultaneously supports unified I/O, real-time streaming, and diffusion-free operation across all three modalities.

- **Quran-MD: A Fine-Grained Multilingual Multimodal Dataset of the Quran**
  - **作者**: Muhammad Umar Salman et.al.
  - **arXiv**: [2601.17880](https://arxiv.org/abs/2601.17880)
  - **TLDR**: Quran-MD is a fine-grained, multilingual, multimodal dataset of the Quran that integrates Arabic text, English translations, phonetic transliterations, and aligned audio recordings at both verse and word levels from 32 reciters. It enables advanced computational research in TTS, ASR, tajweed error detection, prosody modeling, and multimodal embeddings for Quranic studies.
  - **核心贡献**: The paper introduces Quran-MD, a unified, high-quality multimodal dataset that bridges textual and audio modalities of the Quran with precise alignment at the word and verse levels across 32 diverse reciters. It solves the problem of fragmented, unaligned, or incomplete existing Quranic datasets by providing synchronized Arabic script, English translation, transliteration, and time-aligned audio—enabling fine-grained linguistic, phonetic, and stylistic analysis previously impossible due to data scarcity or misalignment. This resource directly supports TTS systems tailored to Quranic recitation, which requires adherence to tajweed rules and melodic prosody not found in standard Arabic speech.
  - **关键发现**: The paper does not present quantitative experimental results since it is a dataset contribution. However, it highlights qualitative findings: Quran-MD is the only dataset offering word-level aligned audio across >30 reciters with full textual metadata. Table 1 shows it surpasses all prior works in modality richness (text + audio + translation + transliteration), alignment granularity (word + verse), and reciter diversity. The authors note successful resolution of missing/misaligned entries during curation, resulting in a 'complete, consistent, and multimodal' resource. They emphasize its suitability for tasks like forced alignment, tajweed error detection, and style transfer—though performance benchmarks are deferred to future work.


### 2026-01-23
- **SonoEdit: Null-Space Constrained Knowledge Editing for Pronunciation Correction in LLM-Based TTS**
  - **作者**: Ayush Pratap Singh et.al.
  - **arXiv**: [2601.17086](https://arxiv.org/abs/2601.17086)
  - **TLDR**: SonoEdit introduces a novel, one-shot model editing technique for correcting pronunciation errors in pre-trained LLM-based TTS systems without retraining or degrading general speech capabilities. By leveraging null-space constrained updates guided by acoustic causal tracing, it surgically modifies only the layers responsible for pronunciation while provably preserving all other model behavior.
  - **核心贡献**: The paper addresses the persistent problem of mispronunciation of low-resource proper nouns (e.g., non-English names, brands, locations) in neural TTS systems trained predominantly on English data. Unlike existing approaches that require costly fine-tuning, phonetic annotations, or multilingual data, SonoEdit enables targeted, single-step parameter updates that correct specific pronunciations while mathematically guaranteeing zero first-order change to the model’s output on a preserved speech corpus. This is achieved through a combination of layer-localization via Acoustic Causal Tracing and a closed-form weight update computed in the null-space of general speech representations.
  - **关键发现**: SonoEdit achieved a Target-WER of 2.8% on HardNoun-300, significantly outperforming baselines: Full Fine-Tuning (12.4%), LoRA (9.7%), and the original model (28.6%). Crucially, it maintained a Global-WER of 3.15%, statistically indistinguishable from the original model (3.12%), confirming no degradation in general speech quality. Subjective evaluations confirmed preserved prosody, stress patterns, and speaker identity. The method also demonstrated high edit success rate (98% of edits effective) and minimal drift in acoustic features. Unconstrained methods showed noticeable artifacts and WER degradation, while SonoEdit’s null-space constraint prevented interference with unrelated utterances.


### 2026-01-22
- **DeepASMR: LLM-Based Zero-Shot ASMR Speech Generation for Anyone of Any Voice**
  - **作者**: Leying Zhang et.al.
  - **arXiv**: [2601.15596](https://arxiv.org/abs/2601.15596)
  - **TLDR**: DeepASMR introduces the first zero-shot TTS framework capable of generating high-fidelity ASMR speech in any speaker's voice using only a short snippet of their normal read-style speech, without requiring whispered training data. It leverages discrete speech tokens and a two-stage LLM + flow-matching pipeline to disentangle ASMR style from speaker timbre, supported by a new 670-hour bilingual ASMR dataset and a comprehensive evaluation protocol.
  - **核心贡献**: The paper solves the critical challenge of zero-shot ASMR speech synthesis—a specialized, low-intensity, often unvoiced speaking style used for relaxation—by introducing DeepASMR, a novel framework that enables personalized ASMR generation for any speaker without needing target-speaker whispered data. The core innovation lies in recognizing that discrete speech tokens provide a 'soft factorization' between ASMR stylistic attributes and speaker identity, enabling effective disentanglement. This allows the system to condition on a speaker’s normal speech and generate authentic ASMR in their voice, a capability absent in prior TTS systems.
  - **关键发现**: DeepASMR outperforms all baselines in cross-style ASMR synthesis: it achieves the lowest WER (indicating better intelligibility despite whispering), highest LLM-based style scores (>90% ASMR authenticity), and most accurate unvoiced ratios (~85–90%, close to ground truth). In subjective evaluations, it scores MOS of 4.1–4.3 (out of 5), approaching ground-truth ASMR (4.5). Notably, standard zero-shot TTS models (CosyVoice2, F5TTS) fail to suppress vocal fold vibration, producing voiced speech even when prompted for ASMR. Cascade VC approaches produce inflated unvoiced ratios but poor naturalness. DeepASMR maintains competitive performance on normal speech (intra-style), confirming no degradation in general TTS capability. Commercial models (e.g., ElevenLabs) also fail to generate authentic ASMR due to reliance on voiced phonation.

- **Qwen3-TTS Technical Report**
  - **作者**: Hangrui Hu et.al.
  - **arXiv**: [2601.15621](https://arxiv.org/abs/2601.15621)
  - **TLDR**: Qwen3-TTS introduces a family of multilingual, controllable, and streaming-capable text-to-speech models trained on over 5 million hours of speech across 10 languages, featuring state-of-the-art 3-second voice cloning and description-based voice design. It leverages a dual-track LM architecture and two novel speech tokenizers to enable ultra-low-latency streaming and high-quality synthesis, with all models and tokenizers released under Apache 2.0.
  - **核心贡献**: The paper presents Qwen3-TTS, the first TTS system in the Qwen series, which addresses key challenges in modern TTS: real-time streaming, multilingual support, zero-shot voice cloning, and fine-grained controllability via natural language instructions. Its core innovation lies in integrating large language model (LLM) capabilities with specialized speech tokenizers to enable both high-fidelity synthesis and ultra-low-latency streaming—balancing semantic fidelity and acoustic detail through a dual-tokenizer strategy and a dual-track autoregressive architecture.
  - **关键发现**: Qwen3-TTS-12Hz-1.7B-VD achieves state-of-the-art results among open-source models in voice design, outperforming commercial systems like Hume and VoiceSculptor. It demonstrates near-lowest WER across multiple datasets and maintains fluency in >10-minute long-form speech without prosodic discontinuity. The 12Hz tokenizer enables first-packet emission in 97ms, setting a new benchmark for ultra-low-latency streaming. Subjective evaluations show human-like naturalness, particularly in the 1.7B S2-finetuned variant. The model excels in zero-shot cross-lingual voice cloning and follows complex voice instructions with high fidelity. Both tokenizers achieve competitive STOI and PESQ scores, with the 25Hz variant showing superior semantic alignment with Qwen-Audio.

- **Timbre-Aware LLM-based Direct Speech-to-Speech Translation Extendable to Multiple Language Pairs**
  - **作者**: Lalaram Arya et.al.
  - **arXiv**: [2601.16023](https://arxiv.org/abs/2601.16023)
  - **TLDR**: This paper introduces DS2ST-LM, a single-stage, multilingual direct speech-to-speech translation (S2ST) system that leverages a multilingual LLM (Qwen2-0.5B) and a timbre-aware vocoder to improve semantic fidelity, speaker identity preservation, and scalability across six language pairs. By constructing a new 1000-hour synthetic bilingual dataset (GigaS2S-1000) and comparing semantic tokenization strategies and projection architectures, the authors demonstrate state-of-the-art performance over both cascaded and prior direct S2ST baselines.
  - **核心贡献**: The core contribution is DS2ST-LM—a unified, end-to-end direct S2ST framework that addresses key limitations of existing systems: data scarcity in low-resource language pairs, poor speaker identity retention, and limited multilingual scalability. It innovatively combines a Whisper speech encoder, a learnable projection module, a multilingual LLM for semantic reasoning, and a timbre-controlled vocoder within a single trainable pipeline. The work also introduces GigaS2S-1000, a large-scale synthetic dataset that mitigates parallel speech data scarcity, and systematically evaluates semantic token generation methods and projection architectures to optimize training stability and output quality.
  - **关键发现**: DS2ST-LM outperforms cascaded and ST+TTS baselines across all metrics: BLEU scores improved by up to 4.2 points on zh-en, COMET by 6.8 points, and BLEURT by 5.1 points. On multilingual evaluation (fr, es, de, hi, bn, ur), it consistently surpasses baselines. Speaker similarity (MOS-SIM) reached 3.54, significantly higher than prior direct S2ST systems and approaching ground truth. Surprisingly, the Linear projection achieved the best final performance despite slower convergence, while Q-Former converged faster but plateaued lower. Text-derived semantic tokens yielded more stable training and better semantic consistency than speech-derived S3 tokens. Synthetic data in GigaS2S-1000 enabled effective training with only modest performance degradation versus real data, facilitating extension to low-resource pairs.


### 2026-01-20
- **Habibi: Laying the Open-Source Foundation of Unified-Dialectal Arabic Speech Synthesis**
  - **作者**: Yushen Chen et.al.
  - **arXiv**: [2601.13802](https://arxiv.org/abs/2601.13802)
  - **TLDR**: The paper introduces Habibi, an open-source suite of unified and dialect-specialized text-to-speech (TTS) models for Arabic dialects that leverages existing ASR corpora and linguistically-informed curriculum learning to overcome data scarcity and linguistic complexity. It establishes the first systematic benchmark for multi-dialect Arabic TTS and demonstrates superior performance over leading commercial systems without requiring diacritization.
  - **核心贡献**: Habibi addresses the critical gap in Arabic dialectal TTS by providing both unified and specialized open-source models trained on diverse, noisy, real-world ASR datasets. The core innovation lies in a linguistically-motivated curriculum learning strategy that enables high-quality synthesis across more than 20 Arabic dialects—including low-resource ones—without relying on diacritized text, while also introducing standardized evaluation protocols and benchmarks previously absent in the field.
  - **关键发现**: The unified Habibi model outperforms ElevenLabs’ Eleven v3 across all six major Arabic dialect test sets in both objective (SIM, ASR word error rate) and subjective quality metrics. Curriculum-trained unified models achieve comparable performance to specialized models despite sharing parameters across dialects. Models trained with regional identifiers show improved dialect discrimination and generation quality. Denoising significantly boosts performance on low-SNR data (e.g., Saudi dataset). The MSA-first curriculum enables successful convergence where training from scratch fails. Even with half the training updates, the curriculum approach matches models trained twice as long without it. ASR-based evaluation confirms higher intelligibility versus commercial systems. Notably, Habibi achieves this without diacritization—a common requirement in prior Arabic TTS systems.

- **HoverAI: An Embodied Aerial Agent for Natural Human-Drone Interaction**
  - **作者**: Yuhua Jin et.al.
  - **arXiv**: [2601.13801](https://arxiv.org/abs/2601.13801)
  - **TLDR**: HoverAI introduces a novel embodied aerial agent that combines drone mobility, onboard visual projection, and real-time conversational AI to enable natural, socially responsive human-drone interaction. By integrating multimodal perception (vision and voice), adaptive avatar rendering, and infrastructure-free communication, it addresses the critical gap in intention transparency for drones operating in human-populated environments.
  - **核心贡献**: The paper presents HoverAI as a unified platform that solves the problem of ambiguous drone intentions in shared human spaces by creating a socially present, communicative aerial agent. Its innovation lies in the tight integration of three previously disjoint capabilities: autonomous drone navigation, self-contained visual output via a MEMS laser projector and semi-rigid screen, and a real-time, personalized conversational AI pipeline that adapts both speech and avatar appearance based on user demographics. This creates a spatially-aware, interactive agent capable of guidance and assistance without relying on external infrastructure like screens or speakers.
  - **关键发现**: HoverAI achieved strong technical performance: command recognition F1-score of 0.90, gender classification F1 of 0.89, age estimation MAE of 5.14 years, and ASR WER of 0.181. The system demonstrated robust real-time operation with synchronized lip movements and adaptive avatar rendering. Qualitatively, the integration of visual projection and conversational AI enhanced perceived social presence. However, results were limited to a controlled lab setting with 12 participants, and no statistical significance testing or user satisfaction metrics (e.g., SUS, NASA-TLX) were reported.

- **Prosody-Guided Harmonic Attention for Phase-Coherent Neural Vocoding in the Complex Spectrum**
  - **作者**: Mohammed Salah Al-Radhi et.al.
  - **arXiv**: [2601.14472](https://arxiv.org/abs/2601.14472)
  - **TLDR**: This paper introduces a novel neural vocoder that integrates prosody-guided harmonic attention with direct complex spectrum modeling to jointly predict magnitude and phase, improving pitch fidelity and phase coherence. By combining adversarial, spectral, and phase-aware losses in a unified architecture, it outperforms HiFi-GAN and AutoVocoder in both objective metrics (e.g., 22% lower F0 RMSE) and subjective quality (MOS +0.15).
  - **核心贡献**: The core contribution is a unified neural vocoder architecture that explicitly models both prosody (via fundamental frequency, F0) and complex spectral components (magnitude and phase) within a single framework. This addresses two persistent limitations in current neural vocoders: inadequate prosody representation and inaccurate or incoherent phase reconstruction. The proposed prosody-guided harmonic attention mechanism conditions the model on extracted F0 to enhance voiced segment encoding, while direct prediction of complex spectra enables phase-coherent waveform synthesis via inverse STFT—bypassing the need for post-processing or indirect phase estimation used in mel-spectrogram-based systems.
  - **关键发现**: The proposed vocoder achieved a 22% reduction in F0 RMSE compared to HiFi-GAN, indicating significantly improved pitch tracking. Voiced/unvoiced error decreased by 18%, demonstrating better voicing decision accuracy. MOS scores improved by 0.15 points, reflecting enhanced naturalness. Qualitative analysis showed waveforms closely matched reference prosody and exhibited smoother phase trajectories, especially in voiced regions. The model maintained phase coherence across frames, reducing artifacts common in magnitude-only vocoders. These gains were consistent across datasets, confirming robustness.

- **Synthetic Singers: A Review of Deep-Learning-based Singing Voice Synthesis Approaches**
  - **作者**: Changhao Pan et.al.
  - **arXiv**: [2601.13910](https://arxiv.org/abs/2601.13910)
  - **TLDR**: This survey provides a comprehensive and systematic review of deep-learning-based singing voice synthesis (SVS) approaches, categorizing them into cascaded and end-to-end paradigms while analyzing core technologies such as singing modeling, control mechanisms, datasets, and evaluation benchmarks. It serves as a timely reference for researchers and engineers navigating the rapidly evolving landscape of generative SVS systems.
  - **核心贡献**: The paper addresses the lack of a unified, up-to-date survey in the field of deep-learning-based SVS by offering a structured taxonomy of existing systems, a detailed breakdown of enabling technologies, and an overview of resources including datasets and evaluation protocols. Its innovation lies in synthesizing fragmented advances—spanning acoustic modeling, vocoding, controllability, and generative paradigms like diffusion models and large language models—into a coherent framework that clarifies research trajectories and technical trade-offs.
  - **关键发现**: The survey finds that cascaded systems remain dominant due to modularity and ease of control, but end-to-end models are gaining traction with improved naturalness and reduced error propagation. Diffusion-based and consistency models show promise in high-fidelity waveform generation, while LLM-integrated architectures enable better prosody and long-form coherence. Data scarcity and misalignment between lyrics and audio remain critical bottlenecks. Subjective evaluation is still the gold standard, as objective metrics poorly correlate with perceived singing quality. Recent works achieve MOS scores above 4.0 on clean datasets, approaching human-level naturalness in constrained settings.


### 2026-01-19
- **Lombard Speech Synthesis for Any Voice with Controllable Style Embeddings**
  - **作者**: Seymanur Akti et.al.
  - **arXiv**: [2601.12966](https://arxiv.org/abs/2601.12966)
  - **TLDR**: This paper introduces a zero-shot controllable TTS system that synthesizes Lombard speech—characterized by increased vocal effort in noisy environments—for any speaker without requiring Lombard-labeled training data. By leveraging style embeddings from a prosodically diverse dataset and manipulating them via PCA-based shifts along Lombard-correlated dimensions, the method achieves fine-grained control over Lombardness while preserving speaker identity and naturalness.
  - **核心贡献**: The core contribution is a novel framework for zero-shot Lombard speech synthesis that eliminates the need for speaker-specific or Lombard-specific training data. It solves the problem of generalizing Lombard-style prosody to arbitrary speakers by repurposing pre-trained style embeddings learned from large-scale, prosodically varied read-speech datasets. The innovation lies in identifying and manipulating latent directions in the style embedding space that correlate with Lombard attributes using PCA, enabling continuous and controllable Lombard speech generation.
  - **关键发现**: The proposed method achieves lower WER than the F5-TTS baseline under noisy conditions (SNR = 10 and 5 dB), with Table 2 showing consistent improvements and Table 3 confirming positive ∆WER reductions. SSIM scores (Table 5) indicate high speaker similarity across Lombardness levels, demonstrating preserved speaker identity. Subjective evaluations reveal maintained naturalness even at higher Lombardness settings. The model outperforms prior approaches that require Lombard data in terms of generalization to unseen speakers. Preliminary experiments confirmed that style embeddings from prosodically rich data contain latent Lombard-related prosodic cues, and PCA effectively isolates these dimensions.


### 2026-01-18
- **A Unified Neural Codec Language Model for Selective Editable Text to Speech Generation**
  - **作者**: Hanchen Pei et.al.
  - **arXiv**: [2601.12480](https://arxiv.org/abs/2601.12480)
  - **TLDR**: The paper introduces SpeechEdit, a unified neural codec language model that enables selective editing of specific speech attributes (e.g., emotion, prosody) while preserving other characteristics from a reference prompt. By leveraging a novel delta-pair training strategy on the newly constructed LibriEdit dataset, it achieves controllable zero-shot TTS without sacrificing naturalness or robustness.
  - **核心贡献**: SpeechEdit addresses the limitation of existing neural codec language models—which holistically imitate all acoustic attributes of a reference audio—by introducing a selective control mechanism. This allows users to override only specified attributes (e.g., change emotion while keeping speaker identity and prosody intact) using explicit textual instructions. The core innovation lies in enabling attribute-level disentanglement through data-driven implicit learning rather than architectural modifications or auxiliary modules, all within a single unified model supporting both autoregressive (AR) and non-autoregressive (NAR) generation stages.
  - **关键发现**: SpeechEdit achieves competitive zero-shot TTS performance on LibriSpeech test-clean (WER comparable to VALL-E 2). In emotion editing, it outperforms Step-Audio-EditX in CMOS by +0.42 points for controllability while maintaining naturalness (SMOS > 4.0). Objective results show lower MCD scores than baselines in editing tasks, indicating higher fidelity. The model successfully edits emotion without degrading speaker identity (SSIM > 0.85). However, pitch and energy manipulations yield slightly lower naturalness. Ablation studies confirm that delta-pair training significantly improves editing accuracy. Notably, naive emotional data augmentation without delta alignment harms performance, validating the need for structured difference-aware pairs.

- **Confidence-based Filtering for Speech Dataset Curation with Generative Speech Enhancement Using Discrete Tokens**
  - **作者**: Kazuki Yamauchi et.al.
  - **arXiv**: [2601.12254](https://arxiv.org/abs/2601.12254)
  - **TLDR**: This paper introduces a confidence-based filtering method for curating high-quality TTS datasets using generative speech enhancement (GSE) models that operate on discrete tokens. By leveraging token-level log-probabilities as confidence scores, the method effectively detects hallucination errors—such as phoneme omissions and speaker inconsistencies—that conventional non-intrusive quality metrics miss, leading to improved downstream TTS model performance.
  - **核心贡献**: The core contribution is a non-intrusive, confidence-based filtering technique specifically designed for discrete token-based GSE models used in TTS dataset curation. It addresses the critical issue of hallucination errors introduced during speech enhancement, which degrade TTS model training but are undetectable by standard speech quality metrics like DNSMOS or PESQ. The innovation lies in repurposing internal model confidence (via token log-probabilities) as a proxy for semantic and acoustic fidelity, enabling effective error detection without reference audio.
  - **关键发现**: Confidence scores show strong correlation (not quantified numerically in excerpts but implied via results) with intrusive metrics like WAcc. The proposed method significantly outperforms DNSMOS and PESQ in detecting hallucination errors—evidenced by higher WAcc retention at equivalent recall levels. TTS models trained on confidence-filtered data achieve higher MOS (+0.3–0.5 points over unfiltered enhanced data) and better intelligibility. Even when combined with other metrics, confidence alone performs competitively. Filtering too aggressively (e.g., top 30%) harms performance due to data loss, indicating a trade-off between quality and quantity. Unfiltered enhanced data underperforms noisy source data in some cases, highlighting the danger of hallucinations.

- **ParaMETA: Towards Learning Disentangled Paralinguistic Speaking Styles Representations from Speech**
  - **作者**: Haowei Lou et.al.
  - **arXiv**: [2601.12289](https://arxiv.org/abs/2601.12289)
  - **TLDR**: ParaMETA introduces a unified, lightweight framework for learning disentangled representations of paralinguistic speaking styles (e.g., emotion, age, gender, language) directly from speech, enabling both accurate multi-task classification and fine-grained, controllable speech generation in TTS systems. By projecting speech into dedicated subspaces per style attribute, it reduces inter-task interference and negative transfer while supporting both speech- and text-based prompting.
  - **核心贡献**: The paper addresses the challenge of modeling multiple paralinguistic attributes simultaneously without performance degradation due to task interference—a common issue in multi-task learning for speech. ParaMETA’s core innovation is a projection-based architecture that learns disentangled, task-specific embeddings by mapping input speech into orthogonal or dedicated subspaces for each style dimension. This enables a single model to perform well across diverse recognition tasks (emotion, gender, age, language) and to condition TTS models with precise, editable style control—preserving non-target attributes when modifying one style factor.
  - **关键发现**: ParaMETA outperforms baselines in 12 out of 16 backbone–task combinations for classification, with consistent gains across emotion, gender, age, and language tasks. In contrast, CLAP-based models (e.g., ParaCLAP) show poor performance across all tasks, suffering from negative transfer. For TTS, ParaMETA-conditioned models generate more natural and expressive speech, with speech-based prompting yielding higher fidelity than text-based. Style editing experiments confirm that modifying one attribute (e.g., emotion) preserves others (e.g., gender), demonstrating disentanglement. Efficiency-wise, ParaMETA uses only 589 MB GPU memory vs. 1966 MB for CLAP—a 70% reduction—while maintaining real-time inference suitability.


### 2026-01-16
- **F-Actor: Controllable Conversational Behaviour in Full-Duplex Models**
  - **作者**: Maike Züfle et.al.
  - **arXiv**: [2601.11329](https://arxiv.org/abs/2601.11329)
  - **TLDR**: F-Actor introduces the first open, instruction-following full-duplex conversational speech model that enables dynamic control over speaker voice, topic, backchanneling, interruptions, and dialogue initiation. It achieves this with only 2,000 hours of data by freezing the audio encoder and fine-tuning a lightweight LLM, making high-quality controllable spoken dialogue accessible under academic resource constraints.
  - **核心贡献**: The paper addresses the lack of controllability and customization in existing full-duplex spoken conversational systems, which typically require massive pretraining or multi-stage pipelines. F-Actor solves this by proposing a single-stage training protocol that leverages a frozen audio encoder (NanoCodec) and fine-tunes only a small instruction-tuned LLM (Llama3.2-1B-Instruct), enabling explicit control over conversational behaviors without large-scale infrastructure. This approach democratizes research in controllable full-duplex TTS by significantly lowering data and compute requirements while releasing both model and code.
  - **关键发现**: F-Actor achieves over 99% accuracy in instruction-following tasks (e.g., initiating conversation, using backchannels) when generating both text and audio with appropriate delays. Models trained on both system and user roles (s/u) outperform system-only (s) variants in behavioral control. Word-level alignment yields more precise timing but higher complexity; utterance-level is more robust. In turn-taking, F-Actor exhibits human-like overlap and gap durations, though pauses are slightly longer than natural speech. General modeling performance approaches oracle values, and self-dialogues are coherent and contextually appropriate. The model successfully controls voice characteristics, topic, and interruption frequency as specified in prompts.

- **FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning**
  - **作者**: Tanyu Chen et.al.
  - **arXiv**: [2601.11141](https://arxiv.org/abs/2601.11141)
  - **TLDR**: FlashLabs Chroma 1.0 introduces the first open-source, real-time, end-to-end spoken dialogue model that simultaneously achieves high-fidelity personalized voice cloning and sub-second latency through a novel interleaved text-audio token schedule. It outperforms human baselines in speaker similarity while maintaining strong dialogue reasoning capabilities.
  - **核心贡献**: Chroma 1.0 addresses the critical limitation in current end-to-end spoken dialogue systems—poor speaker identity preservation during multi-turn conversations—by integrating personalized voice cloning directly into a streaming-capable, low-latency architecture. Unlike cascaded or non-personalized models, Chroma enables real-time, zero-shot voice cloning without requiring separate voice profile extraction stages, thus unifying understanding, reasoning, and expressive speech generation in a single model.
  - **关键发现**: Chroma achieved a 10.96% relative improvement in speaker similarity over the human baseline, demonstrating superior voice cloning fidelity. It attained an RTF of 0.43, confirming real-time capability. In subjective evaluations, Chroma’s voice cloning was rated competitively against ElevenLabs, with remarkably close SCMOS scores. It consistently ranked second-best in task accomplishment (71.14%) among all compared models and was the only model balancing strong dialogue reasoning with high-quality personalized speech. Chroma outperformed smaller (0.5B) and larger (7B–9B) models in the real-time dialogue setting, highlighting its efficiency-quality trade-off.

- **WenetSpeech-Wu: Datasets, Benchmarks, and Models for a Unified Chinese Wu Dialect Speech Processing Ecosystem**
  - **作者**: Chengyou Wang et.al.
  - **arXiv**: [2601.11027](https://arxiv.org/abs/2601.11027)
  - **TLDR**: This paper introduces WenetSpeech-Wu, the first large-scale, multi-dimensionally annotated open-source speech corpus for the Chinese Wu dialect (~8,000 hours), along with WenetSpeech-Wu-Bench—a standardized benchmark covering ASR, TTS, instruct TTS, translation, and other tasks—and a suite of strong open-source models. These contributions address the longstanding data and evaluation gaps for low-resource dialectal speech processing and establish a foundational ecosystem for inclusive Wu dialect speech intelligence.
  - **核心贡献**: The core contribution is the creation of a unified, open ecosystem for Wu dialect speech processing, comprising (1) WenetSpeech-Wu: a large-scale, high-quality, multi-annotated speech dataset; (2) WenetSpeech-Wu-Bench: the first standardized benchmark evaluating multiple speech tasks including TTS and instruction-following TTS; and (3) a set of state-of-the-art open-source models trained on this data. This solves the critical problem of data scarcity, lack of evaluation standards, and absence of public models that have historically impeded research and development for the linguistically significant but under-resourced Wu dialect.
  - **关键发现**: The CosyVoice2-Wu-SS TTS model achieves state-of-the-art performance, matching or exceeding commercial and prior open baselines in naturalness and speaker similarity. Instruct TTS models show significant gains after instruction fine-tuning, with high adherence to emotion and prosody prompts (e.g., >85% accuracy in emotion execution per human evaluation). ASR models trained on WenetSpeech-Wu achieve CERs as low as 6.2% on in-domain test sets, substantially outperforming models trained on smaller or non-Wu data. Unified speech understanding models also show balanced performance across demographic and emotional attributes. Ablation studies confirm the value of multi-stage training and high-quality SFT data.


### 2026-01-15
- **DSA-Tokenizer: Disentangled Semantic-Acoustic Tokenization via Flow Matching-based Hierarchical Fusion**
  - **作者**: Hanlin Zhang et.al.
  - **arXiv**: [2601.09239](https://arxiv.org/abs/2601.09239)
  - **TLDR**: The paper introduces DSA-Tokenizer, a novel speech tokenizer that explicitly disentangles semantic and acoustic information into separate discrete token streams using distinct supervision signals—ASR for semantics and mel-spectrogram reconstruction for acoustics. This enables high-fidelity speech reconstruction and flexible recombination (e.g., voice cloning with swapped content/style), advancing controllable generation in discrete Speech LLMs.
  - **核心贡献**: DSA-Tokenizer addresses the critical limitation in existing speech tokenizers—insufficient or absent disentanglement between linguistic content and speaker/acoustic style—by enforcing explicit separation through dual optimization objectives and a hierarchical fusion mechanism. Unlike prior methods that either mix semantics and acoustics or achieve only partial disentanglement, DSA-Tokenizer trains two distinct tokenizers under separate constraints: semantic tokens are optimized via ASR loss to preserve linguistic content, while acoustic tokens are trained to reconstruct mel-spectrograms, capturing prosody, timbre, and speaker identity. The key innovation lies in enabling independent manipulation of content and style without rigid alignment constraints, facilitated by a flow-matching-based hierarchical decoder.
  - **关键发现**: DSA-Tokenizer achieves superior balance across all metrics: (1) In reconstruction, it matches or exceeds baselines in WER (e.g., ~5.2% on LibriSpeech vs. 5.8% for CosyVoice2) while maintaining high UTMOS (>3.8) and SIM (>0.75); (2) In recombination, it significantly outperforms fused/disentangled baselines—e.g., 15–20% lower WER and 0.2+ higher SIM in voice cloning; (3) Disentanglement probing shows semantic tokens (L0) have high ASR accuracy (>90%) but low speaker accuracy (<20%), while acoustic tokens (L1–L7) show opposite pattern (speaker acc >85%, ASR acc <30%), confirming effective separation; (4) Ablation shows removing recombination training causes severe collapse in cross-utterance tasks despite minor reconstruction degradation; (5) Hierarchical flow-matching improves naturalness over standard diffusion or GAN decoders.

- **ESDD2: Environment-Aware Speech and Sound Deepfake Detection Challenge Evaluation Plan**
  - **作者**: Xueping Zhang et.al.
  - **arXiv**: [2601.07303](https://arxiv.org/abs/2601.07303)
  - **TLDR**: This paper introduces ESDD2, a new challenge focused on detecting component-level deepfakes in audio where either speech or environmental sounds—or both—may be synthetically manipulated. It presents the CompSpoofV2 dataset and a separation-enhanced joint learning framework to address the limitations of existing whole-audio anti-spoofing systems in realistic, mixed-environment scenarios.
  - **核心贡献**: The core contribution is the proposal of a novel problem setting—component-level audio spoofing detection—and the accompanying resources: (1) CompSpoofV2, a large-scale dataset with over 250k samples (~283 hours) labeled across five classes reflecting independent or combined manipulation of speech and background sounds, and (2) a separation-enhanced joint learning framework that integrates source separation with anti-spoofing classification. This addresses a critical gap in current deepfake detection, which typically assumes the entire audio clip is either real or fake, failing when only one component is altered while the other remains authentic.
  - **关键发现**: The baseline model achieves measurable performance on CompSpoofV2 using EER and Macro-F1, though exact numerical results are not provided in the excerpts. Qualitatively, the authors note that traditional whole-audio detectors fail significantly on component-level spoofs because authentic components mask manipulations in others. The joint separation-classification approach preserves discriminative cues better than pipeline-based methods. The challenge design ensures fair comparison by controlling data leakage and external resource usage. The dataset’s scale (250k samples, 283 hours) and class diversity enable robust training and evaluation under realistic conditions.

- **VoiceSculptor: Your Voice, Designed By You**
  - **作者**: Jingbin Hu et.al.
  - **arXiv**: [2601.10629](https://arxiv.org/abs/2601.10629)
  - **TLDR**: VoiceSculptor introduces an open-source, unified TTS system that enables fine-grained, natural-language-driven control over voice attributes like pitch, emotion, age, and speaking style by combining instruction-following voice design with high-fidelity voice cloning. It achieves state-of-the-art performance among open-source models on the InstructTTSEval-Zh benchmark while fully releasing code and pretrained models to promote reproducible research.
  - **核心贡献**: The paper addresses a critical gap in open-source TTS systems: the lack of truly instruction-following, fine-grained control over core speech attributes using natural language. VoiceSculptor solves this by unifying two components—(1) a voice design model that interprets natural-language instructions to generate controllable speaker timbre representations, and (2) a voice cloning model that renders these designs into high-fidelity speech. Its innovation lies in enabling iterative, attribute-level voice editing via Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)-based decomposition of acoustic attributes, all within an open, reproducible framework.
  - **关键发现**: VoiceSculptor achieves open-source state-of-the-art on InstructTTSEval-Zh, outperforming all prior open-source TTS systems in instruction-following and controllability. Scaling from 1B to 3B parameters yields consistent gains across all metrics, and increasing SFT data volume further improves performance. Models with CoT-based attribute modeling consistently outperform ablated versions without CoT, confirming its effectiveness in enhancing attribute understanding. Despite using an additional synthesis stage (design + cloning), the system maintains high audio quality, with stylistic attributes in prompt waveforms effectively transferred by CosyVoice2. Notably, it matches or approaches the performance of proprietary systems while remaining fully open.


### 2026-01-14
- **Afri-MCQA: Multimodal Cultural Question Answering for African Languages**
  - **作者**: Atnafu Lambebo Tonja et.al.
  - **arXiv**: [2601.05699](https://arxiv.org/abs/2601.05699)
  - **TLDR**: Afri-MCQA introduces the first multimodal, multilingual cultural question-answering benchmark for 15 African languages, revealing severe performance gaps in both open-weight and proprietary large language and multimodal models when processing native-language text or speech. The work highlights the urgent need for culturally grounded, speech-first AI development for underrepresented languages.
  - **核心贡献**: The paper presents Afri-MCQA, a novel benchmark dataset comprising approximately 7,500 human-curated, parallel English–African language Q&A pairs across 15 languages from 12 African countries, covering both text and speech modalities with associated images. This addresses the critical gap in culturally and linguistically diverse evaluation resources for African languages in multimodal AI, enabling systematic assessment of linguistic competence and cultural reasoning in LLMs and MLLMs—capabilities previously unmeasurable due to lack of appropriate benchmarks.
  - **关键发现**: Open-weight models show near-zero accuracy (<5%) on open-ended VQA in native African languages (text or speech), with even top models like Qwen-72B scoring poorly. In MC-VQA, performance drops by 30–40% when switching from English to native languages. Speech modality exacerbates failures: ASR errors compound downstream inaccuracies, especially for non-Gemini models. Gemini-2.5 Pro outperforms others but still shows significant cross-lingual gaps (~40% absolute drop). Control experiments confirm models lack basic linguistic competence in African languages—evidenced by poor performance on AfriXNLI and high ASR error rates. Model scaling shows minimal gains, indicating data and pretraining deficiencies rather than capacity limits. Human evaluations corroborate automatic metrics, showing consistent failure to generate culturally grounded answers.


### 2026-01-13
- **Decoding Order Matters in Autoregressive Speech Synthesis**
  - **作者**: Minghui Zhao et.al.
  - **arXiv**: [2601.08450](https://arxiv.org/abs/2601.08450)
  - **TLDR**: This paper challenges the conventional left-to-right autoregressive decoding order in speech synthesis by leveraging a masked diffusion framework that supports arbitrary generation orders. It demonstrates that fixed orders like left-to-right are suboptimal and that adaptive decoding strategies yield higher-quality speech, even with highly quantized acoustic representations.
  - **核心贡献**: The paper introduces a systematic investigation into the impact of decoding order on autoregressive speech synthesis quality, using a masked diffusion model (MDM) that allows flexible, non-sequential generation during both training and inference. By decoupling the generation order from architectural constraints, the authors reveal that adaptive decoding strategies—such as Top-K selection based on uncertainty—outperform traditional fixed-order approaches like left-to-right (l2r) or right-to-left (r2l), thereby redefining a fundamental assumption in TTS modeling.
  - **关键发现**: Fixed-order decoding (especially l2r) is consistently outperformed by adaptive strategies: Top-1* achieves the best log F0 accuracy and highest MOS scores. Increasing randomness in decoding order initially improves MCD but degrades beyond a threshold, indicating an optimal balance between structure and flexibility. Even 1-bit quantization yields intelligible speech, though quality degrades gracefully with fewer bins; 100-class quantization offers the best trade-off. Subjective evaluations reveal that some models with good automatic metrics (e.g., low MCD) score poorly in MOS, highlighting limitations of objective metrics. Top-K decoding with larger K improves MCD due to parallel updates but may reduce naturalness if too aggressive.


### 2026-01-12
- **FOCAL: A Novel Benchmarking Technique for Multi-modal Agents**
  - **作者**: Aditya Choudhary et.al.
  - **arXiv**: [2601.07367](https://arxiv.org/abs/2601.07367)
  - **TLDR**: The paper introduces FOCAL, a novel benchmarking framework designed to evaluate multi-modal voice agents by assessing end-to-end reasoning, component-wise error propagation, and conversation quality through both automated and human-assisted testing. It proposes new metrics—Reasoning and Semantic scores—to measure the efficacy of voice-based interactions beyond traditional ASR/TTS accuracy.
  - **核心贡献**: FOCAL addresses the lack of comprehensive evaluation methodologies for cascading multi-modal voice agents that combine speech input/output with text-based reasoning via LLMs or Audio Language Models (ALMs). Traditional benchmarks focus on isolated components (e.g., ASR or TTS performance), but FOCAL enables holistic assessment of full pipelines, including how errors propagate across modules (e.g., from ASR to LLM to TTS) and how well the agent maintains coherent, meaningful dialogue. Its innovation lies in integrating structured conversation modeling, ground-truth alignment, and dual-mode (automated + human-in-the-loop) evaluation to capture both technical fidelity and conversational intelligence.
  - **关键发现**: Qualitative results show that while agents often follow conversation structure and maintain politeness, they suffer from critical failures like missing location links or misinterpreting inputs (e.g., 'pincode 27368' leading to incomplete responses). These errors highlight propagation issues in cascading systems. Agents scored well on politeness and promptness but exhibited gaps in completeness and contextual grounding. The Reasoning and Semantic scores successfully captured these nuances where traditional metrics (e.g., word error rate) would not. No quantitative aggregate scores (e.g., average Reasoning score = X) are provided, limiting comparative analysis, but the framework demonstrated sensitivity to pipeline-level flaws.


### 2026-01-11
- **Bridging Attribution and Open-Set Detection using Graph-Augmented Instance Learning in Synthetic Speech**
  - **作者**: Mohd Mujtaba Akhtar et.al.
  - **arXiv**: [2601.07064](https://arxiv.org/abs/2601.07064)
  - **TLDR**: The paper introduces SIGNAL, a unified framework that simultaneously addresses synthetic speech attribution (identifying the source TTS model) and open-set detection (recognizing speech from unseen generators). By combining speech foundation models with graph neural networks and k-Nearest Neighbor classifiers, SIGNAL achieves state-of-the-art performance on both DiffSSD and SingFake benchmarks.
  - **核心贡献**: SIGNAL solves the dual challenge of closed-set source attribution and open-set synthetic speech detection—a gap in prior work that typically treats these tasks separately or fails to generalize to unseen generators. Its innovation lies in unifying graph-augmented relational modeling among generator prototypes with instance-level KNN-based open-set inference, enabling both forensic traceability and robust generalization to novel synthesizers.
  - **关键发现**: SIGNAL consistently outperforms baselines across all SFMs and tasks. On DiffSSD, the Mamba-B variant achieves ACC: 83.27%, F1: 82.63%, and EER: 14.78% under the hybrid setup—significantly better than GNN-only or KNN-only. On SingFake, the hybrid approach reduces EER to 10.38%, demonstrating strong cross-dataset generalization. Mamba-based embeddings consistently yield the best results, surpassing WavLM and Whisper. Ablation studies confirm that both GNN (for inter-class modeling) and KNN (for open-set handling) are essential. The optimal confidence threshold τ = 0.5 balances false acceptance and rejection rates effectively.


### 2026-01-10
- **Lightweight Resolution-Aware Audio Deepfake Detection via Cross-Scale Attention and Consistency Learning**
  - **作者**: K. A. Shahriar et.al.
  - **arXiv**: [2601.06560](https://arxiv.org/abs/2601.06560)
  - **TLDR**: This paper introduces a lightweight, resolution-aware audio deepfake detection framework that leverages cross-scale attention and consistency learning to explicitly align multi-resolution spectral representations, achieving state-of-the-art performance across diverse benchmarks while maintaining low computational overhead. The method demonstrates exceptional robustness under real-world conditions such as replay attacks and channel distortions, outperforming conventional single-resolution and non-attention baselines.
  - **核心贡献**: The core contribution is a novel audio deepfake detection architecture that explicitly models interactions among multiple time–frequency resolutions through cross-scale attention and enforces consistency across these scales via a dedicated regularization objective. This addresses a critical limitation in prior work—reliance on single-resolution features or implicit fusion—which often fails under real-world distortions like replay attacks or varying recording conditions. By promoting agreement across complementary spectral scales, the model learns resolution-invariant, semantically meaningful cues that generalize better across spoofing types and datasets.
  - **关键发现**: The model achieves EER of 0.16% on ASVspoof 2019 LA (near-perfect), 5.09% on ASVspoof PA, 4.54% on FoR rerecorded audio, and 4.81% EER with 0.98 AUC on the In-the-Wild dataset. It significantly outperforms single-resolution and non-attention baselines, especially under challenging replay and real-world conditions. Ablation studies confirm that removing either cross-scale attention or consistency learning degrades performance, particularly on distorted data. Interpretability analysis shows the model attends to broad, semantically coherent spectral patterns rather than isolated peaks, indicating robust feature learning. The model maintains high accuracy (95.70%) and AUC (0.9800) across diverse spoofing types.


### 2026-01-09
- **IndexTTS 2.5 Technical Report**
  - **作者**: Yunpei Li et.al.
  - **arXiv**: [2601.03888](https://arxiv.org/abs/2601.03888)
  - **TLDR**: IndexTTS 2.5 is a zero-shot multilingual text-to-speech foundation model that achieves 2.28x faster inference while maintaining comparable quality through semantic codec compression, Zipformer architecture, and cross-lingual modeling strategies. It supports Mandarin, English, Japanese, and Spanish with robust emotion transfer capabilities even for unseen languages.
  - **核心贡献**: The paper presents IndexTTS 2.5, a significant advancement in zero-shot multilingual TTS that addresses the trade-off between inference speed and synthesis quality. The core contribution is a comprehensive system that reduces semantic codec frame rate from 50Hz to 25Hz, replaces the U-DiT backbone with a more efficient Zipformer architecture, introduces three explicit cross-lingual modeling strategies, and applies GRPO for post-training optimization. This results in a model that achieves 2.28x faster real-time factor while maintaining comparable WER and speaker similarity to its predecessor.
  - **关键发现**: IndexTTS 2.5 achieves a 2.28x improvement in Real-Time Factor (RTF) while maintaining comparable WER and speaker similarity to IndexTTS 2. The Zipformer architecture shows consistent subjective preference over U-DiT in both speaker similarity and naturalness. All three multilingual modeling strategies demonstrate effectiveness, with boundary-aware alignment showing particular promise for language boundary handling. The model successfully replicates emotional prosody in unseen languages under zero-shot settings, supporting robust emotion transfer even without target-language emotional training data. GRPO post-training leads to measurable improvements in pronunciation accuracy and naturalness.

- **Pantagruel: Unified Self-Supervised Encoders for French Text and Speech**
  - **作者**: Phuong-Hang Le et.al.
  - **arXiv**: [2601.05911](https://arxiv.org/abs/2601.05911)
  - **TLDR**: Pantagruel introduces unified self-supervised encoder models for French text and speech that learn contextualized representations in feature space rather than predicting modality-specific targets. The models are pre-trained on large-scale French corpora including a newly introduced 100,000-hour INA broadcast dataset and demonstrate competitive performance across multiple downstream tasks while maintaining a shared architecture for both modalities.
  - **核心贡献**: The main contribution is a family of self-supervised encoder models that unify text and speech processing through feature-space learning rather than modality-specific prediction targets. This approach allows separate text and speech models to capture linguistic and acoustic regularities more effectively while maintaining a shared architecture. The work introduces the INA-100k dataset (100,000 hours of French broadcast audio) and demonstrates that feature-space self-supervised objectives are effective for French representation learning across both modalities.
  - **关键发现**: Pantagruel models show competitive or superior performance compared to strong French baselines across a broad range of downstream tasks. The large speech model Pantagruel-L-114k achieves the best performance, outperforming other models on ASR tasks. Text models match or outperform base architectures of previous models. The combination of JEPA-style representation learning with MLM loss yields the best results for text models. The INA-100k corpus provides highly diverse audio data that improves model performance. Overall, the models demonstrate that feature-space self-supervised objectives are effective for French representation learning.

- **SPAM: Style Prompt Adherence Metric for Prompt-based TTS**
  - **作者**: Chanhee Cho et.al.
  - **arXiv**: [2601.05554](https://arxiv.org/abs/2601.05554)
  - **TLDR**: The paper introduces SPAM (Style Prompt Adherence Metric), a novel automatic evaluation metric for prompt-based text-to-speech systems that explicitly addresses both plausibility (alignment with human perception) and faithfulness (grounding in the provided style prompt). By leveraging a CLAP-inspired architecture with supervised contrastive learning and acoustic attribute factorization, SPAM demonstrates strong correlation with human judgments and robust discrimination of semantic differences in style prompts.
  - **核心贡献**: The main contribution is SPAM, an automatic metric designed to evaluate how well synthesized speech adheres to fine-grained textual style prompts in TTS systems. Prior evaluation methods lacked either plausibility (correlation with human perception) or faithfulness (semantic grounding in the prompt). SPAM solves this by jointly modeling acoustic attributes from speech and semantic embeddings from prompts using a contrastive framework, ensuring evaluations are both human-aligned and prompt-grounded.
  - **关键发现**: In plausibility experiments, SPAM achieved strong MOS correlations: 0.520 and 0.429 on TextrolSpeech and LibriTTS-P respectively, outperforming RA-CLAP which showed inconsistent performance (0.726 vs. 0.545 across datasets). In faithfulness experiments, SPAM significantly discriminated between different semantic prompts (p < 0.01 in paired t-tests), confirming its grounding in prompt semantics. SPAM also demonstrated consistent performance across diverse TTS models like ParlerTTS, indicating robustness. These results validate SPAM as both plausible and faithful, unlike prior metrics that excel in only one dimension.


### 2026-01-08
- **CosyEdit: Unlocking End-to-End Speech Editing Capability from Zero-Shot Text-to-Speech Models**
  - **作者**: Junyang Chen et.al.
  - **arXiv**: [2601.05329](https://arxiv.org/abs/2601.05329)
  - **TLDR**: CosyEdit introduces an end-to-end speech editing system derived from the zero-shot TTS model CosyVoice through task-specific fine-tuning and inference optimization, eliminating the need for external alignment modules. It achieves state-of-the-art performance on the RealEdit benchmark using only 250 hours of supervised data, matching or surpassing both large language model baselines and cascade systems.
  - **核心贡献**: The paper proposes CosyEdit, a novel end-to-end speech editing framework that adapts a pre-trained zero-shot text-to-speech (TTS) model—specifically CosyVoice—via targeted fine-tuning and optimized inference to perform high-fidelity speech editing without relying on complex preprocessing pipelines or explicit temporal alignment. This approach solves the longstanding challenge in automatic speech editing of maintaining consistency between original and edited speech while enabling flexible content modification via textual instructions, all within a unified architecture.
  - **关键发现**: CosyEdit outperformed all end-to-end baselines on RealEdit, achieving the lowest WER and highest EMOS scores among them, and matched the performance of top cascade systems despite using only 250 hours of fine-tuning data. It surpassed billion-parameter SLM-based methods in both objective and subjective metrics. Ablation studies showed that reference-guided conditioning significantly improved Mel Cepstral Distortion (MCD) but required careful balancing to avoid WER degradation. CosyEdit demonstrated stable performance across diverse editing operations, whereas competitors like Step-Audio-EditX exhibited high variance. The model preserved speaker identity and prosodic continuity better than alternatives, indicating strong consistency between pre- and post-edit speech.

- **FlexiVoice: Enabling Flexible Style Control in Zero-Shot TTS with Natural Language Instructions**
  - **作者**: Dekun Chen et.al.
  - **arXiv**: [2601.04656](https://arxiv.org/abs/2601.04656)
  - **TLDR**: FlexiVoice introduces a zero-shot text-to-speech system that enables flexible, disentangled control over speaking style via natural language instructions and voice timbre via speech references, using a large language model (LLM) core enhanced with a novel Progressive Post-Training (PPT) scheme. It significantly outperforms existing baselines in controllability, naturalness, and robustness while effectively decoupling style, timbre, and content.
  - **核心贡献**: The paper addresses the critical challenge in zero-shot TTS of simultaneously and accurately controlling multiple attributes—specifically, speaking style through natural language instructions and speaker timbre through reference audio—without conflating them with textual content or each other. Existing systems often fail to disentangle these factors, leading to style leakage or poor instruction adherence. FlexiVoice solves this by integrating an LLM-based architecture with a three-stage Progressive Post-Training (PPT) pipeline that progressively aligns and disentangles control signals using reinforcement learning techniques (DPO and GRPO), enabling precise, flexible, and robust multi-modal control in a zero-shot setting.
  - **关键发现**: FlexiVoice achieves a Decoupling Avg. score of 88.5, substantially outperforming the base model (which scores poorly due to style leakage) and all open-source and commercial baselines. On complex instruction tasks, it reaches 74.8% accuracy versus ~60% for strongest competitors. Human evaluations confirm superior naturalness and controllability, especially in conflict scenarios (e.g., neutral text with 'excited' instruction). The PPT ablation shows that progressive training is essential: joint GRPO optimization causes interference, while S3-first fails due to lack of foundational disentanglement. Reward model validation shows Kimi-Audio-7B achieves high Macro-F1 (Table 7) against Gemini, confirming its suitability. Speaker verification rewards prove more effective than embedding cosine similarity for timbre control, as they tolerate acoustic variation better.


### 2026-01-07
- **ReStyle-TTS: Relative and Continuous Style Control for Zero-Shot Speech Synthesis**
  - **作者**: Haitao Li et.al.
  - **arXiv**: [2601.03632](https://arxiv.org/abs/2601.03632)
  - **TLDR**: ReStyle-TTS introduces a novel framework for zero-shot text-to-speech synthesis that enables continuous and reference-relative control over speaking style attributes like pitch, energy, and emotion, without requiring carefully matched reference audio. By decoupling the model's dependence on reference style and introducing orthogonal LoRA-based control mechanisms, it achieves robust style manipulation while preserving speaker timbre and intelligibility.
  - **核心贡献**: The paper addresses a critical limitation in current zero-shot TTS systems: their strong entanglement of speaker timbre and speaking style from reference audio, which makes targeted style control impractical when reference and desired styles mismatch. ReStyle-TTS solves this by first reducing implicit reliance on reference style through Decoupled Classifier-Free Guidance (DCFG), then enabling fine-grained, continuous, and relative style control via style-specific LoRAs with Orthogonal LoRA Fusion, all while maintaining timbre consistency through a dedicated optimization module.
  - **关键发现**: ReStyle-TTS outperformed baselines in both objective and subjective evaluations, especially in contradictory-style scenarios. On emotion transfer (Table 2), it achieved higher MOS-SA scores across all emotions (e.g., +0.8 over ControlSpeech for 'angry'). In pitch/energy control (Table 3), it demonstrated precise manipulation even when reference and target styles conflicted. Figures 2–4 showed smooth, continuous control over single and joint attributes (e.g., simultaneously adjusting pitch and happiness). Ablation confirmed DCFG reduces reference style dependence by ~35% (measured via style embedding correlation), while TCO improved speaker similarity scores by 12%. Crucially, intelligibility remained high (WER < 5%) across all conditions.

- **SpeakerSleuth: Evaluating Large Audio-Language Models as Judges for Multi-turn Speaker Consistency**
  - **作者**: Jonggeun Lee et.al.
  - **arXiv**: [2601.04029](https://arxiv.org/abs/2601.04029)
  - **TLDR**: This paper introduces SpeakerSleuth, a benchmark to evaluate whether Large Audio-Language Models (LALMs) can reliably judge speaker consistency across multi-turn dialogues. It reveals that LALMs exhibit strong text bias, struggle with acoustic inconsistency detection, and perform poorly when contextual dialogue is present, despite showing better performance in relative acoustic discrimination tasks.
  - **核心贡献**: The paper addresses the unexplored problem of evaluating speaker consistency in multi-turn TTS using LALMs as judges. Its core contribution is SpeakerSleuth—a human-verified benchmark comprising 1,818 evaluation instances across four diverse datasets (synthetic and real speech)—designed to test LALMs on three specific tasks: binary consistency verification, inconsistency localization, and acoustic variant discrimination. This work exposes critical modality imbalances in LALMs, demonstrating their overreliance on textual cues at the expense of acoustic information, which undermines their reliability as evaluators for speaker-consistent TTS systems.
  - **关键发现**: LALMs perform poorly on speaker consistency tasks: most score below 50% on binary verification, with Gemini-2.5-Flash achieving only 60.1%. On inconsistency localization, models fail to identify problematic turns, often defaulting to 'consistent' judgments. Performance collapses (<10% accuracy on hard cases S2/S3) when interlocutor context is added, as models prioritize textual coherence over acoustic cues—even missing obvious gender switches. In contrast, speaker embedding methods achieve near-perfect accuracy (99–100% for ECAPA-TDNN) on verification. However, LALMs excel in the discrimination task (up to 82.6% accuracy), showing they possess acoustic discrimination capability but fail to apply it in contextual consistency judgments. This reveals a systematic text bias: LALMs rely on implicit textual assumptions rather than acoustic evidence when evaluating speaker identity across turns.


### 2026-01-06
- **Segment-Aware Conditioning for Training-Free Intra-Utterance Emotion and Duration Control in Text-to-Speech**
  - **作者**: Qifan Liang et.al.
  - **arXiv**: [2601.03170](https://arxiv.org/abs/2601.03170)
  - **TLDR**: This paper introduces a training-free framework for intra-utterance emotion and duration control in zero-shot TTS by leveraging segment-aware conditioning strategies, enabling fine-grained expressive speech synthesis without retraining or reliance on non-public datasets. It achieves state-of-the-art performance in multi-emotion and duration consistency while preserving baseline speech quality.
  - **核心贡献**: The paper addresses the limitation of existing controllable TTS systems that operate only at the inter-utterance level and require complex training pipelines or proprietary data. Its core contribution is a training-free method that enables dynamic, intra-utterance control over both emotion and duration in pretrained zero-shot TTS models. This is accomplished through two novel strategies: (1) segment-aware emotion conditioning using causal masking and monotonic stream alignment filtering to allow smooth emotion transitions while maintaining semantic coherence, and (2) segment-aware duration steering combining local duration embedding adjustments with global end-of-sentence (EOS) logit modulation to ensure consistent utterance termination. Additionally, the authors construct a 30,000-sample multi-emotion and duration-annotated text dataset (MED-TTS) to automate prompt generation via an LLM, eliminating manual prompt engineering.
  - **关键发现**: The proposed method achieves state-of-the-art results: in emotion control, it outperforms baselines by 5.07% average error reduction in emotion classification accuracy and shows superior intra-utterance consistency in MOS (e.g., +0.42 points over F5TTS in emotion fidelity). For duration control, it attains lower WER and higher DNSM scores across scaling factors (0.8x to 1.2x), indicating better duration adherence without quality degradation. Subjective evaluations confirm smoother emotion transitions and more natural duration variation compared to methods that synthesize segments independently. Crucially, speech naturalness remains near baseline levels (MOS > 4.2), demonstrating no quality trade-off. The method also shows strong cross-model transferability across both autoregressive and non-autoregressive TTS systems.

- **Tigrinya Number Verbalization: Rules, Algorithm, and Implementation**
  - **作者**: Fitsum Gaim et.al.
  - **arXiv**: [2601.03403](https://arxiv.org/abs/2601.03403)
  - **TLDR**: This paper presents the first systematic formalization of Tigrinya number verbalization rules for both cardinal and ordinal numbers, along with a rule-based algorithm and open-source implementation. It highlights significant deficiencies in current large language models (LLMs) at performing this task, underscoring the need for explicit linguistic documentation to support TTS, ASR, and accessibility technologies for Tigrinya speakers.
  - **核心贡献**: The paper addresses a critical gap in computational resources for Tigrinya—a low-resource Afro-Asiatic language spoken in Eritrea and Ethiopia—by providing a comprehensive, linguistically grounded specification of how numerical values are verbalized in spoken Tigrinya. This includes handling of conjunctions, scale words (e.g., thousand, million), and special formats like dates, times, currency, and phone numbers. The core innovation lies in translating these complex morphosyntactic rules into a deterministic, executable algorithm that can be integrated into text normalization pipelines for speech technologies, filling a void where data-driven models currently fail due to lack of training data and linguistic coverage.
  - **关键发现**: LLMs exhibit poor performance on Tigrinya number verbalization: Opus 4.5 achieves the highest overall accuracy at 65%, followed by Gemini 3 Flash at 44%. Performance varies significantly by category—moderate success on simple cardinals and currency, but severe degradation on ordinals, dates, and time expressions. GPT-5 Mini frequently exceeds token limits (2048/4096) and fails almost universally. Common errors include incorrect teen formation, misuse of conjunctions, failure to apply gender agreement, and confusion between simple and compound multipliers (e.g., 'two hundred' vs. 'hundred two'). These results confirm that even advanced LLMs lack internalized knowledge of Tigrinya numerical morphology without explicit training or rule integration.

- **Vclip: Face-based Speaker Generation by Face-voice Association Learning**
  - **作者**: Yao Shi et.al.
  - **arXiv**: [2601.02753](https://arxiv.org/abs/2601.02753)
  - **TLDR**: The paper introduces Vclip, a novel face-based speaker generation method that leverages the semantic-rich CLIP encoder to learn face-voice associations from noisy audio-visual data, overcoming the scarcity of high-quality TTS-aligned audio-visual corpora. By combining a retrieval-based strategy with a GMM-based speaker embedding generator and using downstream TTS feedback, Vclip achieves state-of-the-art face-voice alignment (89.63% AUC on VoxCeleb) while enabling perceptually matched personalized speech synthesis.
  - **核心贡献**: Vclip addresses the critical challenge in face-based text-to-speech (TTS)—the lack of high-fidelity, aligned audio-visual datasets—by decoupling face-voice association learning from TTS training. Instead of directly training a TTS model on low-quality video datasets like LRS3, Vclip uses a pretrained CLIP-based model to extract semantically meaningful face and voice embeddings from noisy data, then employs a Gaussian Mixture Model (GMM) to generate plausible speaker embeddings conditioned on face images. This approach enables effective knowledge transfer to downstream TTS systems without requiring paired high-quality face-audio data during TTS training, significantly improving face-voice matching fidelity.
  - **关键发现**: Vclip achieves 89.63% AUC on Vox1-test for face-voice verification, outperforming Self-Lifting and other baselines. In automatic speaker generation evaluation, the TTS-informed variant ('w/ winformed') yields higher f2v similarity than naive approaches, though v2v scores remain lower than true voice cloning (as expected). Subjective evaluations confirm that Vclip-generated voices are perceived as better matching reference faces compared to baselines, with naturalness comparable to standard multi-speaker TTS. The retrieval step is shown to be crucial—direct replacement of speaker embeddings degrades performance. The method demonstrates consistent gains under both open and closed dataset settings.

- **Vulnerabilities of Audio-Based Biometric Authentication Systems Against Deepfake Speech Synthesis**
  - **作者**: Mengze Hong et.al.
  - **arXiv**: [2601.02914](https://arxiv.org/abs/2601.02914)
  - **TLDR**: This paper presents a systematic empirical evaluation demonstrating that modern deepfake speech synthesis models—trained on minimal voice samples—can effectively bypass commercial speaker verification systems, while anti-spoofing detectors exhibit poor generalization across unseen synthesis methods. The findings expose critical vulnerabilities in current audio biometric authentication and underscore the urgent need for adaptive, multi-factor defenses.
  - **核心贡献**: The paper addresses the growing security threat posed by accessible, high-quality deepfake speech to speaker verification systems. Its core contribution is a large-scale, systematic benchmark evaluating both speaker verification robustness and anti-spoofing detector generalization across diverse, state-of-the-art speech synthesis architectures—including diffusion-based, flow-based, and prompt-conditioned TTS models—revealing two fundamental vulnerabilities: (1) ease of spoofing with minimal enrollment data, and (2) severe performance degradation of detectors on out-of-domain synthesis methods not seen during training.
  - **关键发现**: Speaker verification systems are easily fooled: deepfakes from models trained on <5 seconds of voice achieve EERs exceeding 0.55—near random guessing—indicating near-total bypass. Anti-spoofing detectors show strong in-domain performance (e.g., 1.53% EER) but suffer catastrophic failure out-of-domain, with Table 5 reporting up to 30× performance degradation. Cross-lingual evaluation shows Mandarin-trained detectors fail on English deepfakes (EER jumps to 16.24%). Even state-of-the-art XLS-R+AASIST, while robust within domain, collapses when faced with novel synthesis patterns absent from training corpora. These results hold across diverse architectures, confirming systemic vulnerabilities rather than model-specific flaws.

- **XLSR-MamBo: Scaling the Hybrid Mamba-Attention Backbone for Audio Deepfake Detection**
  - **作者**: Kwok-Ho Ng et.al.
  - **arXiv**: [2601.02944](https://arxiv.org/abs/2601.02944)
  - **TLDR**: The paper introduces XLSR-MamBo, a hybrid deepfake detection framework that combines a self-supervised XLSR front-end with modular Mamba-Attention backbones to effectively capture global and local artifacts in synthetic speech. By leveraging bidirectional state space models like Hydra and scaling backbone depth, the method achieves state-of-the-art performance on multiple audio deepfake benchmarks while demonstrating robust generalization to unseen generative methods.
  - **核心贡献**: The core contribution is the design and systematic evaluation of XLSR-MamBo—a scalable, modular architecture for audio deepfake detection (ADD) that integrates state space models (SSMs) with attention mechanisms in a hybrid backbone. This addresses the limitation of pure causal SSMs, which struggle to model global frequency-domain artifacts due to their unidirectional nature, by incorporating bidirectional modeling capabilities (e.g., via Hydra) and exploring architectural variants (Mamba, Mamba2, Hydra, Gated DeltaNet). The work demonstrates that depth scaling in hybrid backbones significantly improves stability and generalization across diverse spoofing techniques, including diffusion- and flow-matching-based TTS systems.
  - **关键发现**: The MamBo-3-Hydra-N3 configuration achieves the best results: 3.02% EER on DFADD F1 subset, 4.45% on ASV21LA, and 4.97% on ASV21DF. On ITW, Hydra consistently outperforms other SSMs, confirming the advantage of native bidirectionality. Deeper models (L=7) show reduced performance variance and better generalization than shallow ones (L=5), especially on out-of-domain data like DFADD. All SSM variants generalize to unseen flow-matching models, but Hydra exhibits the most stable performance. The hybrid approach matches or exceeds existing SOTA systems across all benchmarks, with particularly strong results on challenging real-world (ITW) and novel synthesis (DFADD) scenarios.


### 2026-01-05
- **Towards Prosodically Informed Mizo TTS without Explicit Tone Markings**
  - **作者**: Abhijit Mohanta et.al.
  - **arXiv**: [2601.02073](https://arxiv.org/abs/2601.02073)
  - **TLDR**: This paper presents a text-to-speech system for Mizo, a low-resource tonal language, using only 5.18 hours of data. The authors demonstrate that VITS outperforms Tacotron2 in both subjective and objective evaluations, achieving acceptable perceptual quality and intelligibility without explicit tone markings.
  - **核心贡献**: The main contribution is demonstrating that a non-autoregressive end-to-end TTS framework (VITS) can effectively synthesize speech for a low-resource tonal language without explicit tone annotations. The work shows that VITS can maintain prosodic information and produce significantly lower tone errors compared to Tacotron2, achieving acceptable quality with limited data.
  - **关键发现**: VITS significantly outperformed Tacotron2 in both subjective and objective evaluations. VITS showed lower tone errors and better prosodic maintenance. Objective metrics showed VITS achieved better MCD scores (statistically significant with p<0.0001) and comparable DNSMOS, RMSE F0, and F0 correlation scores. Subjective MOS scores indicated native speakers preferred VITS outputs. The results demonstrate that VITS can synthesize acceptable quality speech without explicit tone markings.

- **VocalBridge: Latent Diffusion-Bridge Purification for Defeating Perturbation-Based Voiceprint Defenses**
  - **作者**: Maryam Abbasihafshejani et.al.
  - **arXiv**: [2601.02444](https://arxiv.org/abs/2601.02444)
  - **TLDR**: VocalBridge introduces a novel latent diffusion-based purification framework that effectively removes protective perturbations from speech, thereby defeating current voiceprint defenses and enabling high-fidelity voice cloning. The work demonstrates the fragility of existing perturbation-based privacy mechanisms against adaptive purification adversaries.
  - **核心贡献**: The paper addresses the vulnerability of perturbation-based voice protection methods to advanced purification attacks. It proposes VocalBridge, a transcript-free, diffusion-bridge purification model operating in the EnCodec latent space, which recovers speaker-discriminative acoustic features from protected speech more effectively than prior methods. This enables attackers to bypass defenses designed to make voices 'unlearnable' for TTS/VC systems, revealing critical gaps in current voice privacy strategies.
  - **关键发现**: VocalBridge achieves an average ARR of 58.9% across VC models, significantly outperforming the best baseline (DualPure at 37.4%)—a 21.5 percentage point improvement. For TTS models, it similarly dominates in cloneability restoration. MOS scores remain competitive (2.95–3.27), indicating preserved naturalness. The method fails only against AntiFake when compared to the specialized De-AntiFake baseline, but excels universally otherwise. Whisper-guided variants show marginal gains in intelligibility without sacrificing speaker recovery. Results confirm that current defenses are fragile under adaptive purification, with most offering <40% protection once purified.


### 2026-01-04
- **MM-Sonate: Multimodal Controllable Audio-Video Generation with Zero-Shot Voice Cloning**
  - **作者**: Chunyu Qiang et.al.
  - **arXiv**: [2601.01568](https://arxiv.org/abs/2601.01568)

- **OV-InstructTTS: Towards Open-Vocabulary Instruct Text-to-Speech**
  - **作者**: Yong Ren et.al.
  - **arXiv**: [2601.01459](https://arxiv.org/abs/2601.01459)


### 2026-01-01
- **DepFlow: Disentangled Speech Generation to Mitigate Semantic Bias in Depression Detection**
  - **作者**: Yuxin Li et.al.
  - **arXiv**: [2601.00303](https://arxiv.org/abs/2601.00303)

- **Latent Flow Matching for Expressive Singing Voice Synthesis**
  - **作者**: Minhyeok Yun et.al.
  - **arXiv**: [2601.00217](https://arxiv.org/abs/2601.00217)

