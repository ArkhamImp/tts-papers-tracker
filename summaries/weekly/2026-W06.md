# TTS 论文周报
**周期**: 2026-01-28 至 2026-02-03 (第 6 周, 2026 年)
**生成时间**: 2026-02-05 10:56

## 概览
- **论文总数**: 15
- **主题分布**:
  - `合成`: 9
  - `表现力`: 6
  - `多语言`: 4
  - `零样本`: 1
  - `其他`: 1
  - `编辑`: 1

## 重点论文（分析摘要）
- **CoCoEmo: Composable and Controllable Human-Like Emotional TTS via Activation Steering** (2026-02-03)
  - **作者**: Siyi Wang et.al.
  - **arXiv**: [2602.03420](https://arxiv.org/abs/2602.03420)
  - **标签**: expressive, synthesis
  - **TLDR**: CoCoEmo introduces a systematic framework for composable and controllable emotional expression in hybrid text-to-speech (TTS) systems using activation steering, enabling mixed-emotion synthesis and handling of text-emotion mismatches without retraining. It demonstrates that emotional prosody is primarily generated in the speech language model (SLM) stage rather than the flow-matching acoustic module.
  - **核心贡献**: The paper addresses the limitation of existing expressive TTS systems that enforce a single, utterance-level emotion label, thereby failing to capture nuanced, mixed, or contextually misaligned emotional expressions found in human speech. CoCoEmo proposes the first systematic analysis of activation steering for emotional control in hybrid TTS architectures, introducing a lightweight, plug-and-play method that injects emotion-specific steering vectors into selected layers of the SLM. This enables fine-grained, composable emotional control—including synthesis of conflicting emotions and mismatched text-emotion scenarios—while preserving linguistic content and avoiding model retraining or architectural changes.
  - **方法**: Experiments are conducted on two English-language hybrid TTS models: CosyVoice2 (with SLM layers 10–17 optimal for steering) and IndexTTS2 (layers 6–9). Steering vectors are extracted from paired emotional speech datasets (CREMA-D, ESD, MELD) where the same speaker utters identical text under different emotions. Datasets are split into train (vector extraction), validation (steering site selection via linear probing), and test sets. Evaluation includes in-distribution (CREMA-D) and out-of-distribution (IEMOCAP) settings. Baselines include native instruction-based prompting (no internal activation modification) and unsteered model outputs. Objective metrics include E-SIM (emotion similarity via embedding cosine distance), TEP (text-emotion preservation measured by semantic consistency under emotion shift), valence-arousal correlation (ρ), human-likeness rate (H-Rate), speaker similarity (S-SIM), and word error rate (WER). Subjective evaluation uses Mean Opinion Score (MOS) with multiple raters. Mixed-emotion evaluation leverages multi-annotator soft labels to construct ground-truth emotion distributions. Text-emotion mismatch is quantified using ℓ2 distance between predicted and expected valence-arousal values under semantic conflict.
  - **关键发现**: Activation steering significantly improves emotional expressiveness: E-SIM increases by up to 0.15 over baselines, and TEP remains high (>0.85) even under strong emotion shifts, indicating preserved linguistic content. Mixed-emotion synthesis achieves higher MOS (up to 4.1 vs. 3.6 for baselines) and better alignment with human-annotated emotion distributions. Crucially, steering applied only to the SLM yields strong emotional control, while steering the flow-matching module produces negligible effects—confirming the SLM as the primary locus of emotional prosody generation. In high-mismatch IEMOCAP scenarios, CoCoEmo maintains naturalness (MOS ~3.9) and low WER (<5%), whereas baseline models exhibit unnatural or collapsed expressiveness. Both models show consistent gains across architectures, validating the generality of the approach.
  - **评估**: strong (评分: 9/10)

- **LipSody: Lip-to-Speech Synthesis with Enhanced Prosody Consistency** (2026-02-02)
  - **作者**: Jaejun Lee et.al.
  - **arXiv**: [2602.01908](https://arxiv.org/abs/2602.01908)
  - **标签**: expressive, synthesis
  - **TLDR**: LipSody is a novel lip-to-speech synthesis framework that significantly improves prosody consistency—such as pitch, energy, and speaker identity—by integrating visual cues from silent facial videos into a diffusion-based TTS pipeline. Unlike prior methods that prioritize intelligibility alone, LipSody explicitly models prosodic features derived from speaker identity, linguistic content, and emotional context, resulting in more natural and personalized synthesized speech.
  - **核心贡献**: The core contribution of LipSody is the introduction of a prosody-guiding strategy that enhances prosodic consistency in lip-to-speech synthesis by leveraging three complementary visual cues: speaker identity from facial images, linguistic content from lip movements, and emotional context from face video. This addresses a critical limitation in existing diffusion-based models like LipVoicer, which excel at reconstructing intelligible speech but produce prosodically inconsistent or unnatural outputs. By explicitly modeling pitch and energy dynamics within the diffusion generation process, LipSody achieves more expressive and speaker-consistent speech without sacrificing linguistic accuracy.
  - **方法**: The experiments use the LRS3 dataset, comprising 5,502 TED/TEDx videos, with standard pretrain, train, and test splits. The base architecture replicates LipVoicer’s diffusion setup, including a Conformer-based lip reader and HiFi-GAN vocoder. Three model variants are evaluated: (1) the official LipVoicer, (2) a reconstructed LipVoicer (LipVoicerrecon) for controlled comparison, and (3) the full LipSody model with prosody modeling. Objective metrics include Word Error Rate (WER) using an ASR model for intelligibility, and prosody-specific metrics such as global/local pitch deviation, energy consistency, and speaker similarity (Resem and Resemtv). Subjective evaluations include naturalness Mean Opinion Score (MOS) and ABX preference tests. Statistical significance is assessed via one-sample t-tests (p < 0.05). Ablation studies in Table 4 examine performance under different prosody information settings to validate the contribution of each cue.
  - **关键发现**: LipSody achieves comparable WER (21.9%) to LipVoicer, confirming preserved intelligibility. Crucially, it shows significant improvements in prosody-related metrics: reduced global and local pitch deviations, higher energy consistency, and improved speaker similarity scores (Resem and Resemtv). Subjective evaluations reveal statistically significant preference for LipSody in naturalness (MOS) and ABX tests (p < 0.05). Ablation studies confirm that removing any prosody cue degrades performance, with full integration yielding optimal results. Notably, better prosody modeling also correlates with slight gains in intelligibility, suggesting prosody aids linguistic reconstruction. The model demonstrates robustness across diverse speakers and emotional contexts in LRS3.
  - **评估**: strong (评分: 8/10)

- **EmoAra: Emotion-Preserving English Speech Transcription and Cross-Lingual Translation with Arabic Text-to-Speech** (2026-02-01)
  - **作者**: Besher Hassan et.al.
  - **arXiv**: [2602.01170](https://arxiv.org/abs/2602.01170)
  - **标签**: expressive, multilingual, synthesis
  - **TLDR**: EmoAra introduces an end-to-end emotion-preserving cross-lingual speech translation pipeline that converts English customer service speech into emotionally nuanced Arabic speech, integrating SER, ASR, MT, and TTS components. It demonstrates high performance in emotion classification (94% F1), translation (BLEU 56, BERTScore F1 88.7%), and human-rated output quality (81%) in a banking domain.
  - **核心贡献**: The paper presents EmoAra, a novel pipeline that preserves emotional context during English-to-Arabic spoken language translation—addressing a critical gap in customer service applications where emotional tone impacts user experience. Unlike conventional pipelines that treat translation and speech synthesis as emotion-agnostic steps, EmoAra explicitly incorporates emotion recognition early in the pipeline and ensures downstream components are compatible with domain-specific and emotional fidelity requirements, even if emotion is not directly injected into TTS prosody.
  - **方法**: Experiments used the RAVDESS dataset for SER training, covering 8 emotion classes. For MT, a custom English–Arabic banking corpus of ~24k sentence pairs was created by translating English banking scripts using Google Translate and MyMemoryTranslator. Baselines included: (a) a non-augmented CNN SER model without feature engineering or data augmentation, and (b) off-the-shelf MarianMT without fine-tuning. Evaluation metrics: F1-score per emotion class for SER; BLEU and BERTScore F1 for MT; and human evaluation (Likert-scale scoring) for translation fluency and domain appropriateness in Arabic TTS output. Ablation studies compared LSTM and ResNet50 against CNN for SER. Training employed early stopping, checkpointing, and hyperparameter sweeps (learning rate, batch size, epochs).
  - **关键发现**: The final CNN achieved 94% macro F1 for emotion classification, outperforming the baseline across all classes—though 'calm', 'fearful', and 'sad' remained challenging due to acoustic overlap. Fine-tuned MarianMT reached BLEU 56 and BERTScore F1 88.7%, a dramatic improvement over scratch-trained Transformer (BLEU 23) and un-fine-tuned MarianMT (BLEU 25.48). Human evaluators rated 81% of Arabic TTS outputs as appropriate and fluent in the banking context. The pipeline successfully handled average sentence lengths of 15–17 tokens but struggled with very long utterances due to truncation in training data.
  - **评估**: medium (评分: 7/10)

- **EmoShift: Lightweight Activation Steering for Enhanced Emotion-Aware Speech Synthesis** (2026-01-30)
  - **作者**: Li Zhou et.al.
  - **arXiv**: [2601.22873](https://arxiv.org/abs/2601.22873)
  - **标签**: expressive, synthesis
  - **TLDR**: EmoShift introduces a lightweight activation-steering framework called EmoSteer that enhances emotion-aware speech synthesis by learning emotion-specific steering vectors in the output embedding space, enabling precise emotional control without full model fine-tuning. It achieves superior performance over zero-shot and fully fine-tuned baselines with only 10M trainable parameters—less than 1/30 of full fine-tuning—while preserving naturalness and speaker identity.
  - **核心贡献**: The paper addresses the limitation of existing emotion-aware TTS systems that rely on scaling fixed emotion embeddings or external prompts, which restrict their ability to capture emotion-specific latent characteristics. EmoShift’s core innovation is the EmoSteer layer—a lightweight, plug-and-play module that learns emotion-specific offset vectors in the model’s activation space, enabling controllable, interpretable, and stable emotional expression across utterances without modifying or retraining the base TTS model.
  - **方法**: The experiments use the English subset of the Emotional Speech Dataset (ESD), covering five emotions including neutral. Two base models—CosyVoice and another unnamed LLM-based TTS—are used. Baselines include zero-shot emotion prompting and full fine-tuning (SFT). Evaluation metrics include Word Error Rate (WER) via ASR, Speaker Similarity (SpkSIM) using WavLM-Base, Emotion Recognition Accuracy via emotion2vec, Mean Opinion Score (MOS) for naturalness, and Emo-MOS for emotional expressiveness. Subjective evaluation involves 10 listeners rating preference between base and EmoShift-augmented outputs. Objective evaluations measure speech quality and emotion fidelity; controllability is tested by scaling the steering vector magnitude (α) and measuring resulting SER accuracy.
  - **关键发现**: EmoShift outperforms both zero-shot and fully fine-tuned (SFT) baselines across all metrics. In subjective evaluation, it achieves higher MOS and Emo-MOS scores, with listeners preferring EmoShift outputs in pairwise comparisons. Objectively, it improves emotion recognition accuracy—highest for Sad (61.24%) and Neutral (55.84%)—and maintains low WER and high SpkSIM, indicating preserved naturalness and speaker identity. Scaling the steering vector (α) enables smooth control of emotional intensity, validated by monotonic changes in SER accuracy. EmoShift matches or exceeds CosyVoice-SFT-Shift performance despite using <1/30 the trainable parameters.
  - **评估**: strong (评分: 8/10)

- **Evaluating and Rewarding LALMs for Expressive Role-Play TTS via Mean Continuation Log-Probability** (2026-01-30)
  - **作者**: Yong Ren et.al.
  - **arXiv**: [2601.22661](https://arxiv.org/abs/2601.22661)
  - **标签**: expressive, synthesis
  - **TLDR**: This paper introduces Mean Continuation Log-Probability (MCLP) as a novel metric and reinforcement learning reward to improve stylistic consistency in Large Audio Language Model (LALM)-based Role-Play Text-to-Speech (RP-TTS). By leveraging in-context learning of pretrained LALMs, MCLP quantifies how well generated speech aligns with character profiles and scene context, significantly outperforming strong baselines in both objective and subjective evaluations on a newly constructed multi-turn RP-TTS dataset.
  - **核心贡献**: The core contribution is the proposal of Mean Continuation Log-Probability (MCLP), which serves dual purposes: (1) as an objective evaluation metric for stylistic consistency in expressive role-play TTS, addressing the lack of reliable style quantification in prior work; and (2) as a reinforcement learning (RL) reward signal to fine-tune LALMs for better alignment with role-play instructions. The innovation lies in using the intrinsic likelihood modeling capability of pretrained LALMs—via in-context learning—to compute the log-probability of ground-truth speech continuations conditioned on model-generated speech, thereby capturing dynamic, context-sensitive stylistic coherence across multi-turn dialogues rather than treating style as static or attribute-based.
  - **方法**: The authors construct a new RP-TTS dataset called Drp from the WenetSpeech corpus, sourcing drama-tagged YouTube videos. The pipeline includes filtering for quality, using Qwen-VL-7B to generate objective scene descriptions (ignoring dialogue), and prompting LLMs to extract character profiles from full episode scripts. The final dataset contains ~100 hours of speech with detailed annotations. Evaluation uses both objective metrics (WER, Character Error Rate [CER], Pinyin Error Rate, MCLP) and subjective Mean Opinion Score (MOS) assessed by 32 native Chinese professional annotators on 31 curated samples. Baselines include GPT-Audio (closed-source), SFT-only LALM, and Audio-2-mini variants. Two test conditions are evaluated: with audio history (W. Audio History) and without (W/O. Audio History). Ablation studies examine the impact of SFT, GRPO, and reward components (MCLP vs. WER). Training uses 1 epoch of SFT followed by GRPO with advantage estimation relative to the SFT reference policy.
  - **关键发现**: The proposed method achieves state-of-the-art performance: MOS of 3.646 (95% CI reported), approaching ground-truth naturalness, and significantly outperforms all baselines in both settings. It attains the lowest CER and Pinyin error rates, indicating high phonetic fidelity. MCLP scores confirm superior stylistic consistency. Notably, while baseline models degrade without audio history, the proposed method maintains robust performance, demonstrating effective use of textual context alone. Ablation shows that combining MCLP and WER in GRPO is critical—using only WER leads to reward hacking (e.g., generating bland speech to minimize errors), while MCLP-only yields unsafe outputs. The full pipeline (SFT + GRPO with combined reward) achieves the best balance. Audio-2-mini shows significant performance drops in role-play scenarios, highlighting the need for instruction-aligned training.
  - **评估**: strong (评分: 8/10)

- **Erasing Your Voice Before It's Heard: Training-free Speaker Unlearning for Zero-shot Text-to-Speech** (2026-01-28)
  - **作者**: Myungjin Lee et.al.
  - **arXiv**: [2601.20481](https://arxiv.org/abs/2601.20481)
  - **标签**: zero-shot, synthesis
  - **TLDR**: The paper introduces TruS, a training-free speaker unlearning framework for zero-shot text-to-speech (TTS) systems that prevents unauthorized voice synthesis by suppressing identity-specific activations during inference. Unlike prior retraining-based methods, TruS works on both seen and unseen speakers without modifying the model, offering a scalable privacy safeguard.
  - **核心贡献**: TruS addresses the critical privacy risk in modern zero-shot TTS models—unauthorized voice cloning—by enabling speaker unlearning without any retraining. It innovates by shifting from data-centric deletion to inference-time control, manipulating internal hidden activations to erase target speaker identities while preserving non-identity attributes like prosody and emotion. This is the first method to generalize unlearning to speakers not present in the original training data (unseen opt-out speakers), overcoming a major limitation of existing approaches.
  - **方法**: Experiments use F5-TTS pretrained on the Emilia dataset (retain set). Opt-out speakers are drawn from LibriSpeech: seen opt-out (SO) speakers are those in Emilia but marked for removal; unseen opt-out (UO) are entirely absent from training. Evaluation metrics include speaker similarity (using SV model cosine similarity), Word Error Rate (WER) via ASR to assess intelligibility, and emotion preservation scores on an emotional speech dataset. Baselines include fine-tuned F5-TTS (F5-TTS-FT) and retraining-based unlearning methods SGU and TGU. Ablation studies examine layer selection strategies, number of retain speakers (N=30 chosen), and pool size for ID-prototypes. All tests are zero-shot: no adaptation to target speakers.
  - **关键发现**: TruS reduces speaker similarity for seen opt-out speakers by ~80% compared to baseline F5-TTS, matching the performance of retrained baselines (e.g., F5-TTS-FT) without any training. Crucially, it achieves significant suppression on unseen opt-out speakers (UO), where retraining methods fail entirely. WER remains stable (<2% increase), confirming preserved intelligibility. Emotion preservation scores show minimal degradation, indicating attribute retention. The 'µ + σ' layer selection yields optimal balance between unlearning strength and speech quality. Performance scales with larger retain sets (N=30 optimal).
  - **评估**: strong (评分: 9/10)

- **ASR for Affective Speech: Investigating Impact of Emotion and Speech Generative Strategy** (2026-01-28)
  - **作者**: Ya-Tse Wu et.al.
  - **arXiv**: [2601.20319](https://arxiv.org/abs/2601.20319)
  - **标签**: expressive
  - **TLDR**: This paper investigates how emotional speech synthesized by different TTS models affects ASR performance, revealing that substitution errors dominate and vary across models. The authors propose two novel generative strategies—based on transcription correctness and emotional salience—to curate fine-tuning data, achieving consistent WER improvements on real emotional speech datasets without degrading performance on neutral speech.
  - **核心贡献**: The work addresses the challenge of degraded ASR performance on emotionally expressive speech by analyzing how different emotional TTS systems influence error patterns. It introduces a targeted data curation framework for fine-tuning ASR models using synthetic emotional speech, guided by two novel selection criteria: one prioritizing utterances with high transcription fidelity (low WER) and another emphasizing high emotional salience (measured via arousal/valence). This enables building emotion-aware ASR systems that generalize to real emotional speech while preserving robustness on clean, neutral data.
  - **方法**: The methodology involves generating 30,000 synthetic emotional utterances using three TTS systems (EmoVoice, CosyVoice2, MaskGCT) conditioned on emotion labels. Emotional salience is quantified using a WavLM-based multitask regressor trained to predict arousal and valence. ASR performance is first evaluated on these synthetic sets to characterize error types (substitutions, deletions, insertions). Then, two subset selection strategies are applied to construct fine-tuning corpora. The ASR model (Qwen2-audio-7B) is fine-tuned under a parameter-efficient regime. Evaluation is conducted on three held-out real emotional datasets: MSP-Podcast Test1, Test2 (English podcast clips with dimensional emotion annotations), and IEMOCAP (dyadic acted conversations with categorical and dimensional labels). No real emotional data is used in training. Metrics include WER overall and stratified by emotional expressiveness. Baselines include vanilla fine-tuning on unfiltered synthetic data and the original pretrained model.
  - **关键发现**: All emotional TTS datasets degrade ASR performance compared to neutral speech, with substitution errors being dominant. Among TTS models, MaskGCT yields the lowest WER on synthetic data (4.40%). Fine-tuning with TTS-EMO-G reduces WER by up to 12.3% relative on MSP-Podcast Test2 and 9.8% on IEMOCAP compared to vanilla fine-tuning. Crucially, performance on LibriSpeech (neutral) remains stable (WER change < 0.1%), confirming no degradation. Gains are most pronounced in high-arousal/high-valence regions. The combined TTS-EMO-G strategy consistently outperforms individual strategies across all benchmarks, achieving the best average WER of 14.2% on real emotional datasets.
  - **评估**: strong (评分: 8/10)

## 完整列表（按日期）
### 2026-02-03
- **CoCoEmo: Composable and Controllable Human-Like Emotional TTS via Activation Steering**
  - **作者**: Siyi Wang et.al.
  - **arXiv**: [2602.03420](https://arxiv.org/abs/2602.03420)
  - **TLDR**: CoCoEmo introduces a systematic framework for composable and controllable emotional expression in hybrid text-to-speech (TTS) systems using activation steering, enabling mixed-emotion synthesis and handling of text-emotion mismatches without retraining. It demonstrates that emotional prosody is primarily generated in the speech language model (SLM) stage rather than the flow-matching acoustic module.
  - **核心贡献**: The paper addresses the limitation of existing expressive TTS systems that enforce a single, utterance-level emotion label, thereby failing to capture nuanced, mixed, or contextually misaligned emotional expressions found in human speech. CoCoEmo proposes the first systematic analysis of activation steering for emotional control in hybrid TTS architectures, introducing a lightweight, plug-and-play method that injects emotion-specific steering vectors into selected layers of the SLM. This enables fine-grained, composable emotional control—including synthesis of conflicting emotions and mismatched text-emotion scenarios—while preserving linguistic content and avoiding model retraining or architectural changes.
  - **关键发现**: Activation steering significantly improves emotional expressiveness: E-SIM increases by up to 0.15 over baselines, and TEP remains high (>0.85) even under strong emotion shifts, indicating preserved linguistic content. Mixed-emotion synthesis achieves higher MOS (up to 4.1 vs. 3.6 for baselines) and better alignment with human-annotated emotion distributions. Crucially, steering applied only to the SLM yields strong emotional control, while steering the flow-matching module produces negligible effects—confirming the SLM as the primary locus of emotional prosody generation. In high-mismatch IEMOCAP scenarios, CoCoEmo maintains naturalness (MOS ~3.9) and low WER (<5%), whereas baseline models exhibit unnatural or collapsed expressiveness. Both models show consistent gains across architectures, validating the generality of the approach.


### 2026-02-02
- **LipSody: Lip-to-Speech Synthesis with Enhanced Prosody Consistency**
  - **作者**: Jaejun Lee et.al.
  - **arXiv**: [2602.01908](https://arxiv.org/abs/2602.01908)
  - **TLDR**: LipSody is a novel lip-to-speech synthesis framework that significantly improves prosody consistency—such as pitch, energy, and speaker identity—by integrating visual cues from silent facial videos into a diffusion-based TTS pipeline. Unlike prior methods that prioritize intelligibility alone, LipSody explicitly models prosodic features derived from speaker identity, linguistic content, and emotional context, resulting in more natural and personalized synthesized speech.
  - **核心贡献**: The core contribution of LipSody is the introduction of a prosody-guiding strategy that enhances prosodic consistency in lip-to-speech synthesis by leveraging three complementary visual cues: speaker identity from facial images, linguistic content from lip movements, and emotional context from face video. This addresses a critical limitation in existing diffusion-based models like LipVoicer, which excel at reconstructing intelligible speech but produce prosodically inconsistent or unnatural outputs. By explicitly modeling pitch and energy dynamics within the diffusion generation process, LipSody achieves more expressive and speaker-consistent speech without sacrificing linguistic accuracy.
  - **关键发现**: LipSody achieves comparable WER (21.9%) to LipVoicer, confirming preserved intelligibility. Crucially, it shows significant improvements in prosody-related metrics: reduced global and local pitch deviations, higher energy consistency, and improved speaker similarity scores (Resem and Resemtv). Subjective evaluations reveal statistically significant preference for LipSody in naturalness (MOS) and ABX tests (p < 0.05). Ablation studies confirm that removing any prosody cue degrades performance, with full integration yielding optimal results. Notably, better prosody modeling also correlates with slight gains in intelligibility, suggesting prosody aids linguistic reconstruction. The model demonstrates robustness across diverse speakers and emotional contexts in LRS3.

- **WAXAL: A Large-Scale Multilingual African Language Speech Corpus**
  - **作者**: Abdoulaye Diack et.al.
  - **arXiv**: [2602.02734](https://arxiv.org/abs/2602.02734)
  - **TLDR**: The paper introduces WAXAL, a large-scale, open-access multilingual speech corpus covering 21 Sub-Saharan African languages, comprising 1,250 hours of transcribed natural speech for ASR and over 180 hours of high-quality single-speaker recordings for TTS. It addresses the critical data gap for under-resourced African languages by prioritizing ethical collection, local partnerships, and phonetic balance to enable inclusive speech technology development.
  - **核心贡献**: WAXAL provides the first large-scale, unified, and openly licensed speech dataset specifically designed to support both ASR and TTS systems across 21 African languages—many of which have been historically excluded from speech technology research due to lack of data. The core innovation lies not in novel algorithms but in the systematic, ethically grounded creation of foundational data infrastructure that enables future modeling work. By including both spontaneous speech (for ASR) and studio-quality, phonetically balanced read speech (for TTS), it supports end-to-end development of speech technologies while ensuring speaker diversity, linguistic coverage, and community involvement through partnerships with four African academic and community organizations.
  - **关键发现**: The final release includes 1,250 hours of transcribed ASR data across 14 languages and 180+ hours of TTS data across 10 languages, collectively representing over 100 million speakers. Language distributions are uneven but reflect population sizes and feasibility (e.g., Swahili, Yoruba, and Hausa have larger allocations). The TTS recordings meet industry standards for synthesis training, with low noise and consistent prosody. The ASR data exhibits high lexical and acoustic diversity due to spontaneous elicitation. Compared to existing resources—such as Mozilla Common Voice (which has sparse African language coverage) or radio-based archives (which lack transcripts and speaker metadata)—WAXAL offers superior scale, structure, and suitability for modern neural TTS/ASR pipelines. No quantitative model performance is reported, as the paper is a data release.


### 2026-02-01
- **Bias in the Ear of the Listener: Assessing Sensitivity in Audio Language Models Across Linguistic, Demographic, and Positional Variations**
  - **作者**: Sheng-Lun Wei et.al.
  - **arXiv**: [2602.01030](https://arxiv.org/abs/2602.01030)
  - **TLDR**: This paper presents the first systematic study of bias in multilingual speech-integrated large language models (MLLMs), introducing the BiasInEar dataset—a 70.8-hour, gender- and accent-balanced speech benchmark across English, Chinese, and Korean—and evaluating nine models under linguistic, demographic, and positional perturbations. It reveals that MLLMs are highly sensitive to language and option order but relatively robust to gender and accent, establishing a unified framework for fairness and robustness evaluation in audio-language systems.
  - **核心贡献**: The work addresses the critical gap in evaluating fairness and robustness of multimodal language models when processing spoken input rather than text. By constructing BiasInEar—a carefully curated, speech-augmented version of Global MMLU Lite with controlled variations in language, accent, gender, and answer option order—the authors enable systematic assessment of how speech modality amplifies or mitigates biases inherent in text-based LLMs. The core innovation lies in bridging text- and speech-based evaluation through a standardized, multilingual, and demographically balanced benchmark coupled with four complementary metrics that capture accuracy, uncertainty, sensitivity, and inter-rater consistency.
  - **关键发现**: Key quantitative results include: (1) All models show significant performance drops under option-order reversal, with APES values consistently higher for this perturbation than for gender or accent changes—e.g., Voxtral models exhibit up to 12% accuracy drop in Korean when options are reversed. (2) Language has the strongest effect: average accuracy drops from ~72% in English to ~58% in Korean across models. (3) Gender and accent show minimal impact: accuracy differences <2% between male/female speakers and native/non-native accents within each language. (4) Larger models (e.g., Gemma 120B) achieve higher Fleiss’ κ (>0.65) and lower APES (<0.15), indicating greater robustness. (5) End-to-end models outperform pipeline models in consistency (higher κ) despite similar accuracy, suggesting joint audio-language training stabilizes predictions. (6) Reasoning complexity (e.g., CoT) does not consistently improve robustness and sometimes increases APES. Qualitatively, models exhibit high entropy in low-resource languages (Korean), reflecting uncertainty, and structural biases (option order) dominate over demographic ones—a contrast to findings in pure text-based bias studies.

- **EmoAra: Emotion-Preserving English Speech Transcription and Cross-Lingual Translation with Arabic Text-to-Speech**
  - **作者**: Besher Hassan et.al.
  - **arXiv**: [2602.01170](https://arxiv.org/abs/2602.01170)
  - **TLDR**: EmoAra introduces an end-to-end emotion-preserving cross-lingual speech translation pipeline that converts English customer service speech into emotionally nuanced Arabic speech, integrating SER, ASR, MT, and TTS components. It demonstrates high performance in emotion classification (94% F1), translation (BLEU 56, BERTScore F1 88.7%), and human-rated output quality (81%) in a banking domain.
  - **核心贡献**: The paper presents EmoAra, a novel pipeline that preserves emotional context during English-to-Arabic spoken language translation—addressing a critical gap in customer service applications where emotional tone impacts user experience. Unlike conventional pipelines that treat translation and speech synthesis as emotion-agnostic steps, EmoAra explicitly incorporates emotion recognition early in the pipeline and ensures downstream components are compatible with domain-specific and emotional fidelity requirements, even if emotion is not directly injected into TTS prosody.
  - **关键发现**: The final CNN achieved 94% macro F1 for emotion classification, outperforming the baseline across all classes—though 'calm', 'fearful', and 'sad' remained challenging due to acoustic overlap. Fine-tuned MarianMT reached BLEU 56 and BERTScore F1 88.7%, a dramatic improvement over scratch-trained Transformer (BLEU 23) and un-fine-tuned MarianMT (BLEU 25.48). Human evaluators rated 81% of Arabic TTS outputs as appropriate and fluent in the banking context. The pipeline successfully handled average sentence lengths of 15–17 tokens but struggled with very long utterances due to truncation in training data.

- **VividVoice: A Unified Framework for Scene-Aware Visually-Driven Speech Synthesis**
  - **作者**: Chengyuan Ma et.al.
  - **arXiv**: [2602.02591](https://arxiv.org/abs/2602.02591)
  - **TLDR**: VividVoice introduces a novel task—Scene-Aware Visually-Driven Speech Synthesis—and proposes a unified framework that jointly models visual scenes, speaker identity, and environmental acoustics to generate immersive, contextually aligned speech. It addresses data scarcity and modality decoupling through a new large-scale dataset (Vivid-210K) and a Decoupled Multi-Scene Visual-Audio (D-MSVA) alignment module, significantly outperforming existing baselines in fidelity, clarity, and multimodal consistency.
  - **核心贡献**: The paper defines and tackles the new challenge of generating speech that is not only driven by visual input but also aware of the physical acoustic environment and speaker identity. Existing TTS and audio generation models either ignore environmental acoustics or fail to maintain consistent speaker identity across varying scenes. VividVoice solves this by creating the first large-scale, high-quality multimodal dataset with strong cross-modal alignment between visual scenes, speaker identity, and corresponding audio, and by introducing a novel alignment architecture (D-MSVA) that enables fine-grained disentanglement and control over timbre and environmental acoustic features.
  - **关键发现**: VividVoice achieved a MOS of 3.08 in subjective evaluation, significantly higher than baselines. In A/B preference tests, it was preferred 68% of the time over the stronger Attn-Fusion baseline. Objective metrics showed lower Word Error Rates and higher audio fidelity. Ablation studies confirmed D-MSVA’s contribution, with performance dropping when the decoupled memory bank or contrastive supervision was removed. The model demonstrated strong decoupling ability—users could independently vary speaker identity and environmental acoustics without interference. The Vivid-210K dataset itself was validated with 98.6% automated alignment accuracy and high subjective quality ratings from domain experts.


### 2026-01-31
- **Edit Content, Preserve Acoustics: Imperceptible Text-Based Speech Editing via Self-Consistency Rewards**
  - **作者**: Yong Ren et.al.
  - **arXiv**: [2602.00560](https://arxiv.org/abs/2602.00560)
  - **TLDR**: This paper introduces a novel text-based speech editing framework that decouples semantic content editing from acoustic rendering to achieve imperceptible edits while preserving speaker identity, prosody, and naturalness. By leveraging a pre-trained TTS model as an implicit critic within a reinforcement learning setup using Self-Consistency Rewards and Group Relative Policy Optimization (GRPO), the method significantly outperforms existing autoregressive and non-autoregressive baselines in intelligibility, robustness, and perceptual quality.
  - **核心贡献**: The core contribution is a principled 'Edit Content, Preserve Acoustics' framework that addresses the fundamental limitation of prior text-based speech editing methods: the entanglement of linguistic content and acoustic style in the waveform or codec space. This entanglement causes instability, boundary artifacts, and speaker drift during editing. The proposed solution decouples editing into a semantic token space for content modification and uses a Flow Matching decoder for high-fidelity acoustic reconstruction, guided by a self-consistency reward derived from a frozen pre-trained TTS model to ensure contextual and perceptual alignment.
  - **关键发现**: The proposed method achieved state-of-the-art results across all metrics: lowest WER (indicating highest intelligibility), highest DNSMOS and subjective MOS (superior naturalness), and best speaker similarity preservation—especially under long-duration edits where baselines degraded significantly. In substitution tasks, it matched or exceeded NAR performance; in insertion tasks, it vastly outperformed both AR (which suffered from boundary artifacts) and NAR (which often predicted silence). GRPO alignment provided substantial gains in naturalness and stability without harming speaker similarity. The method showed minimal degradation in performance even with extended masked regions, demonstrating robust long-context modeling.


### 2026-01-30
- **EmoShift: Lightweight Activation Steering for Enhanced Emotion-Aware Speech Synthesis**
  - **作者**: Li Zhou et.al.
  - **arXiv**: [2601.22873](https://arxiv.org/abs/2601.22873)
  - **TLDR**: EmoShift introduces a lightweight activation-steering framework called EmoSteer that enhances emotion-aware speech synthesis by learning emotion-specific steering vectors in the output embedding space, enabling precise emotional control without full model fine-tuning. It achieves superior performance over zero-shot and fully fine-tuned baselines with only 10M trainable parameters—less than 1/30 of full fine-tuning—while preserving naturalness and speaker identity.
  - **核心贡献**: The paper addresses the limitation of existing emotion-aware TTS systems that rely on scaling fixed emotion embeddings or external prompts, which restrict their ability to capture emotion-specific latent characteristics. EmoShift’s core innovation is the EmoSteer layer—a lightweight, plug-and-play module that learns emotion-specific offset vectors in the model’s activation space, enabling controllable, interpretable, and stable emotional expression across utterances without modifying or retraining the base TTS model.
  - **关键发现**: EmoShift outperforms both zero-shot and fully fine-tuned (SFT) baselines across all metrics. In subjective evaluation, it achieves higher MOS and Emo-MOS scores, with listeners preferring EmoShift outputs in pairwise comparisons. Objectively, it improves emotion recognition accuracy—highest for Sad (61.24%) and Neutral (55.84%)—and maintains low WER and high SpkSIM, indicating preserved naturalness and speaker identity. Scaling the steering vector (α) enables smooth control of emotional intensity, validated by monotonic changes in SER accuracy. EmoShift matches or exceeds CosyVoice-SFT-Shift performance despite using <1/30 the trainable parameters.

- **Evaluating and Rewarding LALMs for Expressive Role-Play TTS via Mean Continuation Log-Probability**
  - **作者**: Yong Ren et.al.
  - **arXiv**: [2601.22661](https://arxiv.org/abs/2601.22661)
  - **TLDR**: This paper introduces Mean Continuation Log-Probability (MCLP) as a novel metric and reinforcement learning reward to improve stylistic consistency in Large Audio Language Model (LALM)-based Role-Play Text-to-Speech (RP-TTS). By leveraging in-context learning of pretrained LALMs, MCLP quantifies how well generated speech aligns with character profiles and scene context, significantly outperforming strong baselines in both objective and subjective evaluations on a newly constructed multi-turn RP-TTS dataset.
  - **核心贡献**: The core contribution is the proposal of Mean Continuation Log-Probability (MCLP), which serves dual purposes: (1) as an objective evaluation metric for stylistic consistency in expressive role-play TTS, addressing the lack of reliable style quantification in prior work; and (2) as a reinforcement learning (RL) reward signal to fine-tune LALMs for better alignment with role-play instructions. The innovation lies in using the intrinsic likelihood modeling capability of pretrained LALMs—via in-context learning—to compute the log-probability of ground-truth speech continuations conditioned on model-generated speech, thereby capturing dynamic, context-sensitive stylistic coherence across multi-turn dialogues rather than treating style as static or attribute-based.
  - **关键发现**: The proposed method achieves state-of-the-art performance: MOS of 3.646 (95% CI reported), approaching ground-truth naturalness, and significantly outperforms all baselines in both settings. It attains the lowest CER and Pinyin error rates, indicating high phonetic fidelity. MCLP scores confirm superior stylistic consistency. Notably, while baseline models degrade without audio history, the proposed method maintains robust performance, demonstrating effective use of textual context alone. Ablation shows that combining MCLP and WER in GRPO is critical—using only WER leads to reward hacking (e.g., generating bland speech to minimize errors), while MCLP-only yields unsafe outputs. The full pipeline (SFT + GRPO with combined reward) achieves the best balance. Audio-2-mini shows significant performance drops in role-play scenarios, highlighting the need for instruction-aligned training.

- **Now You Hear Me: Audio Narrative Attacks Against Large Audio-Language Models**
  - **作者**: Ye Yu et.al.
  - **arXiv**: [2601.23255](https://arxiv.org/abs/2601.23255)
  - **TLDR**: This paper introduces 'Audio Narrative Attacks,' a novel jailbreak technique that embeds harmful instructions within narrative-style synthetic speech to bypass safety mechanisms in large audio-language models (LALMs). By leveraging prosodic and stylistic cues via advanced TTS, the method achieves a 98.26% success rate on models like Gemini 2.0 Flash—far exceeding text-only attacks—highlighting critical vulnerabilities in speech-based AI systems.
  - **核心贡献**: The paper identifies and exploits a previously uncharacterized vulnerability in end-to-end large audio-language models: their susceptibility to disallowed content when delivered through narratively structured, prosodically modulated synthetic speech. Unlike traditional text-based jailbreaks, this approach leverages paralinguistic features—such as tone, pacing, and vocal affect—to manipulate model compliance without altering lexical content. The core innovation lies in treating speech not just as a carrier of text but as a modality with its own semantic and behavioral influence, thereby circumventing alignment safeguards calibrated primarily for textual inputs.
  - **关键发现**: The audio narrative attack achieves a 98.26% success rate on Gemini 2.0 Flash, vastly outperforming text-only baselines (which show near-zero success due to robust textual filters). On GPT-4o and Qwen2.5-Omni, the method also shows significant gains over AdvWave and other audio jailbreaks. Tone-only manipulation (without adversarial wording) yields a 10–20% compliance boost across models, proving prosody alone influences behavior. Human speech experiments confirm the effect generalizes beyond synthetic voices. Common failure modes include premature termination (43% of cases) and internal representation instability, especially in compact models like Qwen2.5-Omni. The results indicate that current safety mechanisms fail to account for paralinguistic cues in raw audio inputs.


### 2026-01-29
- **Speech Quality-Based Localization of Low-Quality Speech and Text-to-Speech Synthesis Artefacts**
  - **作者**: Michael Kuhlmann et.al.
  - **arXiv**: [2601.21886](https://arxiv.org/abs/2601.21886)
  - **TLDR**: This paper introduces a method to enhance the interpretability of speech quality assessment (SQA) models by regularizing utterance-level predictors with a segment-based consistency constraint, enabling reliable frame-level quality scoring. The approach is validated through two applications: detecting partial spoofing attacks and localizing synthesis artefacts in state-of-the-art TTS systems, with listening tests confirming that low-scoring segments are perceptually degraded.
  - **核心贡献**: The main contribution is a regularization technique that enforces temporal consistency in frame-level predictions derived from utterance-level SQA models, thereby reducing stochasticity and improving localization accuracy without requiring frame-level ground truth labels. This bridges the gap between holistic quality scoring and interpretable, fine-grained error detection—addressing a key limitation in existing SQA systems that lack explainability for their predictions.
  - **关键发现**: On PartialSpoof, models with consistency loss achieved higher F1-scores (e.g., up to ~0.65 vs. ~0.55 without), demonstrating improved localization. On BVCC, Spearman rank correlation (SRCC) improved slightly with consistency regularization, indicating maintained utterance-level performance. Listening tests showed that segments with low frame-level scores were rated as poor quality significantly more often than random controls (e.g., >60% vs. <40% poor ratings). The model was particularly sensitive to glitches, discontinuities, and non-verbal vocalizations misaligned with linguistic content. However, precision-recall trade-offs were observed: higher thresholds increased precision but reduced recall.

- **Unit-Based Agent for Semi-Cascaded Full-Duplex Dialogue Systems**
  - **作者**: Haoyuan Yu et.al.
  - **arXiv**: [2601.20230](https://arxiv.org/abs/2601.20230)
  - **TLDR**: This paper introduces a unit-based agent framework for semi-cascaded full-duplex dialogue systems that decomposes conversations into minimal conversational units, enabling real-time, train-free, plug-and-play interaction using a multimodal large language model (MLLM) with auxiliary modules like VAD and TTS. The system achieves competitive performance on the HumDial dataset, ranking second in the Human-like Spoken Dialogue Systems Challenge (Track 2).
  - **核心贡献**: The core contribution is a novel framework for full-duplex spoken dialogue that structures interactions around minimal conversational units—discrete segments of speech that can be processed independently—allowing the system to dynamically decide when to transition between listening and speaking. This addresses the challenge of natural turn-taking and low-latency response generation in real-time human-computer dialogue without requiring end-to-end training. By leveraging a pre-trained multimodal LLM as the central reasoning engine and integrating off-the-shelf auxiliary modules (VAD, TTS, speaker verification), the system operates in a train-free, modular fashion, significantly reducing development complexity while maintaining high responsiveness and naturalness.
  - **关键发现**: The proposed system ranked second among all participating teams on the test set of the Human-like Spoken Dialogue Systems Challenge (Track 2). It demonstrated effective handling of full-duplex interaction with low response latency on the HumDial dataset. Qualitatively, the unit-based approach enabled more natural turn-taking behaviors, including appropriate use of backchannels and timely interruptions. The results indicate that a multimodal LLM can effectively replace traditional cascaded dialogue components without task-specific training, achieving near state-of-the-art performance in a plug-and-play setting.


### 2026-01-28
- **ASR for Affective Speech: Investigating Impact of Emotion and Speech Generative Strategy**
  - **作者**: Ya-Tse Wu et.al.
  - **arXiv**: [2601.20319](https://arxiv.org/abs/2601.20319)
  - **TLDR**: This paper investigates how emotional speech synthesized by different TTS models affects ASR performance, revealing that substitution errors dominate and vary across models. The authors propose two novel generative strategies—based on transcription correctness and emotional salience—to curate fine-tuning data, achieving consistent WER improvements on real emotional speech datasets without degrading performance on neutral speech.
  - **核心贡献**: The work addresses the challenge of degraded ASR performance on emotionally expressive speech by analyzing how different emotional TTS systems influence error patterns. It introduces a targeted data curation framework for fine-tuning ASR models using synthetic emotional speech, guided by two novel selection criteria: one prioritizing utterances with high transcription fidelity (low WER) and another emphasizing high emotional salience (measured via arousal/valence). This enables building emotion-aware ASR systems that generalize to real emotional speech while preserving robustness on clean, neutral data.
  - **关键发现**: All emotional TTS datasets degrade ASR performance compared to neutral speech, with substitution errors being dominant. Among TTS models, MaskGCT yields the lowest WER on synthetic data (4.40%). Fine-tuning with TTS-EMO-G reduces WER by up to 12.3% relative on MSP-Podcast Test2 and 9.8% on IEMOCAP compared to vanilla fine-tuning. Crucially, performance on LibriSpeech (neutral) remains stable (WER change < 0.1%), confirming no degradation. Gains are most pronounced in high-arousal/high-valence regions. The combined TTS-EMO-G strategy consistently outperforms individual strategies across all benchmarks, achieving the best average WER of 14.2% on real emotional datasets.

- **Audio Deepfake Detection in the Age of Advanced Text-to-Speech models**
  - **作者**: Robin Singh et.al.
  - **arXiv**: [2601.20510](https://arxiv.org/abs/2601.20510)
  - **TLDR**: This paper presents a comprehensive evaluation of audio deepfake detection systems against three cutting-edge TTS models—Dia2 (streaming), Maya1 (LLM-based), and MeloTTS (non-autoregressive)—demonstrating that single-paradigm detectors fail to generalize across architectures, while a multi-view detection approach combining semantic, structural, and signal-level features achieves robust performance. The work underscores the urgent need for integrated, adaptive detection frameworks in response to rapidly evolving synthetic speech technologies.
  - **核心贡献**: The paper addresses the critical gap in audio deepfake detection caused by the rapid advancement of diverse TTS architectures that render traditional, artifact-focused detectors obsolete. Its core contribution is the first empirical characterization of how detection efficacy varies dramatically across modern TTS paradigms (streaming, LLM-based, non-autoregressive) and the demonstration that a multi-view detection strategy—fusing complementary analytical perspectives—significantly outperforms single-method approaches. This reframes the detection problem from targeting fixed artifacts to adapting to heterogeneous generative mechanisms.
  - **关键发现**: Single-paradigm detectors showed significant performance variance: Whisper-MesoNet achieved 17.05% EER on Dia2 but degraded substantially on Maya1; XLS-R-SLS performed best overall but still recorded 27.10% EER on Maya1; SSL-AASIST excelled on Dia2 (low EER) but struggled with LLM-based synthesis. In contrast, the multi-view approach (exemplified by XLS-R-SLS and the proprietary UncovAI model) maintained robustness across all TTS types, with UncovAI achieving near-perfect separation (AUC ≈ 1.0). Critically, detectors trained on older vocoder artifacts suffered ~50% AUC drop on new TTS models. Maya1 proved most evasive due to its semantic grounding and multi-temporal token generation, which minimized traditional acoustic artifacts.

- **Erasing Your Voice Before It's Heard: Training-free Speaker Unlearning for Zero-shot Text-to-Speech**
  - **作者**: Myungjin Lee et.al.
  - **arXiv**: [2601.20481](https://arxiv.org/abs/2601.20481)
  - **TLDR**: The paper introduces TruS, a training-free speaker unlearning framework for zero-shot text-to-speech (TTS) systems that prevents unauthorized voice synthesis by suppressing identity-specific activations during inference. Unlike prior retraining-based methods, TruS works on both seen and unseen speakers without modifying the model, offering a scalable privacy safeguard.
  - **核心贡献**: TruS addresses the critical privacy risk in modern zero-shot TTS models—unauthorized voice cloning—by enabling speaker unlearning without any retraining. It innovates by shifting from data-centric deletion to inference-time control, manipulating internal hidden activations to erase target speaker identities while preserving non-identity attributes like prosody and emotion. This is the first method to generalize unlearning to speakers not present in the original training data (unseen opt-out speakers), overcoming a major limitation of existing approaches.
  - **关键发现**: TruS reduces speaker similarity for seen opt-out speakers by ~80% compared to baseline F5-TTS, matching the performance of retrained baselines (e.g., F5-TTS-FT) without any training. Crucially, it achieves significant suppression on unseen opt-out speakers (UO), where retraining methods fail entirely. WER remains stable (<2% increase), confirming preserved intelligibility. Emotion preservation scores show minimal degradation, indicating attribute retention. The 'µ + σ' layer selection yields optimal balance between unlearning strength and speech quality. Performance scales with larger retain sets (N=30 optimal).

