# TTS 论文周报
**周期**: 2026-01-30 至 2026-02-05 (第 6 周, 2026 年)
**生成时间**: 2026-02-08 10:48

## 概览
- **论文总数**: 14
- **主题分布**:
  - `合成`: 9
  - `多语言`: 5
  - `表现力`: 5
  - `零样本`: 2
  - `编辑`: 1
  - `编解码器`: 1

## 重点论文（分析摘要）
- **Zero-Shot TTS With Enhanced Audio Prompts: Bsc Submission For The 2026 Wildspoof Challenge TTS Track** (2026-02-05)
  - **作者**: Jose Giraldo et.al.
  - **arXiv**: [2602.05770](https://arxiv.org/abs/2602.05770)
  - **标签**: zero-shot, synthesis
  - **TLDR**: This paper presents a submission to the 2026 Wildspoof Challenge TTS Track, focusing on zero-shot TTS for spontaneous, in-the-wild speech. The core approach combines non-autoregressive TTS models (StyleTTS2 and F5-TTS) with a novel multi-stage audio enhancement pipeline using the Sidon model to clean noisy reference prompts. Key findings demonstrate that finetuning on enhanced audio significantly improves synthesis robustness and that prompt quality and length are critical factors for zero-shot performance.
  - **核心贡献**: The main contribution is a comprehensive system for zero-shot TTS synthesis capable of handling the spontaneous and noisy nature of in-the-wild speech. It solves the problem of generating natural, prosodically varied speech from low-quality, real-world audio prompts. The innovation lies in the synergistic integration of a state-of-the-art speech enhancement model (Sidon) with modern non-autoregressive TTS architectures, coupled with a systematic analysis of how the quality and duration of the reference audio prompt impact the final synthesis output.
  - **方法**: Experimental setup involves training and evaluating StyleTTS2 and F5-TTS on the TTS In the Wild (TITW) dataset, a spontaneous speech corpus. Baselines include the models trained on raw data versus enhanced data, and comparisons of different enhancement models (Sidon vs. Demucs). Evaluation metrics include UTMOS and DNSMOS for speech quality and naturalness, and Word Error Rate (WER) for intelligibility, computed using an ASR model (Nemo Conformer Large). Experiments systematically vary the length (short vs. long) and enhancement state of the reference audio prompt used for zero-shot voice cloning.
  - **关键发现**: 1. Finetuning on Sidon-enhanced audio yields superior performance, achieving up to 4.21 UTMOS and 3.47 DNSMOS. 2. Using longer audio prompts consistently improves speaker similarity scores for both models. 3. Audio enhancement significantly improves quality metrics (UTMOS, DNSMOS) but does not degrade intelligibility (WER is unaffected). 4. The impact of enhancement varies by model architecture; it is more crucial for F5-TTS. 5. Sidon model significantly outperforms standard Demucs in signal quality for this task.
  - **评估**: medium. The experimental design is clear and addresses relevant variables (prompt length, enhancement). It uses standard and appropriate metrics (UTMOS, DNSMOS, WER) and compares against a relevant baseline (Demucs). However, as a challenge submission, it may lack extensive ablation studies (e.g., isolating the contribution of flexible duration modeling) and comparison to a broader set of state-of-the-art TTS models. The reliance on a single primary dataset (TITW) also limits the assessment of generalizability. (评分: 7/10)

- **PFluxTTS: Hybrid Flow-Matching TTS with Robust Cross-Lingual Voice Cloning and Inference-Time Model Fusion** (2026-02-04)
  - **作者**: Vikentii Pankov et.al.
  - **arXiv**: [2602.04160](https://arxiv.org/abs/2602.04160)
  - **标签**: zero-shot, multilingual, synthesis
  - **TLDR**: PFluxTTS introduces a hybrid flow-matching TTS system that fuses duration-guided and alignment-free models at inference time to overcome the stability-naturalness trade-off, significantly improves cross-lingual voice cloning without requiring prompt transcripts, and enhances audio quality via a modified PeriodWave vocoder with 48 kHz super-resolution. It outperforms leading open-source and commercial TTS systems in naturalness, intelligibility, and speaker similarity on challenging cross-lingual benchmarks.
  - **核心贡献**: The paper addresses three critical gaps in modern flow-matching TTS: (1) the inherent trade-off between stability (from duration-guided models) and naturalness/fluency (from alignment-free models), (2) poor cross-lingual voice cloning performance when using short, transcript-free reference audio, and (3) limited audio fidelity due to reliance on low-rate mel-spectrogram features. The core innovation is a dual-decoder architecture that combines independently trained duration-guided (DG) and alignment-free (AF) flow-matching models through inference-time vector-field fusion, enabling dynamic balancing of control and fluency. Additionally, it introduces robust speaker cloning using sequence-level speech-prompt embeddings within a FLUX-based decoder and integrates a high-fidelity vocoder pipeline for 48 kHz output.
  - **方法**: Experiments focused on cross-lingual English TTS using multilingual speech prompts from real-world conversational datasets. Subjective evaluation used 40 utterances assessed via CMOS (Comparative Mean Opinion Score) and SMOS (Speaker Similarity MOS) across four languages. Objective metrics included WER (Word Error Rate), CER (Character Error Rate), LSD (Log-Spectral Distance), and SPK-SIM (speaker similarity score). Baselines included open-source models (F5-TTS, FishSpeech, SparkTTS, ChatterBox) and the commercial ElevenLabs Multilingual v2. Evaluation datasets included VoxLingua107 (for cross-lingual dev/test) and in-the-wild conversational data; monolingual datasets like LibriSpeech and VCTK were noted but not primary. Ablation studies tested model fusion (α schedules), DG-only vs. fused performance, and prompt conditioning strategies. All systems used short reference audio (<10s) with no transcript provided during cloning.
  - **关键发现**: PFluxTTS achieved a MOS of 4.11 in naturalness, matching ChatterBox, while reducing WER to 6.9% (23% lower than ChatterBox’s 9.0%). It surpassed ElevenLabs in speaker similarity by +0.32 SMOS. In ablation, the fused model (α=0.75) reduced CER to 8.6% versus higher errors for DG-only or AF-only variants. In pairwise CMOS tests, the fused model was preferred in 79% of cases (p<0.012). It also achieved the best LSD scores on both VoxLingua-dev and in-the-wild data. The system demonstrated robustness in challenging scenarios (e.g., noisy, accented, or code-switched prompts) where most open-source models failed, using only short, transcript-free references and no extra fine-tuning.
  - **评估**: strong (评分: 9/10)

- **CoCoEmo: Composable and Controllable Human-Like Emotional TTS via Activation Steering** (2026-02-03)
  - **作者**: Siyi Wang et.al.
  - **arXiv**: [2602.03420](https://arxiv.org/abs/2602.03420)
  - **标签**: expressive, synthesis
  - **TLDR**: CoCoEmo introduces a systematic framework for composable and controllable emotional expression in hybrid text-to-speech (TTS) systems using activation steering, enabling mixed-emotion synthesis and handling of text-emotion mismatches without retraining. It demonstrates that emotional prosody is primarily generated in the speech language model (SLM) stage rather than the flow-matching acoustic module.
  - **核心贡献**: The paper addresses the limitation of existing expressive TTS systems that enforce a single, utterance-level emotion label, thereby failing to capture nuanced, mixed, or contextually misaligned emotional expressions found in human speech. CoCoEmo proposes the first systematic analysis of activation steering for emotional control in hybrid TTS architectures, introducing a lightweight, plug-and-play method that injects emotion-specific steering vectors into selected layers of the SLM. This enables fine-grained, composable emotional control—including synthesis of conflicting emotions and mismatched text-emotion scenarios—while preserving linguistic content and avoiding model retraining or architectural changes.
  - **方法**: Experiments are conducted on two English-language hybrid TTS models: CosyVoice2 (with SLM layers 10–17 optimal for steering) and IndexTTS2 (layers 6–9). Steering vectors are extracted from paired emotional speech datasets (CREMA-D, ESD, MELD) where the same speaker utters identical text under different emotions. Datasets are split into train (vector extraction), validation (steering site selection via linear probing), and test sets. Evaluation includes in-distribution (CREMA-D) and out-of-distribution (IEMOCAP) settings. Baselines include native instruction-based prompting (no internal activation modification) and unsteered model outputs. Objective metrics include E-SIM (emotion similarity via embedding cosine distance), TEP (text-emotion preservation measured by semantic consistency under emotion shift), valence-arousal correlation (ρ), human-likeness rate (H-Rate), speaker similarity (S-SIM), and word error rate (WER). Subjective evaluation uses Mean Opinion Score (MOS) with multiple raters. Mixed-emotion evaluation leverages multi-annotator soft labels to construct ground-truth emotion distributions. Text-emotion mismatch is quantified using ℓ2 distance between predicted and expected valence-arousal values under semantic conflict.
  - **关键发现**: Activation steering significantly improves emotional expressiveness: E-SIM increases by up to 0.15 over baselines, and TEP remains high (>0.85) even under strong emotion shifts, indicating preserved linguistic content. Mixed-emotion synthesis achieves higher MOS (up to 4.1 vs. 3.6 for baselines) and better alignment with human-annotated emotion distributions. Crucially, steering applied only to the SLM yields strong emotional control, while steering the flow-matching module produces negligible effects—confirming the SLM as the primary locus of emotional prosody generation. In high-mismatch IEMOCAP scenarios, CoCoEmo maintains naturalness (MOS ~3.9) and low WER (<5%), whereas baseline models exhibit unnatural or collapsed expressiveness. Both models show consistent gains across architectures, validating the generality of the approach.
  - **评估**: strong (评分: 9/10)

- **LipSody: Lip-to-Speech Synthesis with Enhanced Prosody Consistency** (2026-02-02)
  - **作者**: Jaejun Lee et.al.
  - **arXiv**: [2602.01908](https://arxiv.org/abs/2602.01908)
  - **标签**: expressive, synthesis
  - **TLDR**: LipSody is a novel lip-to-speech synthesis framework that significantly improves prosody consistency—such as pitch, energy, and speaker identity—by integrating visual cues from silent facial videos into a diffusion-based TTS pipeline. Unlike prior methods that prioritize intelligibility alone, LipSody explicitly models prosodic features derived from speaker identity, linguistic content, and emotional context, resulting in more natural and personalized synthesized speech.
  - **核心贡献**: The core contribution of LipSody is the introduction of a prosody-guiding strategy that enhances prosodic consistency in lip-to-speech synthesis by leveraging three complementary visual cues: speaker identity from facial images, linguistic content from lip movements, and emotional context from face video. This addresses a critical limitation in existing diffusion-based models like LipVoicer, which excel at reconstructing intelligible speech but produce prosodically inconsistent or unnatural outputs. By explicitly modeling pitch and energy dynamics within the diffusion generation process, LipSody achieves more expressive and speaker-consistent speech without sacrificing linguistic accuracy.
  - **方法**: The experiments use the LRS3 dataset, comprising 5,502 TED/TEDx videos, with standard pretrain, train, and test splits. The base architecture replicates LipVoicer’s diffusion setup, including a Conformer-based lip reader and HiFi-GAN vocoder. Three model variants are evaluated: (1) the official LipVoicer, (2) a reconstructed LipVoicer (LipVoicerrecon) for controlled comparison, and (3) the full LipSody model with prosody modeling. Objective metrics include Word Error Rate (WER) using an ASR model for intelligibility, and prosody-specific metrics such as global/local pitch deviation, energy consistency, and speaker similarity (Resem and Resemtv). Subjective evaluations include naturalness Mean Opinion Score (MOS) and ABX preference tests. Statistical significance is assessed via one-sample t-tests (p < 0.05). Ablation studies in Table 4 examine performance under different prosody information settings to validate the contribution of each cue.
  - **关键发现**: LipSody achieves comparable WER (21.9%) to LipVoicer, confirming preserved intelligibility. Crucially, it shows significant improvements in prosody-related metrics: reduced global and local pitch deviations, higher energy consistency, and improved speaker similarity scores (Resem and Resemtv). Subjective evaluations reveal statistically significant preference for LipSody in naturalness (MOS) and ABX tests (p < 0.05). Ablation studies confirm that removing any prosody cue degrades performance, with full integration yielding optimal results. Notably, better prosody modeling also correlates with slight gains in intelligibility, suggesting prosody aids linguistic reconstruction. The model demonstrates robustness across diverse speakers and emotional contexts in LRS3.
  - **评估**: strong (评分: 8/10)

- **EmoAra: Emotion-Preserving English Speech Transcription and Cross-Lingual Translation with Arabic Text-to-Speech** (2026-02-01)
  - **作者**: Besher Hassan et.al.
  - **arXiv**: [2602.01170](https://arxiv.org/abs/2602.01170)
  - **标签**: expressive, multilingual, synthesis
  - **TLDR**: EmoAra introduces an end-to-end emotion-preserving cross-lingual speech translation pipeline that converts English customer service speech into emotionally nuanced Arabic speech, integrating SER, ASR, MT, and TTS components. It demonstrates high performance in emotion classification (94% F1), translation (BLEU 56, BERTScore F1 88.7%), and human-rated output quality (81%) in a banking domain.
  - **核心贡献**: The paper presents EmoAra, a novel pipeline that preserves emotional context during English-to-Arabic spoken language translation—addressing a critical gap in customer service applications where emotional tone impacts user experience. Unlike conventional pipelines that treat translation and speech synthesis as emotion-agnostic steps, EmoAra explicitly incorporates emotion recognition early in the pipeline and ensures downstream components are compatible with domain-specific and emotional fidelity requirements, even if emotion is not directly injected into TTS prosody.
  - **方法**: Experiments used the RAVDESS dataset for SER training, covering 8 emotion classes. For MT, a custom English–Arabic banking corpus of ~24k sentence pairs was created by translating English banking scripts using Google Translate and MyMemoryTranslator. Baselines included: (a) a non-augmented CNN SER model without feature engineering or data augmentation, and (b) off-the-shelf MarianMT without fine-tuning. Evaluation metrics: F1-score per emotion class for SER; BLEU and BERTScore F1 for MT; and human evaluation (Likert-scale scoring) for translation fluency and domain appropriateness in Arabic TTS output. Ablation studies compared LSTM and ResNet50 against CNN for SER. Training employed early stopping, checkpointing, and hyperparameter sweeps (learning rate, batch size, epochs).
  - **关键发现**: The final CNN achieved 94% macro F1 for emotion classification, outperforming the baseline across all classes—though 'calm', 'fearful', and 'sad' remained challenging due to acoustic overlap. Fine-tuned MarianMT reached BLEU 56 and BERTScore F1 88.7%, a dramatic improvement over scratch-trained Transformer (BLEU 23) and un-fine-tuned MarianMT (BLEU 25.48). Human evaluators rated 81% of Arabic TTS outputs as appropriate and fluent in the banking context. The pipeline successfully handled average sentence lengths of 15–17 tokens but struggled with very long utterances due to truncation in training data.
  - **评估**: medium (评分: 7/10)

- **EmoShift: Lightweight Activation Steering for Enhanced Emotion-Aware Speech Synthesis** (2026-01-30)
  - **作者**: Li Zhou et.al.
  - **arXiv**: [2601.22873](https://arxiv.org/abs/2601.22873)
  - **标签**: expressive, synthesis
  - **TLDR**: EmoShift introduces a lightweight activation-steering framework called EmoSteer that enhances emotion-aware speech synthesis by learning emotion-specific steering vectors in the output embedding space, enabling precise emotional control without full model fine-tuning. It achieves superior performance over zero-shot and fully fine-tuned baselines with only 10M trainable parameters—less than 1/30 of full fine-tuning—while preserving naturalness and speaker identity.
  - **核心贡献**: The paper addresses the limitation of existing emotion-aware TTS systems that rely on scaling fixed emotion embeddings or external prompts, which restrict their ability to capture emotion-specific latent characteristics. EmoShift’s core innovation is the EmoSteer layer—a lightweight, plug-and-play module that learns emotion-specific offset vectors in the model’s activation space, enabling controllable, interpretable, and stable emotional expression across utterances without modifying or retraining the base TTS model.
  - **方法**: The experiments use the English subset of the Emotional Speech Dataset (ESD), covering five emotions including neutral. Two base models—CosyVoice and another unnamed LLM-based TTS—are used. Baselines include zero-shot emotion prompting and full fine-tuning (SFT). Evaluation metrics include Word Error Rate (WER) via ASR, Speaker Similarity (SpkSIM) using WavLM-Base, Emotion Recognition Accuracy via emotion2vec, Mean Opinion Score (MOS) for naturalness, and Emo-MOS for emotional expressiveness. Subjective evaluation involves 10 listeners rating preference between base and EmoShift-augmented outputs. Objective evaluations measure speech quality and emotion fidelity; controllability is tested by scaling the steering vector magnitude (α) and measuring resulting SER accuracy.
  - **关键发现**: EmoShift outperforms both zero-shot and fully fine-tuned (SFT) baselines across all metrics. In subjective evaluation, it achieves higher MOS and Emo-MOS scores, with listeners preferring EmoShift outputs in pairwise comparisons. Objectively, it improves emotion recognition accuracy—highest for Sad (61.24%) and Neutral (55.84%)—and maintains low WER and high SpkSIM, indicating preserved naturalness and speaker identity. Scaling the steering vector (α) enables smooth control of emotional intensity, validated by monotonic changes in SER accuracy. EmoShift matches or exceeds CosyVoice-SFT-Shift performance despite using <1/30 the trainable parameters.
  - **评估**: strong (评分: 8/10)

- **Evaluating and Rewarding LALMs for Expressive Role-Play TTS via Mean Continuation Log-Probability** (2026-01-30)
  - **作者**: Yong Ren et.al.
  - **arXiv**: [2601.22661](https://arxiv.org/abs/2601.22661)
  - **标签**: expressive, synthesis
  - **TLDR**: This paper introduces Mean Continuation Log-Probability (MCLP) as a novel metric and reinforcement learning reward to improve stylistic consistency in Large Audio Language Model (LALM)-based Role-Play Text-to-Speech (RP-TTS). By leveraging in-context learning of pretrained LALMs, MCLP quantifies how well generated speech aligns with character profiles and scene context, significantly outperforming strong baselines in both objective and subjective evaluations on a newly constructed multi-turn RP-TTS dataset.
  - **核心贡献**: The core contribution is the proposal of Mean Continuation Log-Probability (MCLP), which serves dual purposes: (1) as an objective evaluation metric for stylistic consistency in expressive role-play TTS, addressing the lack of reliable style quantification in prior work; and (2) as a reinforcement learning (RL) reward signal to fine-tune LALMs for better alignment with role-play instructions. The innovation lies in using the intrinsic likelihood modeling capability of pretrained LALMs—via in-context learning—to compute the log-probability of ground-truth speech continuations conditioned on model-generated speech, thereby capturing dynamic, context-sensitive stylistic coherence across multi-turn dialogues rather than treating style as static or attribute-based.
  - **方法**: The authors construct a new RP-TTS dataset called Drp from the WenetSpeech corpus, sourcing drama-tagged YouTube videos. The pipeline includes filtering for quality, using Qwen-VL-7B to generate objective scene descriptions (ignoring dialogue), and prompting LLMs to extract character profiles from full episode scripts. The final dataset contains ~100 hours of speech with detailed annotations. Evaluation uses both objective metrics (WER, Character Error Rate [CER], Pinyin Error Rate, MCLP) and subjective Mean Opinion Score (MOS) assessed by 32 native Chinese professional annotators on 31 curated samples. Baselines include GPT-Audio (closed-source), SFT-only LALM, and Audio-2-mini variants. Two test conditions are evaluated: with audio history (W. Audio History) and without (W/O. Audio History). Ablation studies examine the impact of SFT, GRPO, and reward components (MCLP vs. WER). Training uses 1 epoch of SFT followed by GRPO with advantage estimation relative to the SFT reference policy.
  - **关键发现**: The proposed method achieves state-of-the-art performance: MOS of 3.646 (95% CI reported), approaching ground-truth naturalness, and significantly outperforms all baselines in both settings. It attains the lowest CER and Pinyin error rates, indicating high phonetic fidelity. MCLP scores confirm superior stylistic consistency. Notably, while baseline models degrade without audio history, the proposed method maintains robust performance, demonstrating effective use of textual context alone. Ablation shows that combining MCLP and WER in GRPO is critical—using only WER leads to reward hacking (e.g., generating bland speech to minimize errors), while MCLP-only yields unsafe outputs. The full pipeline (SFT + GRPO with combined reward) achieves the best balance. Audio-2-mini shows significant performance drops in role-play scenarios, highlighting the need for instruction-aligned training.
  - **评估**: strong (评分: 8/10)

## 完整列表（按日期）
### 2026-02-05
- **ARCHI-TTS: A flow-matching-based Text-to-Speech Model with Self-supervised Semantic Aligner and Accelerated Inference**
  - **作者**: Chunyat Wu et.al.
  - **arXiv**: [2602.05207](https://arxiv.org/abs/2602.05207)
  - **TLDR**: ARCHI-TTS is a non-autoregressive, flow-matching-based TTS model that addresses two major challenges in diffusion-based TTS: robust text-speech alignment and high inference cost. It introduces a dedicated self-supervised semantic aligner for temporal consistency and an efficient inference strategy that reuses encoder features to drastically accelerate synthesis. The model achieves state-of-the-art word error rates on standard benchmarks while maintaining high synthesis quality and efficiency.
  - **核心贡献**: The core contribution is a novel TTS architecture that synergistically combines a flow-matching generative model with a dedicated semantic aligner to solve alignment and efficiency problems simultaneously. It solves the problem of inaccurate text-audio alignment in non-autoregressive models by using a separate, trainable aligner module, which improves semantic consistency. It also solves the slow iterative inference of diffusion/flow models by proposing a feature reuse strategy across denoising steps, significantly reducing computational overhead without quality loss.
  - **关键发现**: 1) **SOTA Intelligibility**: Achieves WER of 1.98% on LibriSpeech-PC test-clean, and 1.47%/1.42% on SeedTTS test-en/test-zh, outperforming all compared models. 2) **High Efficiency**: Can generate 10-second audio in 0.4 seconds on an A100 GPU (RTF ~0.04), significantly faster than typical diffusion models. 3) **Ablation Results**: The semantic aligner and CTC loss are crucial for low WER. Speaker embeddings improve MOS. The VAE latent representation is effective; a VQ-regularized variant showed benefits. 4) **Feature Reuse Trade-off**: High encoder feature sharing ratio greatly accelerates inference but causes some performance drop; a balance is needed (e.g., at NFE=32, it achieves a good speed-quality trade-off). 5) **Multi-lingual Capability**: Performs well on both English and Chinese test sets within a single model.

- **Wave-Trainer-Fit: Neural Vocoder with Trainable Prior and Fixed-Point Iteration towards High-Quality Speech Generation from SSL features**
  - **作者**: Hien Ohnaka et.al.
  - **arXiv**: [2602.05443](https://arxiv.org/abs/2602.05443)
  - **TLDR**: WaveTrainerFit is a neural vocoder that generates high-quality speech from Self-Supervised Learning (SSL) features. It improves upon the WaveFit vocoder by introducing a trainable prior and reference-aware gain adjustment, enabling faster, higher-quality synthesis with fewer inference steps. The method is robust to the depth of SSL feature extraction and outperforms baselines in both objective and subjective metrics.
  - **核心贡献**: The paper's main contribution is the WaveTrainerFit vocoder, which solves the problem of high-quality, efficient waveform generation from data-driven SSL features—a challenging task due to their abstract and compressed nature. It innovates by integrating a trainable prior (modeled via a VAE-like framework) and a reference energy constraint into a WaveFit (GAN+Diffusion) backbone. This allows the inference process to start from an informative, energy-matched initial noise rather than pure Gaussian noise, simplifying the denoising task and enabling high-quality synthesis in fewer iterations.
  - **关键发现**: 1. WaveTrainerFit outperformed WaveFit baselines in all objective metrics (SpeechBERTScore, Speaker Similarity, LSD, F0 Corr) and subjective N-MOS/S-MOS across multiple SSL models (WavLM, XLS-R, Whisper) at T=5 steps. 2. It achieved high quality with only 5 inference steps, whereas WaveFit required more. 3. It showed robust performance across different SSL feature extraction depths (layers 2, 8, 24), outperforming the baseline at all layers, with particularly strong gains in speaker similarity for deeper layers. 4. The method's RTF was comparable to the baseline, confirming efficiency gains are from reduced steps, not slower per-step computation. 5. Performance varied by SSL model, with WavLM features yielding best results.

- **Zero-Shot TTS With Enhanced Audio Prompts: Bsc Submission For The 2026 Wildspoof Challenge TTS Track**
  - **作者**: Jose Giraldo et.al.
  - **arXiv**: [2602.05770](https://arxiv.org/abs/2602.05770)
  - **TLDR**: This paper presents a submission to the 2026 Wildspoof Challenge TTS Track, focusing on zero-shot TTS for spontaneous, in-the-wild speech. The core approach combines non-autoregressive TTS models (StyleTTS2 and F5-TTS) with a novel multi-stage audio enhancement pipeline using the Sidon model to clean noisy reference prompts. Key findings demonstrate that finetuning on enhanced audio significantly improves synthesis robustness and that prompt quality and length are critical factors for zero-shot performance.
  - **核心贡献**: The main contribution is a comprehensive system for zero-shot TTS synthesis capable of handling the spontaneous and noisy nature of in-the-wild speech. It solves the problem of generating natural, prosodically varied speech from low-quality, real-world audio prompts. The innovation lies in the synergistic integration of a state-of-the-art speech enhancement model (Sidon) with modern non-autoregressive TTS architectures, coupled with a systematic analysis of how the quality and duration of the reference audio prompt impact the final synthesis output.
  - **关键发现**: 1. Finetuning on Sidon-enhanced audio yields superior performance, achieving up to 4.21 UTMOS and 3.47 DNSMOS. 2. Using longer audio prompts consistently improves speaker similarity scores for both models. 3. Audio enhancement significantly improves quality metrics (UTMOS, DNSMOS) but does not degrade intelligibility (WER is unaffected). 4. The impact of enhancement varies by model architecture; it is more crucial for F5-TTS. 5. Sidon model significantly outperforms standard Demucs in signal quality for this task.


### 2026-02-04
- **PFluxTTS: Hybrid Flow-Matching TTS with Robust Cross-Lingual Voice Cloning and Inference-Time Model Fusion**
  - **作者**: Vikentii Pankov et.al.
  - **arXiv**: [2602.04160](https://arxiv.org/abs/2602.04160)
  - **TLDR**: PFluxTTS introduces a hybrid flow-matching TTS system that fuses duration-guided and alignment-free models at inference time to overcome the stability-naturalness trade-off, significantly improves cross-lingual voice cloning without requiring prompt transcripts, and enhances audio quality via a modified PeriodWave vocoder with 48 kHz super-resolution. It outperforms leading open-source and commercial TTS systems in naturalness, intelligibility, and speaker similarity on challenging cross-lingual benchmarks.
  - **核心贡献**: The paper addresses three critical gaps in modern flow-matching TTS: (1) the inherent trade-off between stability (from duration-guided models) and naturalness/fluency (from alignment-free models), (2) poor cross-lingual voice cloning performance when using short, transcript-free reference audio, and (3) limited audio fidelity due to reliance on low-rate mel-spectrogram features. The core innovation is a dual-decoder architecture that combines independently trained duration-guided (DG) and alignment-free (AF) flow-matching models through inference-time vector-field fusion, enabling dynamic balancing of control and fluency. Additionally, it introduces robust speaker cloning using sequence-level speech-prompt embeddings within a FLUX-based decoder and integrates a high-fidelity vocoder pipeline for 48 kHz output.
  - **关键发现**: PFluxTTS achieved a MOS of 4.11 in naturalness, matching ChatterBox, while reducing WER to 6.9% (23% lower than ChatterBox’s 9.0%). It surpassed ElevenLabs in speaker similarity by +0.32 SMOS. In ablation, the fused model (α=0.75) reduced CER to 8.6% versus higher errors for DG-only or AF-only variants. In pairwise CMOS tests, the fused model was preferred in 79% of cases (p<0.012). It also achieved the best LSD scores on both VoxLingua-dev and in-the-wild data. The system demonstrated robustness in challenging scenarios (e.g., noisy, accented, or code-switched prompts) where most open-source models failed, using only short, transcript-free references and no extra fine-tuning.

- **WAXAL: A Large-Scale Multilingual African Language Speech Corpus**
  - **作者**: Abdoulaye Diack et.al.
  - **arXiv**: [2602.02734](https://arxiv.org/abs/2602.02734)
  - **TLDR**: The paper introduces WAXAL, a large-scale, open-access multilingual speech corpus covering 21 Sub-Saharan African languages, comprising 1,250 hours of transcribed natural speech for ASR and over 180 hours of high-quality single-speaker recordings for TTS. It addresses the critical data gap for under-resourced African languages by prioritizing ethical collection, local partnerships, and phonetic balance to enable inclusive speech technology development.
  - **核心贡献**: WAXAL provides the first large-scale, unified, and openly licensed speech dataset specifically designed to support both ASR and TTS systems across 21 African languages—many of which have been historically excluded from speech technology research due to lack of data. The core innovation lies not in novel algorithms but in the systematic, ethically grounded creation of foundational data infrastructure that enables future modeling work. By including both spontaneous speech (for ASR) and studio-quality, phonetically balanced read speech (for TTS), it supports end-to-end development of speech technologies while ensuring speaker diversity, linguistic coverage, and community involvement through partnerships with four African academic and community organizations.
  - **关键发现**: The final release includes 1,250 hours of transcribed ASR data across 14 languages and 180+ hours of TTS data across 10 languages, collectively representing over 100 million speakers. Language distributions are uneven but reflect population sizes and feasibility (e.g., Swahili, Yoruba, and Hausa have larger allocations). The TTS recordings meet industry standards for synthesis training, with low noise and consistent prosody. The ASR data exhibits high lexical and acoustic diversity due to spontaneous elicitation. Compared to existing resources—such as Mozilla Common Voice (which has sparse African language coverage) or radio-based archives (which lack transcripts and speaker metadata)—WAXAL offers superior scale, structure, and suitability for modern neural TTS/ASR pipelines. No quantitative model performance is reported, as the paper is a data release.


### 2026-02-03
- **CoCoEmo: Composable and Controllable Human-Like Emotional TTS via Activation Steering**
  - **作者**: Siyi Wang et.al.
  - **arXiv**: [2602.03420](https://arxiv.org/abs/2602.03420)
  - **TLDR**: CoCoEmo introduces a systematic framework for composable and controllable emotional expression in hybrid text-to-speech (TTS) systems using activation steering, enabling mixed-emotion synthesis and handling of text-emotion mismatches without retraining. It demonstrates that emotional prosody is primarily generated in the speech language model (SLM) stage rather than the flow-matching acoustic module.
  - **核心贡献**: The paper addresses the limitation of existing expressive TTS systems that enforce a single, utterance-level emotion label, thereby failing to capture nuanced, mixed, or contextually misaligned emotional expressions found in human speech. CoCoEmo proposes the first systematic analysis of activation steering for emotional control in hybrid TTS architectures, introducing a lightweight, plug-and-play method that injects emotion-specific steering vectors into selected layers of the SLM. This enables fine-grained, composable emotional control—including synthesis of conflicting emotions and mismatched text-emotion scenarios—while preserving linguistic content and avoiding model retraining or architectural changes.
  - **关键发现**: Activation steering significantly improves emotional expressiveness: E-SIM increases by up to 0.15 over baselines, and TEP remains high (>0.85) even under strong emotion shifts, indicating preserved linguistic content. Mixed-emotion synthesis achieves higher MOS (up to 4.1 vs. 3.6 for baselines) and better alignment with human-annotated emotion distributions. Crucially, steering applied only to the SLM yields strong emotional control, while steering the flow-matching module produces negligible effects—confirming the SLM as the primary locus of emotional prosody generation. In high-mismatch IEMOCAP scenarios, CoCoEmo maintains naturalness (MOS ~3.9) and low WER (<5%), whereas baseline models exhibit unnatural or collapsed expressiveness. Both models show consistent gains across architectures, validating the generality of the approach.


### 2026-02-02
- **LipSody: Lip-to-Speech Synthesis with Enhanced Prosody Consistency**
  - **作者**: Jaejun Lee et.al.
  - **arXiv**: [2602.01908](https://arxiv.org/abs/2602.01908)
  - **TLDR**: LipSody is a novel lip-to-speech synthesis framework that significantly improves prosody consistency—such as pitch, energy, and speaker identity—by integrating visual cues from silent facial videos into a diffusion-based TTS pipeline. Unlike prior methods that prioritize intelligibility alone, LipSody explicitly models prosodic features derived from speaker identity, linguistic content, and emotional context, resulting in more natural and personalized synthesized speech.
  - **核心贡献**: The core contribution of LipSody is the introduction of a prosody-guiding strategy that enhances prosodic consistency in lip-to-speech synthesis by leveraging three complementary visual cues: speaker identity from facial images, linguistic content from lip movements, and emotional context from face video. This addresses a critical limitation in existing diffusion-based models like LipVoicer, which excel at reconstructing intelligible speech but produce prosodically inconsistent or unnatural outputs. By explicitly modeling pitch and energy dynamics within the diffusion generation process, LipSody achieves more expressive and speaker-consistent speech without sacrificing linguistic accuracy.
  - **关键发现**: LipSody achieves comparable WER (21.9%) to LipVoicer, confirming preserved intelligibility. Crucially, it shows significant improvements in prosody-related metrics: reduced global and local pitch deviations, higher energy consistency, and improved speaker similarity scores (Resem and Resemtv). Subjective evaluations reveal statistically significant preference for LipSody in naturalness (MOS) and ABX tests (p < 0.05). Ablation studies confirm that removing any prosody cue degrades performance, with full integration yielding optimal results. Notably, better prosody modeling also correlates with slight gains in intelligibility, suggesting prosody aids linguistic reconstruction. The model demonstrates robustness across diverse speakers and emotional contexts in LRS3.


### 2026-02-01
- **Bias in the Ear of the Listener: Assessing Sensitivity in Audio Language Models Across Linguistic, Demographic, and Positional Variations**
  - **作者**: Sheng-Lun Wei et.al.
  - **arXiv**: [2602.01030](https://arxiv.org/abs/2602.01030)
  - **TLDR**: This paper presents the first systematic study of bias in multilingual speech-integrated large language models (MLLMs), introducing the BiasInEar dataset—a 70.8-hour, gender- and accent-balanced speech benchmark across English, Chinese, and Korean—and evaluating nine models under linguistic, demographic, and positional perturbations. It reveals that MLLMs are highly sensitive to language and option order but relatively robust to gender and accent, establishing a unified framework for fairness and robustness evaluation in audio-language systems.
  - **核心贡献**: The work addresses the critical gap in evaluating fairness and robustness of multimodal language models when processing spoken input rather than text. By constructing BiasInEar—a carefully curated, speech-augmented version of Global MMLU Lite with controlled variations in language, accent, gender, and answer option order—the authors enable systematic assessment of how speech modality amplifies or mitigates biases inherent in text-based LLMs. The core innovation lies in bridging text- and speech-based evaluation through a standardized, multilingual, and demographically balanced benchmark coupled with four complementary metrics that capture accuracy, uncertainty, sensitivity, and inter-rater consistency.
  - **关键发现**: Key quantitative results include: (1) All models show significant performance drops under option-order reversal, with APES values consistently higher for this perturbation than for gender or accent changes—e.g., Voxtral models exhibit up to 12% accuracy drop in Korean when options are reversed. (2) Language has the strongest effect: average accuracy drops from ~72% in English to ~58% in Korean across models. (3) Gender and accent show minimal impact: accuracy differences <2% between male/female speakers and native/non-native accents within each language. (4) Larger models (e.g., Gemma 120B) achieve higher Fleiss’ κ (>0.65) and lower APES (<0.15), indicating greater robustness. (5) End-to-end models outperform pipeline models in consistency (higher κ) despite similar accuracy, suggesting joint audio-language training stabilizes predictions. (6) Reasoning complexity (e.g., CoT) does not consistently improve robustness and sometimes increases APES. Qualitatively, models exhibit high entropy in low-resource languages (Korean), reflecting uncertainty, and structural biases (option order) dominate over demographic ones—a contrast to findings in pure text-based bias studies.

- **EmoAra: Emotion-Preserving English Speech Transcription and Cross-Lingual Translation with Arabic Text-to-Speech**
  - **作者**: Besher Hassan et.al.
  - **arXiv**: [2602.01170](https://arxiv.org/abs/2602.01170)
  - **TLDR**: EmoAra introduces an end-to-end emotion-preserving cross-lingual speech translation pipeline that converts English customer service speech into emotionally nuanced Arabic speech, integrating SER, ASR, MT, and TTS components. It demonstrates high performance in emotion classification (94% F1), translation (BLEU 56, BERTScore F1 88.7%), and human-rated output quality (81%) in a banking domain.
  - **核心贡献**: The paper presents EmoAra, a novel pipeline that preserves emotional context during English-to-Arabic spoken language translation—addressing a critical gap in customer service applications where emotional tone impacts user experience. Unlike conventional pipelines that treat translation and speech synthesis as emotion-agnostic steps, EmoAra explicitly incorporates emotion recognition early in the pipeline and ensures downstream components are compatible with domain-specific and emotional fidelity requirements, even if emotion is not directly injected into TTS prosody.
  - **关键发现**: The final CNN achieved 94% macro F1 for emotion classification, outperforming the baseline across all classes—though 'calm', 'fearful', and 'sad' remained challenging due to acoustic overlap. Fine-tuned MarianMT reached BLEU 56 and BERTScore F1 88.7%, a dramatic improvement over scratch-trained Transformer (BLEU 23) and un-fine-tuned MarianMT (BLEU 25.48). Human evaluators rated 81% of Arabic TTS outputs as appropriate and fluent in the banking context. The pipeline successfully handled average sentence lengths of 15–17 tokens but struggled with very long utterances due to truncation in training data.

- **VividVoice: A Unified Framework for Scene-Aware Visually-Driven Speech Synthesis**
  - **作者**: Chengyuan Ma et.al.
  - **arXiv**: [2602.02591](https://arxiv.org/abs/2602.02591)
  - **TLDR**: VividVoice introduces a novel task—Scene-Aware Visually-Driven Speech Synthesis—and proposes a unified framework that jointly models visual scenes, speaker identity, and environmental acoustics to generate immersive, contextually aligned speech. It addresses data scarcity and modality decoupling through a new large-scale dataset (Vivid-210K) and a Decoupled Multi-Scene Visual-Audio (D-MSVA) alignment module, significantly outperforming existing baselines in fidelity, clarity, and multimodal consistency.
  - **核心贡献**: The paper defines and tackles the new challenge of generating speech that is not only driven by visual input but also aware of the physical acoustic environment and speaker identity. Existing TTS and audio generation models either ignore environmental acoustics or fail to maintain consistent speaker identity across varying scenes. VividVoice solves this by creating the first large-scale, high-quality multimodal dataset with strong cross-modal alignment between visual scenes, speaker identity, and corresponding audio, and by introducing a novel alignment architecture (D-MSVA) that enables fine-grained disentanglement and control over timbre and environmental acoustic features.
  - **关键发现**: VividVoice achieved a MOS of 3.08 in subjective evaluation, significantly higher than baselines. In A/B preference tests, it was preferred 68% of the time over the stronger Attn-Fusion baseline. Objective metrics showed lower Word Error Rates and higher audio fidelity. Ablation studies confirmed D-MSVA’s contribution, with performance dropping when the decoupled memory bank or contrastive supervision was removed. The model demonstrated strong decoupling ability—users could independently vary speaker identity and environmental acoustics without interference. The Vivid-210K dataset itself was validated with 98.6% automated alignment accuracy and high subjective quality ratings from domain experts.


### 2026-01-31
- **Edit Content, Preserve Acoustics: Imperceptible Text-Based Speech Editing via Self-Consistency Rewards**
  - **作者**: Yong Ren et.al.
  - **arXiv**: [2602.00560](https://arxiv.org/abs/2602.00560)
  - **TLDR**: This paper introduces a novel text-based speech editing framework that decouples semantic content editing from acoustic rendering to achieve imperceptible edits while preserving speaker identity, prosody, and naturalness. By leveraging a pre-trained TTS model as an implicit critic within a reinforcement learning setup using Self-Consistency Rewards and Group Relative Policy Optimization (GRPO), the method significantly outperforms existing autoregressive and non-autoregressive baselines in intelligibility, robustness, and perceptual quality.
  - **核心贡献**: The core contribution is a principled 'Edit Content, Preserve Acoustics' framework that addresses the fundamental limitation of prior text-based speech editing methods: the entanglement of linguistic content and acoustic style in the waveform or codec space. This entanglement causes instability, boundary artifacts, and speaker drift during editing. The proposed solution decouples editing into a semantic token space for content modification and uses a Flow Matching decoder for high-fidelity acoustic reconstruction, guided by a self-consistency reward derived from a frozen pre-trained TTS model to ensure contextual and perceptual alignment.
  - **关键发现**: The proposed method achieved state-of-the-art results across all metrics: lowest WER (indicating highest intelligibility), highest DNSMOS and subjective MOS (superior naturalness), and best speaker similarity preservation—especially under long-duration edits where baselines degraded significantly. In substitution tasks, it matched or exceeded NAR performance; in insertion tasks, it vastly outperformed both AR (which suffered from boundary artifacts) and NAR (which often predicted silence). GRPO alignment provided substantial gains in naturalness and stability without harming speaker similarity. The method showed minimal degradation in performance even with extended masked regions, demonstrating robust long-context modeling.


### 2026-01-30
- **EmoShift: Lightweight Activation Steering for Enhanced Emotion-Aware Speech Synthesis**
  - **作者**: Li Zhou et.al.
  - **arXiv**: [2601.22873](https://arxiv.org/abs/2601.22873)
  - **TLDR**: EmoShift introduces a lightweight activation-steering framework called EmoSteer that enhances emotion-aware speech synthesis by learning emotion-specific steering vectors in the output embedding space, enabling precise emotional control without full model fine-tuning. It achieves superior performance over zero-shot and fully fine-tuned baselines with only 10M trainable parameters—less than 1/30 of full fine-tuning—while preserving naturalness and speaker identity.
  - **核心贡献**: The paper addresses the limitation of existing emotion-aware TTS systems that rely on scaling fixed emotion embeddings or external prompts, which restrict their ability to capture emotion-specific latent characteristics. EmoShift’s core innovation is the EmoSteer layer—a lightweight, plug-and-play module that learns emotion-specific offset vectors in the model’s activation space, enabling controllable, interpretable, and stable emotional expression across utterances without modifying or retraining the base TTS model.
  - **关键发现**: EmoShift outperforms both zero-shot and fully fine-tuned (SFT) baselines across all metrics. In subjective evaluation, it achieves higher MOS and Emo-MOS scores, with listeners preferring EmoShift outputs in pairwise comparisons. Objectively, it improves emotion recognition accuracy—highest for Sad (61.24%) and Neutral (55.84%)—and maintains low WER and high SpkSIM, indicating preserved naturalness and speaker identity. Scaling the steering vector (α) enables smooth control of emotional intensity, validated by monotonic changes in SER accuracy. EmoShift matches or exceeds CosyVoice-SFT-Shift performance despite using <1/30 the trainable parameters.

- **Evaluating and Rewarding LALMs for Expressive Role-Play TTS via Mean Continuation Log-Probability**
  - **作者**: Yong Ren et.al.
  - **arXiv**: [2601.22661](https://arxiv.org/abs/2601.22661)
  - **TLDR**: This paper introduces Mean Continuation Log-Probability (MCLP) as a novel metric and reinforcement learning reward to improve stylistic consistency in Large Audio Language Model (LALM)-based Role-Play Text-to-Speech (RP-TTS). By leveraging in-context learning of pretrained LALMs, MCLP quantifies how well generated speech aligns with character profiles and scene context, significantly outperforming strong baselines in both objective and subjective evaluations on a newly constructed multi-turn RP-TTS dataset.
  - **核心贡献**: The core contribution is the proposal of Mean Continuation Log-Probability (MCLP), which serves dual purposes: (1) as an objective evaluation metric for stylistic consistency in expressive role-play TTS, addressing the lack of reliable style quantification in prior work; and (2) as a reinforcement learning (RL) reward signal to fine-tune LALMs for better alignment with role-play instructions. The innovation lies in using the intrinsic likelihood modeling capability of pretrained LALMs—via in-context learning—to compute the log-probability of ground-truth speech continuations conditioned on model-generated speech, thereby capturing dynamic, context-sensitive stylistic coherence across multi-turn dialogues rather than treating style as static or attribute-based.
  - **关键发现**: The proposed method achieves state-of-the-art performance: MOS of 3.646 (95% CI reported), approaching ground-truth naturalness, and significantly outperforms all baselines in both settings. It attains the lowest CER and Pinyin error rates, indicating high phonetic fidelity. MCLP scores confirm superior stylistic consistency. Notably, while baseline models degrade without audio history, the proposed method maintains robust performance, demonstrating effective use of textual context alone. Ablation shows that combining MCLP and WER in GRPO is critical—using only WER leads to reward hacking (e.g., generating bland speech to minimize errors), while MCLP-only yields unsafe outputs. The full pipeline (SFT + GRPO with combined reward) achieves the best balance. Audio-2-mini shows significant performance drops in role-play scenarios, highlighting the need for instruction-aligned training.

- **Now You Hear Me: Audio Narrative Attacks Against Large Audio-Language Models**
  - **作者**: Ye Yu et.al.
  - **arXiv**: [2601.23255](https://arxiv.org/abs/2601.23255)
  - **TLDR**: This paper introduces 'Audio Narrative Attacks,' a novel jailbreak technique that embeds harmful instructions within narrative-style synthetic speech to bypass safety mechanisms in large audio-language models (LALMs). By leveraging prosodic and stylistic cues via advanced TTS, the method achieves a 98.26% success rate on models like Gemini 2.0 Flash—far exceeding text-only attacks—highlighting critical vulnerabilities in speech-based AI systems.
  - **核心贡献**: The paper identifies and exploits a previously uncharacterized vulnerability in end-to-end large audio-language models: their susceptibility to disallowed content when delivered through narratively structured, prosodically modulated synthetic speech. Unlike traditional text-based jailbreaks, this approach leverages paralinguistic features—such as tone, pacing, and vocal affect—to manipulate model compliance without altering lexical content. The core innovation lies in treating speech not just as a carrier of text but as a modality with its own semantic and behavioral influence, thereby circumventing alignment safeguards calibrated primarily for textual inputs.
  - **关键发现**: The audio narrative attack achieves a 98.26% success rate on Gemini 2.0 Flash, vastly outperforming text-only baselines (which show near-zero success due to robust textual filters). On GPT-4o and Qwen2.5-Omni, the method also shows significant gains over AdvWave and other audio jailbreaks. Tone-only manipulation (without adversarial wording) yields a 10–20% compliance boost across models, proving prosody alone influences behavior. Human speech experiments confirm the effect generalizes beyond synthetic voices. Common failure modes include premature termination (43% of cases) and internal representation instability, especially in compact models like Qwen2.5-Omni. The results indicate that current safety mechanisms fail to account for paralinguistic cues in raw audio inputs.

