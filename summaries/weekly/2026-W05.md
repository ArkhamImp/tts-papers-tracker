# TTS 论文周报
**周期**: 2026-01-23 至 2026-01-30 (第 5 周, 2026 年)
**生成时间**: 2026-02-03 18:20

## 概览
- **论文总数**: 15
- **主题分布**:
  - `合成`: 7
  - `其他`: 4
  - `表现力`: 4
  - `LLM 基础`: 2
  - `多语言`: 2
  - `编辑`: 1
  - `流式`: 1
  - `零样本`: 1

## 重点论文（分析摘要）
- **EmoShift: Lightweight Activation Steering for Enhanced Emotion-Aware Speech Synthesis** (2026-01-30)
  - **作者**: Li Zhou et.al.
  - **arXiv**: [2601.22873](https://arxiv.org/abs/2601.22873)
  - **标签**: expressive, synthesis
  - **TLDR**: EmoShift is a lightweight TTS framework that uses a small trainable EmoSteer layer to learn emotion-specific steering vectors for precise and controllable emotional speech synthesis.
  - **核心贡献**: Introducing the EmoSteer layer, a parameter-efficient activation-steering module that learns a latent offset vector for each target emotion to precisely control expression without extensive retraining.
  - **方法**: A lightweight framework built on a pre-trained TTS model (likely LLM-based). It adds a small EmoSteer layer that learns a steering vector for each emotion category. This vector is applied to the model's output embeddings to shift the speech generation towards the target emotional style, using only ~10M trainable parameters via efficient fine-tuning.
  - **关键发现**: Outperformed zero-shot and fully fine-tuned baselines in objective (e.g., emotion classification accuracy) and subjective (naturalness, expressiveness, similarity) evaluations. The EmoSteer layer effectively captures emotion-specific characteristics and shows potential for controlling emotional intensity.
  - **评估**: medium (评分: 7/10)

- **Evaluating and Rewarding LALMs for Expressive Role-Play TTS via Mean Continuation Log-Probability** (2026-01-30)
  - **作者**: Yong Ren et.al.
  - **arXiv**: [2601.22661](https://arxiv.org/abs/2601.22661)
  - **标签**: expressive, synthesis
  - **TLDR**: The paper proposes Mean Continuation Log-Probability (MCLP) as a novel metric and reinforcement learning reward to improve the stylistic consistency of Large Audio Language Models in expressive role-play text-to-speech.
  - **核心贡献**: Introducing MCLP, a metric derived from a pre-trained LALM's in-context learning capability, to objectively quantify and optimize speaking style consistency in role-play TTS, addressing a key evaluation bottleneck.
  - **方法**: 1) Formulate MCLP: Use a frozen, pre-trained LALM to compute the average log-probability of the ground-truth speech continuation given the generated speech and role-play context (character/scene). 2) Use MCLP as a reward signal in reinforcement learning (PPO) to fine-tune the LALM-based TTS model for better style alignment. 3) Construct a new RP-TTS dataset with detailed annotations to support training and evaluation.
  - **关键发现**: Models fine-tuned with the MCLP reward significantly outperformed strong LALM baselines (e.g., VoiceCraft, AudioPaLM) in both objective metrics (MCLP, speaker similarity) and subjective human evaluations (naturalness, style consistency, role adherence). The MCLP metric showed high correlation with human judgments.
  - **评估**: strong (评分: 8/10)

- **Erasing Your Voice Before It's Heard: Training-free Speaker Unlearning for Zero-shot Text-to-Speech** (2026-01-28)
  - **作者**: Myungjin Lee et.al.
  - **arXiv**: [2601.20481](https://arxiv.org/abs/2601.20481)
  - **标签**: zero-shot, synthesis
  - **摘要**: Modern zero-shot text-to-speech (TTS) models offer unprecedented expressivity but also pose serious crime risks, as they can synthesize voices of individuals who never consented. In this context, speaker unlearning aims to prevent the generation of s...

- **ASR for Affective Speech: Investigating Impact of Emotion and Speech Generative Strategy** (2026-01-28)
  - **作者**: Ya-Tse Wu et.al.
  - **arXiv**: [2601.20319](https://arxiv.org/abs/2601.20319)
  - **标签**: expressive
  - **摘要**: This work investigates how emotional speech and generative strategies affect ASR performance. We analyze speech synthesized from three emotional TTS models and find that substitution errors dominate, with emotional expressiveness varying across model...

- **T-Mimi: A Transformer-based Mimi Decoder for Real-Time On-Phone TTS** (2026-01-27)
  - **作者**: Haibin Wu et.al.
  - **arXiv**: [2601.20094](https://arxiv.org/abs/2601.20094)
  - **标签**: streaming, llm-based, synthesis
  - **摘要**: Neural audio codecs provide promising acoustic features for speech synthesis, with representative streaming codecs like Mimi providing high-quality acoustic features for real-time Text-to-Speech (TTS) applications. However, Mimi's decoder, which empl...

- **Phonological Tokenizer: Prosody-Aware Phonetic Token via Multi-Objective Fine-Tuning with Differentiable K-Means** (2026-01-27)
  - **作者**: Kentaro Onda et.al.
  - **arXiv**: [2601.19781](https://arxiv.org/abs/2601.19781)
  - **标签**: expressive
  - **摘要**: In recent years, there has been growing interest in representing speech with discrete tokens, which serve as pseudo-text for speech language models (speechLMs) and as efficient intermediate representations for downstream tasks. These tokens are typic...

- **SonoEdit: Null-Space Constrained Knowledge Editing for Pronunciation Correction in LLM-Based TTS** (2026-01-23)
  - **作者**: Ayush Pratap Singh et.al.
  - **arXiv**: [2601.17086](https://arxiv.org/abs/2601.17086)
  - **标签**: llm-based, editing, synthesis
  - **摘要**: Neural text-to-speech (TTS) systems systematically mispronounce low-resource proper nouns, particularly non-English names, brands, and geographic locations, due to their underrepresentation in predominantly English training corpora. Existing solution...

## 完整列表（按日期）
### 2026-01-30
- **EmoShift: Lightweight Activation Steering for Enhanced Emotion-Aware Speech Synthesis**
  - **作者**: Li Zhou et.al.
  - **arXiv**: [2601.22873](https://arxiv.org/abs/2601.22873)
  - **TLDR**: EmoShift is a lightweight TTS framework that uses a small trainable EmoSteer layer to learn emotion-specific steering vectors for precise and controllable emotional speech synthesis.
  - **核心贡献**: Introducing the EmoSteer layer, a parameter-efficient activation-steering module that learns a latent offset vector for each target emotion to precisely control expression without extensive retraining.
  - **关键发现**: Outperformed zero-shot and fully fine-tuned baselines in objective (e.g., emotion classification accuracy) and subjective (naturalness, expressiveness, similarity) evaluations. The EmoSteer layer effectively captures emotion-specific characteristics and shows potential for controlling emotional intensity.

- **Evaluating and Rewarding LALMs for Expressive Role-Play TTS via Mean Continuation Log-Probability**
  - **作者**: Yong Ren et.al.
  - **arXiv**: [2601.22661](https://arxiv.org/abs/2601.22661)
  - **TLDR**: The paper proposes Mean Continuation Log-Probability (MCLP) as a novel metric and reinforcement learning reward to improve the stylistic consistency of Large Audio Language Models in expressive role-play text-to-speech.
  - **核心贡献**: Introducing MCLP, a metric derived from a pre-trained LALM's in-context learning capability, to objectively quantify and optimize speaking style consistency in role-play TTS, addressing a key evaluation bottleneck.
  - **关键发现**: Models fine-tuned with the MCLP reward significantly outperformed strong LALM baselines (e.g., VoiceCraft, AudioPaLM) in both objective metrics (MCLP, speaker similarity) and subjective human evaluations (naturalness, style consistency, role adherence). The MCLP metric showed high correlation with human judgments.

- **Now You Hear Me: Audio Narrative Attacks Against Large Audio-Language Models**
  - **作者**: Ye Yu et.al.
  - **arXiv**: [2601.23255](https://arxiv.org/abs/2601.23255)
  - **TLDR**: The paper demonstrates a highly effective audio jailbreak attack that embeds disallowed instructions within narrative-style synthetic speech to bypass safety mechanisms in large audio-language models.
  - **核心贡献**: Introducing and characterizing a new class of vulnerability—audio narrative attacks—that exploits the modality gap between text and speech safety alignment in large audio-language models.
  - **关键发现**: The attack achieved a 98.26% success rate on Gemini 2.0 Flash, substantially outperforming text-only baselines; it showed high effectiveness across multiple state-of-the-art models, highlighting a critical vulnerability in speech-based interfaces.


### 2026-01-29
- **Speech Quality-Based Localization of Low-Quality Speech and Text-to-Speech Synthesis Artefacts**
  - **作者**: Michael Kuhlmann et.al.
  - **arXiv**: [2601.21886](https://arxiv.org/abs/2601.21886)
  - **摘要**: A large number of works view the automatic assessment of speech from an utterance- or system-level perspective. While such approaches are good in judg...

- **Unit-Based Agent for Semi-Cascaded Full-Duplex Dialogue Systems**
  - **作者**: Haoyuan Yu et.al.
  - **arXiv**: [2601.20230](https://arxiv.org/abs/2601.20230)
  - **摘要**: Full-duplex voice interaction is crucial for natural human computer interaction. We present a framework that decomposes complex dialogue into minimal ...


### 2026-01-28
- **ASR for Affective Speech: Investigating Impact of Emotion and Speech Generative Strategy**
  - **作者**: Ya-Tse Wu et.al.
  - **arXiv**: [2601.20319](https://arxiv.org/abs/2601.20319)
  - **摘要**: This work investigates how emotional speech and generative strategies affect ASR performance. We analyze speech synthesized from three emotional TTS m...

- **Audio Deepfake Detection in the Age of Advanced Text-to-Speech models**
  - **作者**: Robin Singh et.al.
  - **arXiv**: [2601.20510](https://arxiv.org/abs/2601.20510)
  - **摘要**: Recent advances in Text-to-Speech (TTS) systems have substantially increased the realism of synthetic speech, raising new challenges for audio deepfak...

- **Erasing Your Voice Before It's Heard: Training-free Speaker Unlearning for Zero-shot Text-to-Speech**
  - **作者**: Myungjin Lee et.al.
  - **arXiv**: [2601.20481](https://arxiv.org/abs/2601.20481)
  - **摘要**: Modern zero-shot text-to-speech (TTS) models offer unprecedented expressivity but also pose serious crime risks, as they can synthesize voices of indi...


### 2026-01-27
- **Phonological Tokenizer: Prosody-Aware Phonetic Token via Multi-Objective Fine-Tuning with Differentiable K-Means**
  - **作者**: Kentaro Onda et.al.
  - **arXiv**: [2601.19781](https://arxiv.org/abs/2601.19781)
  - **摘要**: In recent years, there has been growing interest in representing speech with discrete tokens, which serve as pseudo-text for speech language models (s...

- **Rethinking Discrete Speech Representation Tokens for Accent Generation**
  - **作者**: Jinzuomu Zhong et.al.
  - **arXiv**: [2601.19786](https://arxiv.org/abs/2601.19786)
  - **摘要**: Discrete Speech Representation Tokens (DSRTs) have become a foundational component in speech generation. While prior work has extensively studied phon...

- **T-Mimi: A Transformer-based Mimi Decoder for Real-Time On-Phone TTS**
  - **作者**: Haibin Wu et.al.
  - **arXiv**: [2601.20094](https://arxiv.org/abs/2601.20094)
  - **摘要**: Neural audio codecs provide promising acoustic features for speech synthesis, with representative streaming codecs like Mimi providing high-quality ac...


### 2026-01-26
- **UrgentMOS: Unified Multi-Metric and Preference Learning for Robust Speech Quality Assessment**
  - **作者**: Wei Wang et.al.
  - **arXiv**: [2601.18438](https://arxiv.org/abs/2601.18438)
  - **摘要**: Automatic speech quality assessment has become increasingly important as modern speech generation systems continue to advance, while human listening t...


### 2026-01-25
- **AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation**
  - **作者**: Dongjie Cheng et.al.
  - **arXiv**: [2601.17761](https://arxiv.org/abs/2601.17761)
  - **摘要**: Real-world perception and interaction are inherently multimodal, encompassing not only language but also vision and speech, which motivates the develo...

- **Quran-MD: A Fine-Grained Multilingual Multimodal Dataset of the Quran**
  - **作者**: Muhammad Umar Salman et.al.
  - **arXiv**: [2601.17880](https://arxiv.org/abs/2601.17880)
  - **摘要**: We present Quran MD, a comprehensive multimodal dataset of the Quran that integrates textual, linguistic, and audio dimensions at the verse and word l...


### 2026-01-23
- **SonoEdit: Null-Space Constrained Knowledge Editing for Pronunciation Correction in LLM-Based TTS**
  - **作者**: Ayush Pratap Singh et.al.
  - **arXiv**: [2601.17086](https://arxiv.org/abs/2601.17086)
  - **摘要**: Neural text-to-speech (TTS) systems systematically mispronounce low-resource proper nouns, particularly non-English names, brands, and geographic loca...

