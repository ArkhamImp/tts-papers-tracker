# TTS 论文日报
**日期**: 2026-01-30
**生成时间**: 2026-02-03 17:58

## 概览
- **论文数**: 3
- **主题分布**:
  - `expressive`: 2
  - `synthesis`: 2
  - `multilingual`: 1

## 重点论文（分析摘要）
- **EmoShift: Lightweight Activation Steering for Enhanced Emotion-Aware Speech Synthesis**
  - 作者: Li Zhou et.al.
  - arXiv: [2601.22873](https://arxiv.org/abs/2601.22873)
  - 标签: expressive, synthesis
  - TLDR: EmoShift is a lightweight TTS framework that uses a small trainable EmoSteer layer to learn emotion-specific steering vectors for precise and controllable emotional speech synthesis.
  - 核心贡献: Introducing the EmoSteer layer, a parameter-efficient activation-steering module that learns a latent offset vector for each target emotion to precisely control expression without extensive retraining.
  - 方法: A lightweight framework built on a pre-trained TTS model (likely LLM-based). It adds a small EmoSteer layer that learns a steering vector for each emotion category. This vector is applied to the model's output embeddings to shift the speech generation towards the target emotional style, using only ~10M trainable parameters via efficient fine-tuning.
  - 关键发现: Outperformed zero-shot and fully fine-tuned baselines in objective (e.g., emotion classification accuracy) and subjective (naturalness, expressiveness, similarity) evaluations. The EmoSteer layer effectively captures emotion-specific characteristics and shows potential for controlling emotional intensity.
  - 局限性: Implied limitations include evaluation on a specific set of emotions; generalizability to unseen or complex emotional blends is not fully explored. The paper's arXiv ID (2601.22873) is anachronistic (year 2601), suggesting a placeholder, which may indicate a preliminary version.
  - 评估: medium (评分: 7/10)

- **Evaluating and Rewarding LALMs for Expressive Role-Play TTS via Mean Continuation Log-Probability**
  - 作者: Yong Ren et.al.
  - arXiv: [2601.22661](https://arxiv.org/abs/2601.22661)
  - 标签: expressive, synthesis
  - TLDR: The paper proposes Mean Continuation Log-Probability (MCLP) as a novel metric and reinforcement learning reward to improve the stylistic consistency of Large Audio Language Models in expressive role-play text-to-speech.
  - 核心贡献: Introducing MCLP, a metric derived from a pre-trained LALM's in-context learning capability, to objectively quantify and optimize speaking style consistency in role-play TTS, addressing a key evaluation bottleneck.
  - 方法: 1) Formulate MCLP: Use a frozen, pre-trained LALM to compute the average log-probability of the ground-truth speech continuation given the generated speech and role-play context (character/scene). 2) Use MCLP as a reward signal in reinforcement learning (PPO) to fine-tune the LALM-based TTS model for better style alignment. 3) Construct a new RP-TTS dataset with detailed annotations to support training and evaluation.
  - 关键发现: Models fine-tuned with the MCLP reward significantly outperformed strong LALM baselines (e.g., VoiceCraft, AudioPaLM) in both objective metrics (MCLP, speaker similarity) and subjective human evaluations (naturalness, style consistency, role adherence). The MCLP metric showed high correlation with human judgments.
  - 局限性: The method relies on the quality and bias of the frozen pre-trained LALM used to compute MCLP. The constructed dataset, while valuable, may be limited in scale and diversity. The approach is computationally intensive due to RL fine-tuning. Generalization to unseen character styles or languages is not thoroughly explored.
  - 评估: strong (评分: 8/10)

## 完整列表（带分析摘要）
- **EmoShift: Lightweight Activation Steering for Enhanced Emotion-Aware Speech Synthesis**
  - 作者: Li Zhou et.al.
  - arXiv: [2601.22873](https://arxiv.org/abs/2601.22873)
  - TLDR: EmoShift is a lightweight TTS framework that uses a small trainable EmoSteer layer to learn emotion-specific steering vectors for precise and controllable emotional speech synthesis.
  - 核心贡献: Introducing the EmoSteer layer, a parameter-efficient activation-steering module that learns a latent offset vector for each target emotion to precisely control expression without extensive retraining.
  - 关键发现: Outperformed zero-shot and fully fine-tuned baselines in objective (e.g., emotion classification accuracy) and subjective (naturalness, expressiveness, similarity) evaluations. The EmoSteer layer effectively captures emotion-specific characteristics and shows potential for controlling emotional intensity.

- **Evaluating and Rewarding LALMs for Expressive Role-Play TTS via Mean Continuation Log-Probability**
  - 作者: Yong Ren et.al.
  - arXiv: [2601.22661](https://arxiv.org/abs/2601.22661)
  - TLDR: The paper proposes Mean Continuation Log-Probability (MCLP) as a novel metric and reinforcement learning reward to improve the stylistic consistency of Large Audio Language Models in expressive role-play text-to-speech.
  - 核心贡献: Introducing MCLP, a metric derived from a pre-trained LALM's in-context learning capability, to objectively quantify and optimize speaking style consistency in role-play TTS, addressing a key evaluation bottleneck.
  - 关键发现: Models fine-tuned with the MCLP reward significantly outperformed strong LALM baselines (e.g., VoiceCraft, AudioPaLM) in both objective metrics (MCLP, speaker similarity) and subjective human evaluations (naturalness, style consistency, role adherence). The MCLP metric showed high correlation with human judgments.

- **Now You Hear Me: Audio Narrative Attacks Against Large Audio-Language Models**
  - 作者: Ye Yu et.al.
  - arXiv: [2601.23255](https://arxiv.org/abs/2601.23255)
  - TLDR: The paper demonstrates a highly effective audio jailbreak attack that embeds disallowed instructions within narrative-style synthetic speech to bypass safety mechanisms in large audio-language models.
  - 核心贡献: Introducing and characterizing a new class of vulnerability—audio narrative attacks—that exploits the modality gap between text and speech safety alignment in large audio-language models.
  - 关键发现: The attack achieved a 98.26% success rate on Gemini 2.0 Flash, substantially outperforming text-only baselines; it showed high effectiveness across multiple state-of-the-art models, highlighting a critical vulnerability in speech-based interfaces.
