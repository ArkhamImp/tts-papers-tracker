# TTS è®ºæ–‡æ—¥æŠ¥
**æ—¥æœŸ**: 2026-02-05
**ç”Ÿæˆæ—¶é—´**: 2026-02-08 21:06

## æ¦‚è§ˆ
- **è®ºæ–‡æ•°**: 3
- **ä¸»é¢˜åˆ†å¸ƒ**:
  - `synthesis`: 2
  - `zero-shot`: 1
  - `codec`: 1

## ç»Ÿè®¡ä¿¡æ¯

- **å¹³å‡è¯„åˆ†**: 7.7/10
- **é«˜è¯„åˆ†è®ºæ–‡**: 3
- **ä¸­ç­‰è¯„åˆ†è®ºæ–‡**: 0
- **ä½è¯„åˆ†è®ºæ–‡**: 0

## æŠ€æœ¯å¤æ‚åº¦åˆ†å¸ƒ

- **high. The system involves multiple complex, state-of-the-art components: two different non-autoregressive TTS models with custom duration modeling, a multi-stage speech enhancement pipeline with a specialized model (Sidon), and a finetuning strategy on a challenging dataset. The integration and systematic evaluation of these components require significant expertise in speech synthesis and signal processing.**: 1
- **high. The method integrates multiple advanced generative modeling paradigms (Diffusion, GAN, VAE-inspired priors) into a single framework. Implementing the trainable prior with aligned posterior encoders in the time-frequency domain, alongside the multi-scale upsampling WaveFit generator and adversarial training, requires significant expertise in deep learning for audio and careful tuning of interdependent loss functions and hyperparameters.**: 1
- **high**: 1

## é‡ç‚¹è®ºæ–‡ï¼ˆè¯¦ç»†åˆ†æï¼‰

### ğŸ“„ Zero-Shot TTS With Enhanced Audio Prompts: Bsc Submission For The 2026 Wildspoof Challenge TTS Track

- **ä½œè€…**: Jose Giraldo et.al.
- **arXiv**: [2602.05770](https://arxiv.org/abs/2602.05770)
- **æ ‡ç­¾**: zero-shot, synthesis

#### ğŸ¯ TLDR
This paper presents a submission to the 2026 Wildspoof Challenge TTS Track, focusing on zero-shot TTS for spontaneous, in-the-wild speech. The core approach combines non-autoregressive TTS models (StyleTTS2 and F5-TTS) with a novel multi-stage audio enhancement pipeline using the Sidon model to clean noisy reference prompts. Key findings demonstrate that finetuning on enhanced audio significantly improves synthesis robustness and that prompt quality and length are critical factors for zero-shot performance.

#### ğŸ” æ ¸å¿ƒè´¡çŒ®
The main contribution is a comprehensive system for zero-shot TTS synthesis capable of handling the spontaneous and noisy nature of in-the-wild speech. It solves the problem of generating natural, prosodically varied speech from low-quality, real-world audio prompts. The innovation lies in the synergistic integration of a state-of-the-art speech enhancement model (Sidon) with modern non-autoregressive TTS architectures, coupled with a systematic analysis of how the quality and duration of the reference audio prompt impact the final synthesis output.

#### ğŸ› ï¸ æŠ€æœ¯æ–¹æ³•
The approach uses two non-autoregressive TTS architectures: StyleTTS2 and F5-TTS, chosen for their ability to model spontaneous speech variations. A key technical component is flexible duration modeling to improve prosodic naturalness. To handle acoustic noise in reference prompts, a multi-stage enhancement pipeline is implemented using the Sidon model, which is reported to outperform the standard Demucs model in signal quality. The models are finetuned on the TTS In the Wild (TITW) dataset. The paper also details experiments on prompt length (short vs. long) and the impact of enhancement on model performance, using strategies like 'long prompt' for inference.

#### ğŸ’¡ å…³é”®åˆ›æ–°
- 1
- .
-  
- A
- p
- p
- l
- i
- c
- a
- t
- i
- o
- n
-  
- o
- f
-  
- t
- h
- e
-  
- S
- i
- d
- o
- n
-  
- m
- o
- d
- e
- l
-  
- f
- o
- r
-  
- m
- u
- l
- t
- i
- -
- s
- t
- a
- g
- e
-  
- e
- n
- h
- a
- n
- c
- e
- m
- e
- n
- t
-  
- o
- f
-  
- i
- n
- -
- t
- h
- e
- -
- w
- i
- l
- d
-  
- a
- u
- d
- i
- o
-  
- p
- r
- o
- m
- p
- t
- s
- ,
-  
- p
- r
- o
- v
- i
- d
- i
- n
- g
-  
- c
- l
- e
- a
- n
- e
- r
-  
- c
- o
- n
- d
- i
- t
- i
- o
- n
- i
- n
- g
-  
- s
- i
- g
- n
- a
- l
- s
-  
- f
- o
- r
-  
- z
- e
- r
- o
- -
- s
- h
- o
- t
-  
- T
- T
- S
- .
-  
- 2
- .
-  
- S
- y
- s
- t
- e
- m
- a
- t
- i
- c
-  
- e
- m
- p
- i
- r
- i
- c
- a
- l
-  
- a
- n
- a
- l
- y
- s
- i
- s
-  
- o
- f
-  
- t
- h
- e
-  
- r
- e
- l
- a
- t
- i
- o
- n
- s
- h
- i
- p
-  
- b
- e
- t
- w
- e
- e
- n
-  
- r
- e
- f
- e
- r
- e
- n
- c
- e
-  
- p
- r
- o
- m
- p
- t
-  
- q
- u
- a
- l
- i
- t
- y
- /
- l
- e
- n
- g
- t
- h
-  
- a
- n
- d
-  
- z
- e
- r
- o
- -
- s
- h
- o
- t
-  
- T
- T
- S
-  
- p
- e
- r
- f
- o
- r
- m
- a
- n
- c
- e
- ,
-  
- p
- r
- o
- v
- i
- d
- i
- n
- g
-  
- p
- r
- a
- c
- t
- i
- c
- a
- l
-  
- g
- u
- i
- d
- e
- l
- i
- n
- e
- s
- .
-  
- 3
- .
-  
- C
- o
- m
- b
- i
- n
- i
- n
- g
-  
- f
- l
- e
- x
- i
- b
- l
- e
-  
- d
- u
- r
- a
- t
- i
- o
- n
-  
- m
- o
- d
- e
- l
- i
- n
- g
-  
- i
- n
-  
- n
- o
- n
- -
- a
- u
- t
- o
- r
- e
- g
- r
- e
- s
- s
- i
- v
- e
-  
- a
- r
- c
- h
- i
- t
- e
- c
- t
- u
- r
- e
- s
-  
- (
- S
- t
- y
- l
- e
- T
- T
- S
- 2
- ,
-  
- F
- 5
- -
- T
- T
- S
- )
-  
- s
- p
- e
- c
- i
- f
- i
- c
- a
- l
- l
- y
-  
- f
- o
- r
-  
- t
- h
- e
-  
- c
- h
- a
- l
- l
- e
- n
- g
- e
- s
-  
- o
- f
-  
- s
- p
- o
- n
- t
- a
- n
- e
- o
- u
- s
-  
- s
- p
- e
- e
- c
- h
- .
-  
- 4
- .
-  
- D
- e
- m
- o
- n
- s
- t
- r
- a
- t
- i
- n
- g
-  
- t
- h
- a
- t
-  
- f
- i
- n
- e
- t
- u
- n
- i
- n
- g
-  
- T
- T
- S
-  
- m
- o
- d
- e
- l
- s
-  
- o
- n
-  
- e
- n
- h
- a
- n
- c
- e
- d
-  
- a
- u
- d
- i
- o
- ,
-  
- r
- a
- t
- h
- e
- r
-  
- t
- h
- a
- n
-  
- j
- u
- s
- t
-  
- u
- s
- i
- n
- g
-  
- e
- n
- h
- a
- n
- c
- e
- m
- e
- n
- t
-  
- a
- t
-  
- i
- n
- f
- e
- r
- e
- n
- c
- e
- ,
-  
- y
- i
- e
- l
- d
- s
-  
- s
- u
- p
- e
- r
- i
- o
- r
-  
- r
- o
- b
- u
- s
- t
- n
- e
- s
- s
- .

#### ğŸ“Š å…³é”®å‘ç°
1. Finetuning on Sidon-enhanced audio yields superior performance, achieving up to 4.21 UTMOS and 3.47 DNSMOS. 2. Using longer audio prompts consistently improves speaker similarity scores for both models. 3. Audio enhancement significantly improves quality metrics (UTMOS, DNSMOS) but does not degrade intelligibility (WER is unaffected). 4. The impact of enhancement varies by model architecture; it is more crucial for F5-TTS. 5. Sidon model significantly outperforms standard Demucs in signal quality for this task.

#### âš™ï¸ æŠ€æœ¯ä¼˜åŠ¿
1. Pragmatic, hybrid approach addressing both data (enhancement) and model (architecture choice, finetuning) aspects of the problem. 2. Use of non-autoregressive models is computationally efficient and suitable for spontaneous speech. 3. Comprehensive evaluation using multiple relevant metrics (quality, similarity, intelligibility). 4. The focus on prompt quality and length provides actionable insights for deploying zero-shot TTS systems in real-world conditions.

#### âš ï¸ å±€é™æ€§
1. The paper notes that F5-TTS is known to have content preservation issues, which is a model-specific constraint. 2. The enhancement pipeline adds complexity and computational cost to the inference process. 3. Findings on prompt length and enhancement effects are specific to the tested architectures (StyleTTS2, F5-TTS) and may not generalize to all TTS models. 4. The analysis, while thorough, is based on a specific challenge/dataset (Wildspoof/TITW), and generalizability to other 'in-the-wild' conditions is not fully established. 5. The paper is a challenge submission report, so the depth of architectural modifications and ablation studies may be limited.

#### ğŸš€ æœªæ¥å·¥ä½œ
Explicitly mentioned: Investigation of adaptive prompt length strategies. Suggested from context: 1. Exploring other advanced speech enhancement models. 2. Mitigating content preservation issues in models like F5-TTS. 3. Extending the analysis to a wider range of TTS architectures. 4. Developing end-to-end models or training procedures that are inherently more robust to noisy prompts, potentially reducing the need for a separate enhancement stage.

#### ğŸ“ˆ å®é™…åº”ç”¨
1. Generating realistic synthetic voices for content creation (e.g., audiobooks, videos) using only short, noisy user recordings. 2. Improving accessibility tools by allowing users to clone their voice from everyday speech samples. 3. Enhancing human-computer interaction with more natural and expressive synthetic voices that can mimic spontaneous speech patterns. 4. Potential use in entertainment and gaming for dynamic voice generation for characters.

#### ğŸ”— ç›¸å…³å·¥ä½œ
This work sits at the intersection of several active TTS research areas: zero-shot voice cloning, non-autoregressive TTS models (following StyleTTS2 and F5-TTS), and robust TTS for noisy/unconstrained data. It directly builds upon the StyleTTS2 and F5-TTS architectures. It relates to prior work on speech enhancement for TTS (e.g., using Demucs) by proposing a superior alternative (Sidon). The paper's context is the Wildspoof Challenge, which focuses on anti-spoofing and realistic synthesis, positioning it as applied research pushing the frontier of practical, robust TTS systems.

#### ğŸ¯ è¯„ä¼°
**è¯„åˆ†**: 7/10
**è¯„ä¼°ç»“æœ**: medium. The experimental design is clear and addresses relevant variables (prompt length, enhancement). It uses standard and appropriate metrics (UTMOS, DNSMOS, WER) and compares against a relevant baseline (Demucs). However, as a challenge submission, it may lack extensive ablation studies (e.g., isolating the contribution of flexible duration modeling) and comparison to a broader set of state-of-the-art TTS models. The reliance on a single primary dataset (TITW) also limits the assessment of generalizability.

#### ğŸ”§ æŠ€æœ¯å¤æ‚åº¦
**å¤æ‚åº¦**: high. The system involves multiple complex, state-of-the-art components: two different non-autoregressive TTS models with custom duration modeling, a multi-stage speech enhancement pipeline with a specialized model (Sidon), and a finetuning strategy on a challenging dataset. The integration and systematic evaluation of these components require significant expertise in speech synthesis and signal processing.

---

## æ‰€æœ‰è®ºæ–‡åˆ—è¡¨

### 1. Wave-Trainer-Fit: Neural Vocoder with Trainable Prior and Fixed-Point Iteration towards High-Quality Speech Generation from SSL features
- **ä½œè€…**: Hien Ohnaka et.al.
- **arXiv**: [2602.05443](https://arxiv.org/abs/2602.05443)
- **è¯„åˆ†**: 8/10
- **å¤æ‚åº¦**: high. The method integrates multiple advanced generative modeling paradigms (Diffusion, GAN, VAE-inspired priors) into a single framework. Implementing the trainable prior with aligned posterior encoders in the time-frequency domain, alongside the multi-scale upsampling WaveFit generator and adversarial training, requires significant expertise in deep learning for audio and careful tuning of interdependent loss functions and hyperparameters.
- **æ ‡ç­¾**: codec
- **TLDR**: WaveTrainerFit is a neural vocoder that generates high-quality speech from Self-Supervised Learning (SSL) features. It improves upon the WaveFit vocoder by introducing a trainable prior and reference-aware gain adjustment, enabling faster, higher-quality synthesis with fewer inference steps. The method is robust to the depth of SSL feature extraction and outperforms baselines in both objective and subjective metrics.
- **å…³é”®å‘ç°**: 1. WaveTrainerFit outperformed WaveFit baselines in all objective metrics (SpeechBERTScore, Speaker Similarity, LSD, F0 Corr) and subjective N-MOS/S-MOS across multiple SSL models (WavLM, XLS-R, Whisper) at T=5 steps. 2. It achieved high quality with only 5 inference steps, whereas WaveFit required more. 3. It showed robust performance across different SSL feature extraction depths (layers 2, 8, 24), outperforming the baseline at all layers, with particularly strong gains in speaker similarity for deeper layers. 4. The method's RTF was comparable to the baseline, confirming efficiency gains are from reduced steps, not slower per-step computation. 5. Performance varied by SSL model, with WavLM features yielding best results.

### 2. ARCHI-TTS: A flow-matching-based Text-to-Speech Model with Self-supervised Semantic Aligner and Accelerated Inference
- **ä½œè€…**: Chunyat Wu et.al.
- **arXiv**: [2602.05207](https://arxiv.org/abs/2602.05207)
- **è¯„åˆ†**: 8/10
- **å¤æ‚åº¦**: high
- **æ ‡ç­¾**: synthesis
- **TLDR**: ARCHI-TTS is a non-autoregressive, flow-matching-based TTS model that addresses two major challenges in diffusion-based TTS: robust text-speech alignment and high inference cost. It introduces a dedicated self-supervised semantic aligner for temporal consistency and an efficient inference strategy that reuses encoder features to drastically accelerate synthesis. The model achieves state-of-the-art word error rates on standard benchmarks while maintaining high synthesis quality and efficiency.
- **å…³é”®å‘ç°**: 1) **SOTA Intelligibility**: Achieves WER of 1.98% on LibriSpeech-PC test-clean, and 1.47%/1.42% on SeedTTS test-en/test-zh, outperforming all compared models. 2) **High Efficiency**: Can generate 10-second audio in 0.4 seconds on an A100 GPU (RTF ~0.04), significantly faster than typical diffusion models. 3) **Ablation Results**: The semantic aligner and CTC loss are crucial for low WER. Speaker embeddings improve MOS. The VAE latent representation is effective; a VQ-regularized variant showed benefits. 4) **Feature Reuse Trade-off**: High encoder feature sharing ratio greatly accelerates inference but causes some performance drop; a balance is needed (e.g., at NFE=32, it achieves a good speed-quality trade-off). 5) **Multi-lingual Capability**: Performs well on both English and Chinese test sets within a single model.

### 3. Zero-Shot TTS With Enhanced Audio Prompts: Bsc Submission For The 2026 Wildspoof Challenge TTS Track
- **ä½œè€…**: Jose Giraldo et.al.
- **arXiv**: [2602.05770](https://arxiv.org/abs/2602.05770)
- **è¯„åˆ†**: 7/10
- **å¤æ‚åº¦**: high. The system involves multiple complex, state-of-the-art components: two different non-autoregressive TTS models with custom duration modeling, a multi-stage speech enhancement pipeline with a specialized model (Sidon), and a finetuning strategy on a challenging dataset. The integration and systematic evaluation of these components require significant expertise in speech synthesis and signal processing.
- **æ ‡ç­¾**: zero-shot, synthesis
- **TLDR**: This paper presents a submission to the 2026 Wildspoof Challenge TTS Track, focusing on zero-shot TTS for spontaneous, in-the-wild speech. The core approach combines non-autoregressive TTS models (StyleTTS2 and F5-TTS) with a novel multi-stage audio enhancement pipeline using the Sidon model to clean noisy reference prompts. Key findings demonstrate that finetuning on enhanced audio significantly improves synthesis robustness and that prompt quality and length are critical factors for zero-shot performance.
- **å…³é”®å‘ç°**: 1. Finetuning on Sidon-enhanced audio yields superior performance, achieving up to 4.21 UTMOS and 3.47 DNSMOS. 2. Using longer audio prompts consistently improves speaker similarity scores for both models. 3. Audio enhancement significantly improves quality metrics (UTMOS, DNSMOS) but does not degrade intelligibility (WER is unaffected). 4. The impact of enhancement varies by model architecture; it is more crucial for F5-TTS. 5. Sidon model significantly outperforms standard Demucs in signal quality for this task.

## è¶‹åŠ¿åˆ†æ

### ğŸ”¥ çƒ­é—¨ä¸»é¢˜

### ğŸ“ˆ æŠ€æœ¯è¶‹åŠ¿
- zero-shot æŠ€æœ¯çƒ­åº¦æ¥è¿‘
- zero-shot æŠ€æœ¯çƒ­åº¦æ¥è¿‘
- zero-shot æŠ€æœ¯çƒ­åº¦æ¥è¿‘

### âš¡ å¢é•¿ç‡
- è¾ƒå‰æ—¥å¢é•¿: 50.0%

### ğŸ¯ æ½œåœ¨æŒ‘æˆ˜
- æŠ€æœ¯æŒ‘æˆ˜ä»å­˜åœ¨
- è¯„ä¼°æ–¹æ³•éœ€è¦ä¼˜åŒ–
- æŠ€æœ¯æŒ‘æˆ˜ä»å­˜åœ¨
- è¯„ä¼°æ–¹æ³•éœ€è¦ä¼˜åŒ–
- è¯„ä¼°æ–¹æ³•éœ€è¦ä¼˜åŒ–
- è¯„ä¼°æ–¹æ³•éœ€è¦ä¼˜åŒ–
- æŠ€æœ¯æŒ‘æˆ˜ä»å­˜åœ¨

---

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: 2026-02-08 21:06:24*
*æ•°æ®æ¥æº: analysis_cache.json*
