# TTS 论文月报
**月份**: 2026-01
**生成时间**: 2026-02-04 09:52

## 概览
- **论文总数**: 56
- **主题分布**:
  - `合成`: 22
  - `其他`: 17
  - `表现力`: 12
  - `多语言`: 9
  - `零样本`: 7
  - `LLM 基础`: 4
  - `编辑`: 3
  - `编解码器`: 3
  - `流式`: 2

## 重点论文（分析摘要）
- **EmoShift: Lightweight Activation Steering for Enhanced Emotion-Aware Speech Synthesis** (2026-01-30)
  - **作者**: Li Zhou et.al.
  - **arXiv**: [2601.22873](https://arxiv.org/abs/2601.22873)
  - **标签**: expressive, synthesis
  - **TLDR**: EmoShift introduces a lightweight activation-steering framework for emotion-aware TTS that learns emotion-specific steering vectors to enhance expressiveness while preserving naturalness and speaker similarity.
  - **核心贡献**: Proposes EmoShift, a parameter-efficient activation-steering method with an EmoSteer layer that learns emotion-specific steering vectors to control emotional expression in TTS without full fine-tuning.
  - **方法**: EmoShift inserts an EmoSteer layer into an LLM-based TTS model to learn steering vectors for each emotion, capturing latent offsets in the output embedding space. The model is trained with only 10M parameters (less than 1/30 of full fine-tuning) using negative log-likelihood minimization.
  - **关键发现**: EmoShift outperforms zero-shot and fully fine-tuned baselines in both objective (Word Error Rate, Speaker Similarity, Emotion Recognition Accuracy) and subjective (Mean Opinion Score, Emotion MOS) evaluations while maintaining naturalness and speaker similarity.
  - **评估**: medium (评分: 7/10)

- **Evaluating and Rewarding LALMs for Expressive Role-Play TTS via Mean Continuation Log-Probability** (2026-01-30)
  - **作者**: Yong Ren et.al.
  - **arXiv**: [2601.22661](https://arxiv.org/abs/2601.22661)
  - **标签**: expressive, synthesis
  - **TLDR**: A paper introducing Mean Continuation Log-Probability (MCLP) as both an evaluation metric and reward signal for improving expressive role-play TTS using Large Audio Language Models.
  - **核心贡献**: Proposes MCLP to quantify stylistic consistency in role-play TTS and uses it as a reinforcement learning reward to enhance style alignment with instructions.
  - **方法**: Leverages in-context learning of pre-trained LALMs to compute MCLP via continuation log-probability prediction, measuring likelihood of ground-truth speech conditioned on generated speech. Employs MCLP as RL reward using GRPO to align RP-TTS model with role-play instructions.
  - **关键发现**: Significant improvements over strong LALM baselines on both objective (WER, CER, Pinyin Error Rate) and subjective metrics (MOS approaching ground truth at 3.646), with exceptionally low CER and Pinyin error rates.
  - **评估**: strong (评分: 8/10)

- **Erasing Your Voice Before It's Heard: Training-free Speaker Unlearning for Zero-shot Text-to-Speech** (2026-01-28)
  - **作者**: Myungjin Lee et.al.
  - **arXiv**: [2601.20481](https://arxiv.org/abs/2601.20481)
  - **标签**: zero-shot, synthesis
  - **TLDR**: TruS introduces a training-free speaker unlearning method for zero-shot TTS that suppresses specific speaker identities at inference time without retraining.
  - **核心贡献**: The first training-free speaker unlearning framework that generalizes to both seen and unseen opt-out speakers by steering identity-specific hidden activations in TTS models.
  - **方法**: TruS works by steering internal representations in TTS models to suppress identity-related activations using an ID-prototype computed from intermediate features and a pretrained speaker verification model, operating entirely at inference time on pretrained models like F5-TTS.
  - **关键发现**: TruS effectively prevents voice generation of both seen and unseen opt-out speakers while preserving other attributes like prosody and emotion, achieving comparable unlearning performance to retraining-based methods without the computational cost.
  - **评估**: medium (评分: 7/10)

- **ASR for Affective Speech: Investigating Impact of Emotion and Speech Generative Strategy** (2026-01-28)
  - **作者**: Ya-Tse Wu et.al.
  - **arXiv**: [2601.20319](https://arxiv.org/abs/2601.20319)
  - **标签**: expressive
  - **TLDR**: This paper investigates how emotional speech and generative strategies affect ASR performance, introducing two fine-tuning approaches that improve emotion-aware ASR without degrading clean speech recognition.
  - **核心贡献**: Introduces two generative strategies (transcription correctness and emotional salience) for constructing fine-tuning subsets that improve ASR performance on emotional speech while maintaining neutral speech recognition.
  - **方法**: Analyzes speech synthesized from three emotional TTS models (EmoVoice, MaskGCT, CosyVoice2), identifies substitution errors as dominant issue, and proposes two generative strategies for fine-tuning Qwen2-audio-7B model using supervised fine-tuning with LoRA adapters.
  - **关键发现**: Fine-tuning with generative strategies achieves consistent WER improvements on real emotional datasets (MSP Podcast Test1/2, IEMOCAP) without degradation on clean LibriSpeech; combined TTS-EMO-G strategy shows strongest gains, particularly for expressive speech.
  - **评估**: medium (评分: 7/10)

- **T-Mimi: A Transformer-based Mimi Decoder for Real-Time On-Phone TTS** (2026-01-27)
  - **作者**: Haibin Wu et.al.
  - **arXiv**: [2601.20094](https://arxiv.org/abs/2601.20094)
  - **标签**: streaming, llm-based, synthesis
  - **TLDR**: T-Mimi replaces Mimi's convolutional decoder with a transformer-only architecture to achieve 9x faster on-device TTS with minimal quality loss.
  - **核心贡献**: A purely transformer-based decoder for the Mimi codec that reduces on-device TTS latency from 42.1ms to 4.4ms while maintaining audio quality.
  - **方法**: Replaces convolutional components in Mimi's decoder with transformer layers inspired by TS3-Codec, employs quantization-aware training with mixed precision (8-bit for most layers, 32-bit for final layers), and optimizes for mobile CPU efficiency using XNNPACK.
  - **关键发现**: Achieves 9x latency reduction (42.1ms → 4.4ms), maintains comparable audio quality (PESQ scores near baseline), 8-bit quantization reduces storage by 75% with minimal quality loss, final two transformer layers require 32-bit precision for quality preservation.
  - **评估**: medium (评分: 7/10)

- **Phonological Tokenizer: Prosody-Aware Phonetic Token via Multi-Objective Fine-Tuning with Differentiable K-Means** (2026-01-27)
  - **作者**: Kentaro Onda et.al.
  - **arXiv**: [2601.19781](https://arxiv.org/abs/2601.19781)
  - **标签**: expressive
  - **TLDR**: Phonological Tokenizer fine-tunes phonetic tokens via differentiable k-means with multi-task ASR and speech resynthesis objectives to better capture linguistic and prosodic information while discarding speaker identity.
  - **核心贡献**: Introduces a phonological tokenizer that effectively incorporates prosodic information through multi-objective fine-tuning of SSL models, achieving superior performance in prosody-sensitive tasks compared to existing acoustic and phonetic tokenizers.
  - **方法**: Fine-tunes WavLM-large SSL features using differentiable k-means clustering with weighted ASR and speech resynthesis losses, trained on LibriSpeech-100h subset, evaluated across discriminative (ASR, ER, SID), generative (reconstruction, VC), and speechLM tasks.
  - **关键发现**: Outperforms baselines in prosody-sensitive tasks (ER, VC), achieves competitive ASR performance, demonstrates strong speechLM capabilities (GenPPL, UTMOS), and shows optimal performance with balanced α weighting between ASR and reconstruction objectives.
  - **评估**: medium (评分: 7/10)

- **SonoEdit: Null-Space Constrained Knowledge Editing for Pronunciation Correction in LLM-Based TTS** (2026-01-23)
  - **作者**: Ayush Pratap Singh et.al.
  - **arXiv**: [2601.17086](https://arxiv.org/abs/2601.17086)
  - **标签**: llm-based, editing, synthesis
  - **TLDR**: SonoEdit introduces a null-space constrained model editing technique for correcting pronunciation errors in LLM-based TTS systems without retraining.
  - **核心贡献**: A one-shot pronunciation correction method that modifies specific word pronunciations while provably preserving all other model behavior through null-space constrained editing.
  - **方法**: The approach combines acoustic causal tracing to identify relevant Transformer layers (layers 15-21) with null-space constrained editing to compute closed-form weight updates that correct target pronunciations while remaining orthogonal to the subspace governing general speech generation.
  - **关键发现**: SonoEdit achieves 2.8% Target-WER for pronunciation correction while maintaining 3.15% Global-WER, outperforming unconstrained methods and demonstrating minimal degradation in speech quality metrics like WER and prosodic structure preservation.
  - **评估**: medium (评分: 7/10)

- **Timbre-Aware LLM-based Direct Speech-to-Speech Translation Extendable to Multiple Language Pairs** (2026-01-22)
  - **作者**: Lalaram Arya et.al.
  - **arXiv**: [2601.16023](https://arxiv.org/abs/2601.16023)
  - **标签**: multilingual, llm-based
  - **TLDR**: DS2ST-LM introduces a multilingual LLM-based direct speech-to-speech translation framework that outperforms cascaded baselines while preserving speaker identity.
  - **核心贡献**: A scalable single-stage direct S2ST framework leveraging a multilingual LLM with timbre-aware speech synthesis, achieving state-of-the-art performance across multiple language pairs.
  - **方法**: Integrates Whisper speech encoder, learnable projection module, Qwen2-0.5B LLM, and timbre-controlled vocoder; uses GigaS2S-1000 synthetic dataset; evaluates speech-derived vs text-derived semantic tokens and three projection architectures.
  - **关键发现**: Outperforms cascaded and ST+TTS baselines on BLEU, METEOR, BLEURT, and COMET metrics; preserves speaker identity better than prior direct S2ST systems; simple Linear projection achieves highest performance despite faster convergence of higher-capacity projectors.
  - **评估**: strong (评分: 8/10)

- **DeepASMR: LLM-Based Zero-Shot ASMR Speech Generation for Anyone of Any Voice** (2026-01-22)
  - **作者**: Leying Zhang et.al.
  - **arXiv**: [2601.15596](https://arxiv.org/abs/2601.15596)
  - **标签**: zero-shot, llm-based
  - **TLDR**: DeepASMR introduces the first zero-shot TTS system for ASMR speech generation using a two-stage LLM-based approach that can synthesize high-fidelity ASMR from a single snippet of normal speech.
  - **核心贡献**: The first framework for zero-shot ASMR speech generation that can create ASMR-style speech from any speaker using only normal speech as input, eliminating the need for whispered training data.
  - **方法**: A two-stage architecture: (1) LLM-based text-to-semantic encoder that disentangles content from style using discrete speech tokens, and (2) flow-matching acoustic decoder for timbre reconstruction. The system leverages a novel dataset (DeepASMR-DB) and employs virtual pool retrieval for style modeling.
  - **关键发现**: DeepASMR outperforms baseline zero-shot TTS models (CosyVoice2, F5TTS) and commercial models in naturalness and style fidelity for ASMR generation, achieving high performance in both intra and cross-style synthesis scenarios while maintaining competitive performance on normal speech synthesis.
  - **评估**: strong (评分: 8/10)

- **Prosody-Guided Harmonic Attention for Phase-Coherent Neural Vocoding in the Complex Spectrum** (2026-01-20)
  - **作者**: Mohammed Salah Al-Radhi et.al.
  - **arXiv**: [2601.14472](https://arxiv.org/abs/2601.14472)
  - **标签**: expressive
  - **TLDR**: A neural vocoder that jointly models magnitude and phase using prosody-guided harmonic attention, achieving improved pitch accuracy and naturalness over state-of-the-art models.
  - **核心贡献**: Introduces prosody-guided harmonic attention to enhance voiced segment encoding and directly predicts complex spectral components for waveform synthesis, ensuring phase coherence and improved pitch fidelity.
  - **方法**: The approach uses prosody-guided harmonic attention to condition spectral modeling on F0, jointly modeling magnitude and phase within a unified architecture. It employs a multi-objective training strategy integrating adversarial, spectral, and phase-aware losses, and generates waveforms via inverse STFT.
  - **关键发现**: Experiments on LJSpeech and VCTK datasets show F0 RMSE reduced by 22%, voiced/unvoiced error lowered by 18%, and MOS scores improved by 0.15 over HiFi-GAN and AutoVocoder, demonstrating more natural, pitch-accurate, and robust synthetic speech.
  - **评估**: medium (评分: 7/10)

- **Lombard Speech Synthesis for Any Voice with Controllable Style Embeddings** (2026-01-19)
  - **作者**: Seymanur Akti et.al.
  - **arXiv**: [2601.12966](https://arxiv.org/abs/2601.12966)
  - **标签**: expressive, synthesis
  - **TLDR**: A controllable TTS system that synthesizes Lombard speech for any speaker without requiring explicit Lombard data during training by manipulating style embeddings.
  - **核心贡献**: Introduces a method to generate Lombard speech for any speaker by shifting PCA components of style embeddings learned from a large, prosodically diverse dataset, enabling fine-grained control over Lombardness levels.
  - **方法**: Leverages F5-TTS as the base model, injects speaker information using a TDNN architecture, and manipulates style embeddings derived from PCA analysis of Lombard and articulation datasets to control Lombardness during inference.
  - **关键发现**: The method preserves naturalness and speaker identity, enhances intelligibility under noise, and provides fine-grained control over prosody. Evaluations show improved Lombardness and intelligibility compared to baseline F5-TTS, with lower WER and higher SSIM scores across different SNR levels.
  - **评估**: medium (评分: 7/10)

- **A Unified Neural Codec Language Model for Selective Editable Text to Speech Generation** (2026-01-18)
  - **作者**: Hanchen Pei et.al.
  - **arXiv**: [2601.12480](https://arxiv.org/abs/2601.12480)
  - **标签**: multilingual, codec
  - **TLDR**: SpeechEdit introduces a unified neural codec language model for selective editable text-to-speech generation with attribute control.
  - **核心贡献**: A unified neural codec language model that enables selective control over speech attributes like timbre, prosody, and emotion while maintaining naturalness.
  - **方法**: Trains on LibriEdit dataset with delta pairs for attribute control, using a two-stage AR-NAR architecture with explicit control instructions for selective editing.
  - **关键发现**: Achieves competitive naturalness and robustness in zero-shot TTS while offering flexible attribute control, outperforming baselines in emotion editing tasks.
  - **评估**: medium (评分: 7/10)

- **ParaMETA: Towards Learning Disentangled Paralinguistic Speaking Styles Representations from Speech** (2026-01-18)
  - **作者**: Haowei Lou et.al.
  - **arXiv**: [2601.12289](https://arxiv.org/abs/2601.12289)
  - **标签**: expressive
  - **TLDR**: ParaMETA is a unified framework for learning disentangled paralinguistic speaking style representations from speech, enabling both recognition and controllable TTS generation.
  - **核心贡献**: Introduces a projection-based approach to learn task-specific embeddings for multiple speaking styles (emotion, gender, age, language) in a single model, reducing inter-task interference and enabling fine-grained style control in TTS.
  - **方法**: Uses dedicated subspaces for each style type to project speech embeddings, avoiding cross-task suppression. Supports both speech- and text-based prompting for TTS. Trained on a multilingual multi-style dataset with subject-independent evaluation.
  - **关键发现**: Outperforms strong baselines in classification accuracy (best in 12/16 backbone-task combinations) and generates more natural, expressive speech. Achieves 70% reduction in CUDA memory usage compared to CLAP-based methods.
  - **评估**: strong (评分: 8/10)

- **Confidence-based Filtering for Speech Dataset Curation with Generative Speech Enhancement Using Discrete Tokens** (2026-01-18)
  - **作者**: Kazuki Yamauchi et.al.
  - **arXiv**: [2601.12254](https://arxiv.org/abs/2601.12254)
  - **标签**: codec
  - **TLDR**: A confidence-based filtering method using discrete token log-probabilities effectively detects hallucination errors in generative speech enhancement models, improving TTS dataset quality.
  - **核心贡献**: Proposes a non-intrusive filtering method that leverages confidence scores from discrete token-based generative speech enhancement models to detect hallucination errors missed by conventional quality metrics.
  - **方法**: Uses log-probabilities of generated discrete tokens as confidence scores to filter hallucination errors from GSE models. Employs Genhancer as the backbone model with DAC audio tokenizer and WavLM features. Trains TTS models (Matcha-TTS) on curated datasets and evaluates using MOS, DNSMOS, and ASR metrics.
  - **关键发现**: Confidence scores strongly correlate with intrusive SE metrics and effectively detect GSE-specific hallucination errors. Curating in-the-wild datasets with this method improves TTS model performance compared to unfiltered enhanced data or conventional filtering methods.
  - **评估**: medium (评分: 7/10)

- **FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning** (2026-01-16)
  - **作者**: Tanyu Chen et.al.
  - **arXiv**: [2601.11141](https://arxiv.org/abs/2601.11141)
  - **标签**: zero-shot, streaming
  - **TLDR**: FlashLabs Chroma 1.0 is the first open-source, real-time end-to-end spoken dialogue model with personalized voice cloning, achieving sub-second latency and high speaker similarity.
  - **核心贡献**: Introduces an interleaved text-audio token schedule (1:2) for streaming generation, enabling real-time end-to-end spoken dialogue with personalized voice cloning.
  - **方法**: Uses a dual-stream (Thinker-Talker) architecture with a 4B parameter model, trained on 100K steps with a balanced objective for text and audio tokens, leveraging speaker verification fine-tuning for voice cloning.
  - **关键发现**: Achieves 10.96% relative improvement in speaker similarity over human baseline, with Real-Time Factor (RTF) of 0.43, and competitive performance across understanding, reasoning, and dialogue tasks.
  - **评估**: strong (评分: 8/10)

- **DSA-Tokenizer: Disentangled Semantic-Acoustic Tokenization via Flow Matching-based Hierarchical Fusion** (2026-01-15)
  - **作者**: Hanlin Zhang et.al.
  - **arXiv**: [2601.09239](https://arxiv.org/abs/2601.09239)
  - **标签**: codec
  - **TLDR**: DSA-Tokenizer introduces a disentangled semantic-acoustic tokenization approach using flow matching-based hierarchical fusion for speech modeling.
  - **核心贡献**: Proposes a novel tokenizer that explicitly separates speech into discrete semantic and acoustic tokens with distinct optimization constraints, enabling better disentanglement than existing methods.
  - **方法**: Uses ASR supervision for semantic tokens and mel-spectrogram restoration for acoustic tokens, with a hierarchical Flow-Matching decoder to eliminate rigid length constraints and enable flexible recombination.
  - **关键发现**: Achieves superior balance between reconstruction quality and ASR performance compared to baseline models, with robust disentanglement enabling controllable speech generation in LLMs.
  - **评估**: medium (评分: 7/10)

- **SPAM: Style Prompt Adherence Metric for Prompt-based TTS** (2026-01-09)
  - **作者**: Chanhee Cho et.al.
  - **arXiv**: [2601.05554](https://arxiv.org/abs/2601.05554)
  - **标签**: expressive, synthesis
  - **TLDR**: SPAM is a new automatic metric for evaluating style prompt adherence in TTS that is both plausible and faithful by aligning acoustic attributes with style prompts.
  - **核心贡献**: Introduces SPAM, a Style Prompt Adherence Metric that explicitly satisfies plausibility and faithfulness for prompt-based TTS evaluation.
  - **方法**: Factorizes speech into acoustic attributes and aligns them with style prompts using a CLAP-inspired approach with supervised contrastive loss; trained with RA-CLAP teacher model and evaluated on TextrolSpeech and LibriTTS-P datasets.
  - **关键发现**: SPAM achieved strong correlation with MOS (0.520 and 0.429) and demonstrated faithful evaluation by discriminating different semantics of prompts across datasets.
  - **评估**: medium (评分: 7/10)

- **CosyEdit: Unlocking End-to-End Speech Editing Capability from Zero-Shot Text-to-Speech Models** (2026-01-08)
  - **作者**: Junyang Chen et.al.
  - **arXiv**: [2601.05329](https://arxiv.org/abs/2601.05329)
  - **标签**: zero-shot, editing, synthesis
  - **TLDR**: CosyEdit is an end-to-end speech editing model adapted from a zero-shot TTS model, achieving high-quality speech editing without external alignment modules.
  - **核心贡献**: Introduces CosyEdit, an end-to-end speech editing model fine-tuned from CosyVoice, which eliminates the need for external alignment modules and achieves performance comparable to state-of-the-art cascade systems.
  - **方法**: Fine-tuned a 400M-parameter CosyVoice model on a curated 250-hour GigaEdit dataset, using a combination of autoregressive and non-autoregressive components, with optimized inference procedures to internalize speech-text alignment.
  - **关键发现**: CosyEdit outperforms several billion-parameter language model baselines and matches state-of-the-art cascade approaches on the RealEdit benchmark, demonstrating robust and efficient speech editing capabilities.
  - **评估**: strong (评分: 8/10)

- **FlexiVoice: Enabling Flexible Style Control in Zero-Shot TTS with Natural Language Instructions** (2026-01-08)
  - **作者**: Dekun Chen et.al.
  - **arXiv**: [2601.04656](https://arxiv.org/abs/2601.04656)
  - **标签**: zero-shot, expressive, multilingual, synthesis
  - **TLDR**: FlexiVoice is a zero-shot TTS system that enables flexible style control through natural language instructions and speech references, using progressive post-training techniques.
  - **核心贡献**: Introduces a novel Progressive Post-Training (PPT) scheme with Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO) to achieve accurate instruction following and factor disentanglement in zero-shot TTS.
  - **方法**: Built on a pre-trained LLM core with auto-regressive generation and flow matching; employs DPO to align emotional output with instructions, GRPO to disentangle style, timbre, and content, and Instruction GRPO for advanced instruction following using audio-language model rewards.
  - **关键发现**: Outperforms baselines and commercial models on naturalness, controllability, and robustness; achieves strong decoupling of control factors with human evaluation confirming performance; demonstrates effective zero-shot voice cloning and style control.
  - **评估**: strong (评分: 8/10)

- **ReStyle-TTS: Relative and Continuous Style Control for Zero-Shot Speech Synthesis** (2026-01-07)
  - **作者**: Haitao Li et.al.
  - **arXiv**: [2601.03632](https://arxiv.org/abs/2601.03632)
  - **标签**: zero-shot, expressive, synthesis
  - **TLDR**: ReStyle-TTS introduces a framework for continuous and reference-relative style control in zero-shot TTS by decoupling text and reference guidance while preserving speaker timbre.
  - **核心贡献**: Proposes Decoupled Classifier-Free Guidance (DCFG) to reduce model dependency on reference style, combined with style-specific LoRAs and Orthogonal LoRA Fusion for continuous multi-attribute control, plus Timbre Consistency Optimization to prevent timbre drift.
  - **方法**: The approach fine-tunes a base zero-shot TTS model with DCFG to independently control text and reference guidance, applies style-specific LoRAs with orthogonal fusion for disentangled attribute control, and adds TCO to maintain speaker timbre consistency during style manipulation.
  - **关键发现**: Experiments show ReStyle-TTS achieves superior performance in continuous pitch, energy, and emotion control while maintaining speaker timbre and intelligibility, outperforming baselines in both matched and mismatched reference-target style scenarios.
  - **评估**: medium (评分: 7/10)

## 完整列表（按日期）
### 2026-01-31
- **Edit Content, Preserve Acoustics: Imperceptible Text-Based Speech Editing via Self-Consistency Rewards**
  - **作者**: Yong Ren et.al.
  - **arXiv**: [2602.00560](https://arxiv.org/abs/2602.00560)
  - **TLDR**: A novel text-based speech editing method that decouples content editing from acoustic reconstruction using semantic tokens and Flow Matching, achieving superior imperceptibility and robustness.
  - **核心贡献**: Introduces a decoupled architecture that separates semantic editing from acoustic reconstruction, using a pre-trained TTS model as a consistency critic and GRPO for alignment, outperforming existing AR and NAR baselines.
  - **关键发现**: Significantly outperforms state-of-the-art AR and NAR baselines in intelligibility (WER), speaker similarity, and perceptual quality (DNSMOS, Subjective MOS), with robust performance across varying edit durations and contexts.


### 2026-01-30
- **EmoShift: Lightweight Activation Steering for Enhanced Emotion-Aware Speech Synthesis**
  - **作者**: Li Zhou et.al.
  - **arXiv**: [2601.22873](https://arxiv.org/abs/2601.22873)
  - **TLDR**: EmoShift introduces a lightweight activation-steering framework for emotion-aware TTS that learns emotion-specific steering vectors to enhance expressiveness while preserving naturalness and speaker similarity.
  - **核心贡献**: Proposes EmoShift, a parameter-efficient activation-steering method with an EmoSteer layer that learns emotion-specific steering vectors to control emotional expression in TTS without full fine-tuning.
  - **关键发现**: EmoShift outperforms zero-shot and fully fine-tuned baselines in both objective (Word Error Rate, Speaker Similarity, Emotion Recognition Accuracy) and subjective (Mean Opinion Score, Emotion MOS) evaluations while maintaining naturalness and speaker similarity.

- **Evaluating and Rewarding LALMs for Expressive Role-Play TTS via Mean Continuation Log-Probability**
  - **作者**: Yong Ren et.al.
  - **arXiv**: [2601.22661](https://arxiv.org/abs/2601.22661)
  - **TLDR**: A paper introducing Mean Continuation Log-Probability (MCLP) as both an evaluation metric and reward signal for improving expressive role-play TTS using Large Audio Language Models.
  - **核心贡献**: Proposes MCLP to quantify stylistic consistency in role-play TTS and uses it as a reinforcement learning reward to enhance style alignment with instructions.
  - **关键发现**: Significant improvements over strong LALM baselines on both objective (WER, CER, Pinyin Error Rate) and subjective metrics (MOS approaching ground truth at 3.646), with exceptionally low CER and Pinyin error rates.

- **Now You Hear Me: Audio Narrative Attacks Against Large Audio-Language Models**
  - **作者**: Ye Yu et.al.
  - **arXiv**: [2601.23255](https://arxiv.org/abs/2601.23255)
  - **TLDR**: A jailbreak attack on large audio-language models using narrative-style TTS delivery achieves 98.26% success rate, significantly exceeding text-only baselines.
  - **核心贡献**: Demonstrates that prosodic and paralinguistic features in synthetic speech can bypass safety mechanisms in audio-language models, revealing a new attack vector.
  - **关键发现**: Achieves 98.26% success rate on Gemini 2.0 Flash and other models, outperforming text-only baselines; vocal tone variation alone can substantially shift model behavior even without adversarial content.


### 2026-01-29
- **Speech Quality-Based Localization of Low-Quality Speech and Text-to-Speech Synthesis Artefacts**
  - **作者**: Michael Kuhlmann et.al.
  - **arXiv**: [2601.21886](https://arxiv.org/abs/2601.21886)
  - **TLDR**: The paper proposes using utterance-level speech quality predictors with segment-based consistency constraints to improve frame-level interpretability and localization of TTS synthesis artifacts.
  - **核心贡献**: Demonstrates that utterance-level SQA models can be regularized with segment-based consistency constraints to reduce frame-level stochasticity, enabling better localization of synthesis artifacts.
  - **关键发现**: Consistency regularization improves frame-level predictions. Models successfully detect synthesis artifacts with higher precision than random control sets. Listening tests confirm listeners rate detected segments as lower quality.

- **Unit-Based Agent for Semi-Cascaded Full-Duplex Dialogue Systems**
  - **作者**: Haoyuan Yu et.al.
  - **arXiv**: [2601.20230](https://arxiv.org/abs/2601.20230)
  - **TLDR**: A semi-cascaded full-duplex dialogue system using multimodal LLMs and minimal conversational units achieves strong performance without training.
  - **核心贡献**: Introduces a unit-based agent framework that decomposes dialogue into minimal conversational units for independent processing and prediction of transitions.
  - **关键发现**: Achieves second place among all teams on the HumDial test set of the Human-like Spoken Dialogue Systems Challenge (Track 2: Full-Duplex Interaction), demonstrating effectiveness of the framework.


### 2026-01-28
- **ASR for Affective Speech: Investigating Impact of Emotion and Speech Generative Strategy**
  - **作者**: Ya-Tse Wu et.al.
  - **arXiv**: [2601.20319](https://arxiv.org/abs/2601.20319)
  - **TLDR**: This paper investigates how emotional speech and generative strategies affect ASR performance, introducing two fine-tuning approaches that improve emotion-aware ASR without degrading clean speech recognition.
  - **核心贡献**: Introduces two generative strategies (transcription correctness and emotional salience) for constructing fine-tuning subsets that improve ASR performance on emotional speech while maintaining neutral speech recognition.
  - **关键发现**: Fine-tuning with generative strategies achieves consistent WER improvements on real emotional datasets (MSP Podcast Test1/2, IEMOCAP) without degradation on clean LibriSpeech; combined TTS-EMO-G strategy shows strongest gains, particularly for expressive speech.

- **Audio Deepfake Detection in the Age of Advanced Text-to-Speech models**
  - **作者**: Robin Singh et.al.
  - **arXiv**: [2601.20510](https://arxiv.org/abs/2601.20510)
  - **TLDR**: A multi-view detection approach combining semantic, structural, and signal-level analysis outperforms single-paradigm detectors against diverse TTS architectures.
  - **核心贡献**: First empirical characterization of how detection architectures perform against three state-of-the-art TTS models (Dia2, Maya1, MeloTTS), demonstrating that integrated detection strategies are necessary for robust audio deepfake detection.
  - **关键发现**: Single-paradigm detectors show significant variability in performance across TTS architectures, with proprietary model achieving near-perfect separation (EER < 1%) across all models, while multi-view approach demonstrates robust performance across all evaluated models.

- **Erasing Your Voice Before It's Heard: Training-free Speaker Unlearning for Zero-shot Text-to-Speech**
  - **作者**: Myungjin Lee et.al.
  - **arXiv**: [2601.20481](https://arxiv.org/abs/2601.20481)
  - **TLDR**: TruS introduces a training-free speaker unlearning method for zero-shot TTS that suppresses specific speaker identities at inference time without retraining.
  - **核心贡献**: The first training-free speaker unlearning framework that generalizes to both seen and unseen opt-out speakers by steering identity-specific hidden activations in TTS models.
  - **关键发现**: TruS effectively prevents voice generation of both seen and unseen opt-out speakers while preserving other attributes like prosody and emotion, achieving comparable unlearning performance to retraining-based methods without the computational cost.


### 2026-01-27
- **Phonological Tokenizer: Prosody-Aware Phonetic Token via Multi-Objective Fine-Tuning with Differentiable K-Means**
  - **作者**: Kentaro Onda et.al.
  - **arXiv**: [2601.19781](https://arxiv.org/abs/2601.19781)
  - **TLDR**: Phonological Tokenizer fine-tunes phonetic tokens via differentiable k-means with multi-task ASR and speech resynthesis objectives to better capture linguistic and prosodic information while discarding speaker identity.
  - **核心贡献**: Introduces a phonological tokenizer that effectively incorporates prosodic information through multi-objective fine-tuning of SSL models, achieving superior performance in prosody-sensitive tasks compared to existing acoustic and phonetic tokenizers.
  - **关键发现**: Outperforms baselines in prosody-sensitive tasks (ER, VC), achieves competitive ASR performance, demonstrates strong speechLM capabilities (GenPPL, UTMOS), and shows optimal performance with balanced α weighting between ASR and reconstruction objectives.

- **Rethinking Discrete Speech Representation Tokens for Accent Generation**
  - **作者**: Jinzuomu Zhong et.al.
  - **arXiv**: [2601.19786](https://arxiv.org/abs/2601.19786)
  - **TLDR**: This paper systematically investigates how accent information is encoded in Discrete Speech Representation Tokens (DSRTs) and proposes new accent-aware token designs for controllable accent generation.
  - **核心贡献**: First systematic investigation of accent information in DSRTs, proposing a unified evaluation framework combining Accent ABX task and cross-accent Voice Conversion resynthesis, along with new content-only and content-accent DSRT designs that outperform existing approaches.
  - **关键发现**: Accent information is substantially reduced when ASR supervision is used to fine-tune encoders, but cannot be effectively disentangled from phonetic and speaker information through naive codebook size reduction. The proposed content-only and content-accent DSRTs significantly outperform existing designs in controllable accent generation.

- **T-Mimi: A Transformer-based Mimi Decoder for Real-Time On-Phone TTS**
  - **作者**: Haibin Wu et.al.
  - **arXiv**: [2601.20094](https://arxiv.org/abs/2601.20094)
  - **TLDR**: T-Mimi replaces Mimi's convolutional decoder with a transformer-only architecture to achieve 9x faster on-device TTS with minimal quality loss.
  - **核心贡献**: A purely transformer-based decoder for the Mimi codec that reduces on-device TTS latency from 42.1ms to 4.4ms while maintaining audio quality.
  - **关键发现**: Achieves 9x latency reduction (42.1ms → 4.4ms), maintains comparable audio quality (PESQ scores near baseline), 8-bit quantization reduces storage by 75% with minimal quality loss, final two transformer layers require 32-bit precision for quality preservation.


### 2026-01-26
- **UrgentMOS: Unified Multi-Metric and Preference Learning for Robust Speech Quality Assessment**
  - **作者**: Wei Wang et.al.
  - **arXiv**: [2601.18438](https://arxiv.org/abs/2601.18438)
  - **TLDR**: UrgentMOS is a unified speech quality assessment framework that jointly learns from multiple quality metrics and pairwise preferences to achieve state-of-the-art performance across diverse speech quality datasets.
  - **核心贡献**: A unified multi-metric and preference learning framework that can handle heterogeneous datasets with incomplete annotations and supports both absolute and comparative evaluation.
  - **关键发现**: Achieves state-of-the-art performance in both absolute (LCC/SRCC) and comparative (preference accuracy) evaluation across TTS, speech enhancement, and other speech quality datasets; demonstrates robustness to dataset heterogeneity and missing annotations.


### 2026-01-25
- **AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation**
  - **作者**: Dongjie Cheng et.al.
  - **arXiv**: [2601.17761](https://arxiv.org/abs/2601.17761)
  - **TLDR**: AR-Omni is a unified autoregressive model that generates text, images, and speech using a single Transformer decoder without external expert components.
  - **核心贡献**: Presents the first unified autoregressive model for any-to-any generation across text, images, and speech using a single decoder architecture.
  - **关键发现**: Achieves competitive performance across all three modalities while maintaining real-time speech synthesis (0.88 RTF) and diffusion-free image generation, demonstrating the viability of unified autoregressive modeling.

- **Quran-MD: A Fine-Grained Multilingual Multimodal Dataset of the Quran**
  - **作者**: Muhammad Umar Salman et.al.
  - **arXiv**: [2601.17880](https://arxiv.org/abs/2601.17880)
  - **TLDR**: QURAN-MD is a comprehensive multimodal dataset of the Quran integrating Arabic text, English translations, transliterations, and audio recordings from 32 reciters at both verse and word levels.
  - **核心贡献**: A fine-grained multilingual multimodal dataset enabling computational analysis of Quranic recitation, supporting applications in NLP, speech recognition, TTS, tajweed detection, and multimodal embeddings.
  - **关键发现**: Dataset enables fine-grained analysis of pronunciation, phonology, and semantic context; supports ASR, tajweed error detection, Quranic TTS, style transfer, prosody modeling, and personalized tutoring systems.


### 2026-01-23
- **SonoEdit: Null-Space Constrained Knowledge Editing for Pronunciation Correction in LLM-Based TTS**
  - **作者**: Ayush Pratap Singh et.al.
  - **arXiv**: [2601.17086](https://arxiv.org/abs/2601.17086)
  - **TLDR**: SonoEdit introduces a null-space constrained model editing technique for correcting pronunciation errors in LLM-based TTS systems without retraining.
  - **核心贡献**: A one-shot pronunciation correction method that modifies specific word pronunciations while provably preserving all other model behavior through null-space constrained editing.
  - **关键发现**: SonoEdit achieves 2.8% Target-WER for pronunciation correction while maintaining 3.15% Global-WER, outperforming unconstrained methods and demonstrating minimal degradation in speech quality metrics like WER and prosodic structure preservation.


### 2026-01-22
- **DeepASMR: LLM-Based Zero-Shot ASMR Speech Generation for Anyone of Any Voice**
  - **作者**: Leying Zhang et.al.
  - **arXiv**: [2601.15596](https://arxiv.org/abs/2601.15596)
  - **TLDR**: DeepASMR introduces the first zero-shot TTS system for ASMR speech generation using a two-stage LLM-based approach that can synthesize high-fidelity ASMR from a single snippet of normal speech.
  - **核心贡献**: The first framework for zero-shot ASMR speech generation that can create ASMR-style speech from any speaker using only normal speech as input, eliminating the need for whispered training data.
  - **关键发现**: DeepASMR outperforms baseline zero-shot TTS models (CosyVoice2, F5TTS) and commercial models in naturalness and style fidelity for ASMR generation, achieving high performance in both intra and cross-style synthesis scenarios while maintaining competitive performance on normal speech synthesis.

- **Qwen3-TTS Technical Report**
  - **作者**: Hangrui Hu et.al.
  - **arXiv**: [2601.15621](https://arxiv.org/abs/2601.15621)
  - **TLDR**: Qwen3-TTS is a multilingual, controllable, and streaming text-to-speech model family with advanced voice cloning and low-latency capabilities.
  - **核心贡献**: Introduces a dual-track LM architecture with two specialized tokenizers (25Hz and 12Hz) enabling real-time synthesis, 3-second voice cloning, and description-based voice control.
  - **关键发现**: Achieves state-of-the-art performance across multilingual benchmarks, supports zero-shot voice cloning, cross-lingual generation, and long-form speech synthesis with 97ms first-packet emission latency.

- **Timbre-Aware LLM-based Direct Speech-to-Speech Translation Extendable to Multiple Language Pairs**
  - **作者**: Lalaram Arya et.al.
  - **arXiv**: [2601.16023](https://arxiv.org/abs/2601.16023)
  - **TLDR**: DS2ST-LM introduces a multilingual LLM-based direct speech-to-speech translation framework that outperforms cascaded baselines while preserving speaker identity.
  - **核心贡献**: A scalable single-stage direct S2ST framework leveraging a multilingual LLM with timbre-aware speech synthesis, achieving state-of-the-art performance across multiple language pairs.
  - **关键发现**: Outperforms cascaded and ST+TTS baselines on BLEU, METEOR, BLEURT, and COMET metrics; preserves speaker identity better than prior direct S2ST systems; simple Linear projection achieves highest performance despite faster convergence of higher-capacity projectors.


### 2026-01-20
- **Habibi: Laying the Open-Source Foundation of Unified-Dialectal Arabic Speech Synthesis**
  - **作者**: Yushen Chen et.al.
  - **arXiv**: [2601.13802](https://arxiv.org/abs/2601.13802)
  - **TLDR**: Habibi introduces open-source unified-dialectal Arabic TTS models using curriculum learning from ASR data, outperforming commercial systems.
  - **核心贡献**: First systematic benchmark for multi-dialect Arabic speech synthesis with open-source models supporting 20+ dialects.
  - **关键发现**: Unified models outperform commercial ElevenLabs v3 on 6 major dialects, with specialized models showing better dialect-specific performance and curriculum approach enabling effective convergence.

- **HoverAI: An Embodied Aerial Agent for Natural Human-Drone Interaction**
  - **作者**: Yuhua Jin et.al.
  - **arXiv**: [2601.13801](https://arxiv.org/abs/2601.13801)
  - **TLDR**: HoverAI is an embodied aerial agent that combines drone mobility, visual projection, and conversational AI to enable natural human-drone interaction with adaptive avatars.
  - **核心贡献**: Integration of drone mobility, infrastructure-independent visual projection, and real-time conversational AI into a unified platform for socially responsive human-drone interaction.
  - **关键发现**: High accuracy in command recognition (F1: 0.90), demographic estimation (gender F1: 0.89, age MAE: 5.14 years), and speech transcription (WER: 0.181) across 12 participants in controlled experiments.

- **Prosody-Guided Harmonic Attention for Phase-Coherent Neural Vocoding in the Complex Spectrum**
  - **作者**: Mohammed Salah Al-Radhi et.al.
  - **arXiv**: [2601.14472](https://arxiv.org/abs/2601.14472)
  - **TLDR**: A neural vocoder that jointly models magnitude and phase using prosody-guided harmonic attention, achieving improved pitch accuracy and naturalness over state-of-the-art models.
  - **核心贡献**: Introduces prosody-guided harmonic attention to enhance voiced segment encoding and directly predicts complex spectral components for waveform synthesis, ensuring phase coherence and improved pitch fidelity.
  - **关键发现**: Experiments on LJSpeech and VCTK datasets show F0 RMSE reduced by 22%, voiced/unvoiced error lowered by 18%, and MOS scores improved by 0.15 over HiFi-GAN and AutoVocoder, demonstrating more natural, pitch-accurate, and robust synthetic speech.

- **Synthetic Singers: A Review of Deep-Learning-based Singing Voice Synthesis Approaches**
  - **作者**: Changhao Pan et.al.
  - **arXiv**: [2601.13910](https://arxiv.org/abs/2601.13910)
  - **TLDR**: A comprehensive survey of deep-learning-based singing voice synthesis approaches, categorizing systems and analyzing core technologies.
  - **核心贡献**: Systematic categorization of SVS models into cascaded and end-to-end paradigms, with in-depth analysis of singing modeling, control techniques, and relevant resources.
  - **关键发现**: Identification of two major SVS paradigms (cascaded and end-to-end), analysis of three core technologies (singing-voice modeling, control techniques, and evaluation methods), and comprehensive overview of datasets and resources.


### 2026-01-19
- **Lombard Speech Synthesis for Any Voice with Controllable Style Embeddings**
  - **作者**: Seymanur Akti et.al.
  - **arXiv**: [2601.12966](https://arxiv.org/abs/2601.12966)
  - **TLDR**: A controllable TTS system that synthesizes Lombard speech for any speaker without requiring explicit Lombard data during training by manipulating style embeddings.
  - **核心贡献**: Introduces a method to generate Lombard speech for any speaker by shifting PCA components of style embeddings learned from a large, prosodically diverse dataset, enabling fine-grained control over Lombardness levels.
  - **关键发现**: The method preserves naturalness and speaker identity, enhances intelligibility under noise, and provides fine-grained control over prosody. Evaluations show improved Lombardness and intelligibility compared to baseline F5-TTS, with lower WER and higher SSIM scores across different SNR levels.


### 2026-01-18
- **A Unified Neural Codec Language Model for Selective Editable Text to Speech Generation**
  - **作者**: Hanchen Pei et.al.
  - **arXiv**: [2601.12480](https://arxiv.org/abs/2601.12480)
  - **TLDR**: SpeechEdit introduces a unified neural codec language model for selective editable text-to-speech generation with attribute control.
  - **核心贡献**: A unified neural codec language model that enables selective control over speech attributes like timbre, prosody, and emotion while maintaining naturalness.
  - **关键发现**: Achieves competitive naturalness and robustness in zero-shot TTS while offering flexible attribute control, outperforming baselines in emotion editing tasks.

- **Confidence-based Filtering for Speech Dataset Curation with Generative Speech Enhancement Using Discrete Tokens**
  - **作者**: Kazuki Yamauchi et.al.
  - **arXiv**: [2601.12254](https://arxiv.org/abs/2601.12254)
  - **TLDR**: A confidence-based filtering method using discrete token log-probabilities effectively detects hallucination errors in generative speech enhancement models, improving TTS dataset quality.
  - **核心贡献**: Proposes a non-intrusive filtering method that leverages confidence scores from discrete token-based generative speech enhancement models to detect hallucination errors missed by conventional quality metrics.
  - **关键发现**: Confidence scores strongly correlate with intrusive SE metrics and effectively detect GSE-specific hallucination errors. Curating in-the-wild datasets with this method improves TTS model performance compared to unfiltered enhanced data or conventional filtering methods.

- **ParaMETA: Towards Learning Disentangled Paralinguistic Speaking Styles Representations from Speech**
  - **作者**: Haowei Lou et.al.
  - **arXiv**: [2601.12289](https://arxiv.org/abs/2601.12289)
  - **TLDR**: ParaMETA is a unified framework for learning disentangled paralinguistic speaking style representations from speech, enabling both recognition and controllable TTS generation.
  - **核心贡献**: Introduces a projection-based approach to learn task-specific embeddings for multiple speaking styles (emotion, gender, age, language) in a single model, reducing inter-task interference and enabling fine-grained style control in TTS.
  - **关键发现**: Outperforms strong baselines in classification accuracy (best in 12/16 backbone-task combinations) and generates more natural, expressive speech. Achieves 70% reduction in CUDA memory usage compared to CLAP-based methods.


### 2026-01-16
- **F-Actor: Controllable Conversational Behaviour in Full-Duplex Models**
  - **作者**: Maike Z眉fle et.al.
  - **arXiv**: [2601.11329](https://arxiv.org/abs/2601.11329)
  - **TLDR**: F-Actor is an open, instruction-following full-duplex conversational speech model trained efficiently with just 2,000 hours of data by freezing the audio encoder and fine-tuning only the language model.
  - **核心贡献**: First open, instruction-following full-duplex conversational speech model that can be trained efficiently under academic resource constraints, enabling controllable conversational behaviors like backchanneling and interruptions.
  - **关键发现**: Achieves over 99% accuracy in instruction following when combining audio and text streams with word-level alignment, produces coherent conversations with controllable backchanneling and interruptions, and demonstrates efficient training requiring minimal data compared to prior approaches.

- **FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning**
  - **作者**: Tanyu Chen et.al.
  - **arXiv**: [2601.11141](https://arxiv.org/abs/2601.11141)
  - **TLDR**: FlashLabs Chroma 1.0 is the first open-source, real-time end-to-end spoken dialogue model with personalized voice cloning, achieving sub-second latency and high speaker similarity.
  - **核心贡献**: Introduces an interleaved text-audio token schedule (1:2) for streaming generation, enabling real-time end-to-end spoken dialogue with personalized voice cloning.
  - **关键发现**: Achieves 10.96% relative improvement in speaker similarity over human baseline, with Real-Time Factor (RTF) of 0.43, and competitive performance across understanding, reasoning, and dialogue tasks.

- **WenetSpeech-Wu: Datasets, Benchmarks, and Models for a Unified Chinese Wu Dialect Speech Processing Ecosystem**
  - **作者**: Chengyou Wang et.al.
  - **arXiv**: [2601.11027](https://arxiv.org/abs/2601.11027)
  - **TLDR**: WenetSpeech-Wu introduces the first large-scale, multi-dimensionally annotated open-source speech corpus and benchmark for the Wu dialect, along with strong open-source models across multiple speech tasks.
  - **核心贡献**: The first large-scale, multi-dimensionally annotated open-source speech corpus (WenetSpeech-Wu) for the Wu dialect, a standardized benchmark (WenetSpeech-Wu-Bench), and a suite of strong open-source models across ASR, TTS, speech understanding, and instruction-following TTS.
  - **关键发现**: The proposed models achieved competitive performance across multiple tasks, with the Step-Audio2-Wu-ASR model showing state-of-the-art performance across all model scales. The instruct TTS models substantially outperformed existing approaches, and the unified speech understanding models demonstrated strong performance on the WenetSpeech-Wu corpus.


### 2026-01-15
- **DSA-Tokenizer: Disentangled Semantic-Acoustic Tokenization via Flow Matching-based Hierarchical Fusion**
  - **作者**: Hanlin Zhang et.al.
  - **arXiv**: [2601.09239](https://arxiv.org/abs/2601.09239)
  - **TLDR**: DSA-Tokenizer introduces a disentangled semantic-acoustic tokenization approach using flow matching-based hierarchical fusion for speech modeling.
  - **核心贡献**: Proposes a novel tokenizer that explicitly separates speech into discrete semantic and acoustic tokens with distinct optimization constraints, enabling better disentanglement than existing methods.
  - **关键发现**: Achieves superior balance between reconstruction quality and ASR performance compared to baseline models, with robust disentanglement enabling controllable speech generation in LLMs.

- **ESDD2: Environment-Aware Speech and Sound Deepfake Detection Challenge Evaluation Plan**
  - **作者**: Xueping Zhang et.al.
  - **arXiv**: [2601.07303](https://arxiv.org/abs/2601.07303)
  - **TLDR**: A challenge and dataset for detecting component-level audio deepfakes where speech and environmental sounds can be manipulated independently
  - **核心贡献**: Introduction of CompSpoofV2 dataset and ESDD2 challenge for component-level audio anti-spoofing with separation-enhanced joint learning framework
  - **关键发现**: CompSpoofV2 contains over 250k audio samples (283 hours) with five categories; baseline model performance evaluated on validation, evaluation, and test sets using CodaBench platform with Macro-F1 as primary metric

- **VoiceSculptor: Your Voice, Designed By You**
  - **作者**: Jingbin Hu et.al.
  - **arXiv**: [2601.10629](https://arxiv.org/abs/2601.10629)
  - **TLDR**: VoiceSculptor is an open-source unified system that enables fine-grained, instruction-following voice design and high-fidelity cloning through chain-of-thought reasoning and attribute modeling.
  - **核心贡献**: A unified open-source framework integrating instruction-based voice design with high-fidelity cloning, achieving state-of-the-art performance on InstructTTSEval-Zh while maintaining full reproducibility.
  - **关键发现**: Achieves SOTA among open-source models on InstructTTSEval-Zh; scaling studies show consistent gains from larger model size (1B→3B) and richer training data; CoT-based attribute modeling improves controllability without architectural changes.


### 2026-01-14
- **Afri-MCQA: Multimodal Cultural Question Answering for African Languages**
  - **作者**: Atnafu Lambebo Tonja et.al.
  - **arXiv**: [2601.05699](https://arxiv.org/abs/2601.05699)
  - **TLDR**: Afri-MCQA is the first multimodal cultural question-answering benchmark for 15 African languages, revealing significant performance gaps in LLMs across text and speech modalities.
  - **核心贡献**: Introduction of Afri-MCQA, a 7.5k Q&A multilingual cultural benchmark with text and speech modalities, and comprehensive evaluation of LLMs on African languages.
  - **关键发现**: Open-weight models perform poorly on African languages, especially in open-ended VQA and speech; performance gaps exist between native languages and English; model scaling alone insufficient; speech-first approaches and culturally grounded pretraining needed.


### 2026-01-13
- **Decoding Order Matters in Autoregressive Speech Synthesis**
  - **作者**: Minghui Zhao et.al.
  - **arXiv**: [2601.08450](https://arxiv.org/abs/2601.08450)
  - **TLDR**: Decoding order in autoregressive speech synthesis significantly impacts quality, with adaptive strategies outperforming fixed left-to-right approaches.
  - **核心贡献**: Demonstrates that fixed decoding orders like left-to-right are suboptimal for speech synthesis, and adaptive decoding strategies yield better performance.
  - **关键发现**: Adaptive decoding orders outperform fixed ones, with Top-K showing best results; randomness in decoding order affects speech quality; even 1-bit quantization supports high-quality speech.


### 2026-01-12
- **FOCAL: A Novel Benchmarking Technique for Multi-modal Agents**
  - **作者**: Aditya Choudhary et.al.
  - **arXiv**: [2601.07367](https://arxiv.org/abs/2601.07367)
  - **TLDR**: FOCAL is a benchmarking framework for evaluating multi-modal voice agents with novel reasoning and semantic metrics.
  - **核心贡献**: Introduces FOCAL, a framework for end-to-end evaluation of multi-modal agents with component-wise error analysis and two novel metrics (Reasoning and Semantic scores).
  - **关键发现**: Demonstrates evaluation of a RAG-based shopping agent under different modalities, highlighting error propagation and incomplete responses in certain cases.


### 2026-01-11
- **Bridging Attribution and Open-Set Detection using Graph-Augmented Instance Learning in Synthetic Speech**
  - **作者**: Mohd Mujtaba Akhtar et.al.
  - **arXiv**: [2601.07064](https://arxiv.org/abs/2601.07064)
  - **TLDR**: SIGNAL unifies graph-based learning and open-set detection to attribute synthetic speech to its source and detect unseen generators.
  - **核心贡献**: A hybrid GNN-KNN framework that combines relational modeling with instance-level classification for synthetic speech attribution and open-set detection.
  - **关键发现**: SIGNAL consistently improves performance across both source attribution and open-set detection tasks. Mamba-based embeddings delivered the strongest results, with the hybrid GNN+KNN architecture achieving ACC: 83.27%, F1: 82.63%, and EER: 14.78% on DiffSSD. The framework shows robust generalization to unseen generators.


### 2026-01-10
- **Lightweight Resolution-Aware Audio Deepfake Detection via Cross-Scale Attention and Consistency Learning**
  - **作者**: K. A. Shahriar et.al.
  - **arXiv**: [2601.06560](https://arxiv.org/abs/2601.06560)
  - **TLDR**: A lightweight audio deepfake detection model using cross-scale attention and consistency learning achieves near-perfect performance across multiple challenging datasets.
  - **核心贡献**: Introduces a resolution-aware framework that explicitly models and aligns multi-resolution spectral representations through cross-scale attention and consistency learning, achieving strong performance while remaining computationally efficient.
  - **关键发现**: Achieves near-perfect EER of 0.16% on ASVspoof LA, 5.09% on ASVspoof PA, 4.54% on FoR rerecorded audio, and 4.81% EER with 0.98 AUC on in-the-wild deepfakes. Ablation studies confirm cross-scale attention and consistency learning are critical, and the model remains lightweight and efficient.


### 2026-01-09
- **IndexTTS 2.5 Technical Report**
  - **作者**: Yunpei Li et.al.
  - **arXiv**: [2601.03888](https://arxiv.org/abs/2601.03888)
  - **TLDR**: IndexTTS 2.5 improves upon its predecessor with semantic codec compression, Zipformer architecture, multilingual extensions, and GRPO optimization, achieving 2.28x faster inference while maintaining quality.
  - **核心贡献**: Introduces four key improvements to zero-shot multilingual TTS: semantic codec compression (50Hz to 25Hz), Zipformer-based S2M architecture, three explicit cross-lingual modeling strategies, and GRPO-based T2S optimization.
  - **关键发现**: Achieves 2.28x improvement in real-time factor (RTF) while maintaining comparable WER and speaker similarity to IndexTTS 2; Zipformer architecture preferred over U-DiT in subjective evaluations; supports Mandarin, English, Japanese, and Spanish with emotion transfer in unseen languages.

- **Pantagruel: Unified Self-Supervised Encoders for French Text and Speech**
  - **作者**: Phuong-Hang Le et.al.
  - **arXiv**: [2601.05911](https://arxiv.org/abs/2601.05911)
  - **TLDR**: Pantagruel introduces unified self-supervised encoder models for French text and speech using feature-space prediction, achieving competitive performance across both modalities.
  - **核心贡献**: Unified self-supervised encoder models for French text and speech using feature-space prediction within a joint-embedding predictive architecture (JEPA), trained on large-scale French corpora including a new 100k-hour INA broadcast dataset.
  - **关键发现**: Pantagruel models show competitive or superior performance compared to strong French baselines (CamemBERT, FlauBERT, LeBenchmark2.0) across both text and speech tasks. Speech models achieve strong ASR performance, with large models outperforming others. Feature-space prediction proves effective for French representation learning.

- **SPAM: Style Prompt Adherence Metric for Prompt-based TTS**
  - **作者**: Chanhee Cho et.al.
  - **arXiv**: [2601.05554](https://arxiv.org/abs/2601.05554)
  - **TLDR**: SPAM is a new automatic metric for evaluating style prompt adherence in TTS that is both plausible and faithful by aligning acoustic attributes with style prompts.
  - **核心贡献**: Introduces SPAM, a Style Prompt Adherence Metric that explicitly satisfies plausibility and faithfulness for prompt-based TTS evaluation.
  - **关键发现**: SPAM achieved strong correlation with MOS (0.520 and 0.429) and demonstrated faithful evaluation by discriminating different semantics of prompts across datasets.


### 2026-01-08
- **CosyEdit: Unlocking End-to-End Speech Editing Capability from Zero-Shot Text-to-Speech Models**
  - **作者**: Junyang Chen et.al.
  - **arXiv**: [2601.05329](https://arxiv.org/abs/2601.05329)
  - **TLDR**: CosyEdit is an end-to-end speech editing model adapted from a zero-shot TTS model, achieving high-quality speech editing without external alignment modules.
  - **核心贡献**: Introduces CosyEdit, an end-to-end speech editing model fine-tuned from CosyVoice, which eliminates the need for external alignment modules and achieves performance comparable to state-of-the-art cascade systems.
  - **关键发现**: CosyEdit outperforms several billion-parameter language model baselines and matches state-of-the-art cascade approaches on the RealEdit benchmark, demonstrating robust and efficient speech editing capabilities.

- **FlexiVoice: Enabling Flexible Style Control in Zero-Shot TTS with Natural Language Instructions**
  - **作者**: Dekun Chen et.al.
  - **arXiv**: [2601.04656](https://arxiv.org/abs/2601.04656)
  - **TLDR**: FlexiVoice is a zero-shot TTS system that enables flexible style control through natural language instructions and speech references, using progressive post-training techniques.
  - **核心贡献**: Introduces a novel Progressive Post-Training (PPT) scheme with Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO) to achieve accurate instruction following and factor disentanglement in zero-shot TTS.
  - **关键发现**: Outperforms baselines and commercial models on naturalness, controllability, and robustness; achieves strong decoupling of control factors with human evaluation confirming performance; demonstrates effective zero-shot voice cloning and style control.


### 2026-01-07
- **ReStyle-TTS: Relative and Continuous Style Control for Zero-Shot Speech Synthesis**
  - **作者**: Haitao Li et.al.
  - **arXiv**: [2601.03632](https://arxiv.org/abs/2601.03632)
  - **TLDR**: ReStyle-TTS introduces a framework for continuous and reference-relative style control in zero-shot TTS by decoupling text and reference guidance while preserving speaker timbre.
  - **核心贡献**: Proposes Decoupled Classifier-Free Guidance (DCFG) to reduce model dependency on reference style, combined with style-specific LoRAs and Orthogonal LoRA Fusion for continuous multi-attribute control, plus Timbre Consistency Optimization to prevent timbre drift.
  - **关键发现**: Experiments show ReStyle-TTS achieves superior performance in continuous pitch, energy, and emotion control while maintaining speaker timbre and intelligibility, outperforming baselines in both matched and mismatched reference-target style scenarios.

- **SpeakerSleuth: Evaluating Large Audio-Language Models as Judges for Multi-turn Speaker Consistency**
  - **作者**: Jonggeun Lee et.al.
  - **arXiv**: [2601.04029](https://arxiv.org/abs/2601.04029)
  - **TLDR**: Large Audio-Language Models struggle to reliably judge speaker consistency in multi-turn dialogues, often prioritizing text over acoustic cues.
  - **核心贡献**: SpeakerSleuth benchmark evaluates LALMs' ability to assess speaker consistency across multi-turn conversations with 1,818 human-verified instances across four diverse datasets.
  - **关键发现**: LALMs perform poorly on consistency and localization tasks (most scoring below 50%), dramatically degrade when other interlocutors' turns are included, but excel at discrimination tasks (82.6% accuracy). Models prioritize textual coherence over acoustic cues, failing to detect obvious gender switches.


### 2026-01-06
- **Segment-Aware Conditioning for Training-Free Intra-Utterance Emotion and Duration Control in Text-to-Speech**
  - **作者**: Qifan Liang et.al.
  - **arXiv**: [2601.03170](https://arxiv.org/abs/2601.03170)
  - **TLDR**: A training-free method for fine-grained intra-utterance emotion and duration control in TTS using segment-aware conditioning and LLM-based prompt generation.
  - **核心贡献**: Introduces a segment-aware emotion conditioning strategy with causal masking and monotonic stream alignment, combined with duration steering, enabling smooth intra-utterance emotional shifts and local duration adjustments without retraining.
  - **关键发现**: Achieves state-of-the-art intra-utterance consistency in multi-emotion and duration control while maintaining baseline-level speech quality. Demonstrates robust emotional fidelity and smooth emotion transitions, outperforming comparative methods in both objective and subjective evaluations.

- **Tigrinya Number Verbalization: Rules, Algorithm, and Implementation**
  - **作者**: Fitsum Gaim et.al.
  - **arXiv**: [2601.03403](https://arxiv.org/abs/2601.03403)
  - **TLDR**: A systematic formalization of Tigrinya number verbalization rules with open-source implementation, revealing significant gaps in LLM capabilities for this low-resource language.
  - **核心贡献**: First comprehensive documentation of Tigrinya cardinal and ordinal number verbalization rules, including a formal algorithm and open-source implementation.
  - **关键发现**: LLMs show substantial deficiencies in Tigrinya number verbalization, with best model (Opus 4.5) achieving only 65% accuracy overall; performance degrades significantly for non-cardinal categories; GPT-5 Mini failed on most cases due to token limits.

- **Vclip: Face-based Speaker Generation by Face-voice Association Learning**
  - **作者**: Yao Shi et.al.
  - **arXiv**: [2601.02753](https://arxiv.org/abs/2601.02753)
  - **TLDR**: Vclip introduces a face-voice association learning method using CLIP to generate speaker embeddings for face-based TTS, achieving strong cross-modal matching performance.
  - **核心贡献**: Proposes Vclip, a CLIP-based face-voice association model trained on noisy audio-visual data to efficiently learn cross-modal embeddings, enabling retrieval-based speaker generation for TTS.
  - **关键发现**: Achieves 89.63% cross-modal verification AUC on VoxCeleb testset, outperforms Self-Lifting baseline, and demonstrates that retrieval + TTS feedback improves face-voice matching in subjective evaluations.

- **Vulnerabilities of Audio-Based Biometric Authentication Systems Against Deepfake Speech Synthesis**
  - **作者**: Mengze Hong et.al.
  - **arXiv**: [2601.02914](https://arxiv.org/abs/2601.02914)
  - **TLDR**: Modern voice cloning models can easily bypass commercial speaker verification systems, and anti-spoofing detectors struggle to generalize across different audio synthesis methods, revealing critical vulnerabilities in audio-based biometric authentication.
  - **核心贡献**: First systematic empirical evaluation of speaker verification and anti-spoofing systems against diverse deepfake speech synthesis models, revealing two major security vulnerabilities.
  - **关键发现**: Voice cloning models trained on very small samples can bypass commercial speaker verification (EER approaching 0.55); anti-spoofing detectors show 30× performance degradation when encountering unseen synthesis methods; strong in-domain performance fails to generalize to out-of-domain attacks; XLS-R + AASIST achieves best deepfake detection performance but still struggles with unseen models.

- **XLSR-MamBo: Scaling the Hybrid Mamba-Attention Backbone for Audio Deepfake Detection**
  - **作者**: Kwok-Ho Ng et.al.
  - **arXiv**: [2601.02944](https://arxiv.org/abs/2601.02944)
  - **TLDR**: XLSR-MamBo is a hybrid Mamba-Attention architecture that scales backbone depth to improve audio deepfake detection performance.
  - **核心贡献**: Proposes a modular framework integrating XLSR front-end with hybrid Mamba-Attention backbones, demonstrating that scaling backbone depth mitigates performance variance and improves robustness against unseen deepfake synthesis methods.
  - **关键发现**: MamBo-3-Hydra-N3 achieves competitive SOTA performance, with Hydra's native bidirectional modeling providing advantages over heuristic dual-branch strategies; deeper architectures consistently improve performance and reduce variance compared to shallower models.


### 2026-01-05
- **Towards Prosodically Informed Mizo TTS without Explicit Tone Markings**
  - **作者**: Abhijit Mohanta et.al.
  - **arXiv**: [2601.02073](https://arxiv.org/abs/2601.02073)
  - **TLDR**: A low-resource tonal language TTS system for Mizo was developed using Tacotron2 and VITS, with VITS outperforming Tacotron2 in both subjective and objective evaluations.
  - **核心贡献**: Demonstrates that a non-autoregressive, end-to-end TTS framework (VITS) can achieve acceptable perceptual quality and intelligibility for a low-resource tonal language without explicit tone markings.
  - **关键发现**: VITS outperformed Tacotron2 in both subjective (MOS) and objective (DNSMOS, MCD, RMSE F0, F0 correlation) evaluations, with significantly lower tone errors and better overall perceptual quality.

- **VocalBridge: Latent Diffusion-Bridge Purification for Defeating Perturbation-Based Voiceprint Defenses**
  - **作者**: Maryam Abbasihafshejani et.al.
  - **arXiv**: [2601.02444](https://arxiv.org/abs/2601.02444)
  - **TLDR**: VocalBridge is a diffusion-based purification framework that effectively removes protective perturbations from speech, enabling successful voice cloning despite existing defenses.
  - **核心贡献**: Proposes a novel diffusion-bridge purification model (VocalBridge) that learns latent mappings in EnCodec space to remove speaker-protective perturbations while preserving acoustic identity for voice cloning.
  - **关键发现**: VocalBridge outperforms existing purification methods, achieving 37.4% authentication restoration rate (ARR) for TTS and 35.6% for VC models, with MOS values between 2.95-3.27, demonstrating effectiveness against perturbation-based voice defenses.


### 2026-01-04
- **MM-Sonate: Multimodal Controllable Audio-Video Generation with Zero-Shot Voice Cloning**
  - **作者**: Chunyu Qiang et.al.
  - **arXiv**: [2601.01568](https://arxiv.org/abs/2601.01568)

- **OV-InstructTTS: Towards Open-Vocabulary Instruct Text-to-Speech**
  - **作者**: Yong Ren et.al.
  - **arXiv**: [2601.01459](https://arxiv.org/abs/2601.01459)


### 2026-01-01
- **DepFlow: Disentangled Speech Generation to Mitigate Semantic Bias in Depression Detection**
  - **作者**: Yuxin Li et.al.
  - **arXiv**: [2601.00303](https://arxiv.org/abs/2601.00303)

- **Latent Flow Matching for Expressive Singing Voice Synthesis**
  - **作者**: Minhyeok Yun et.al.
  - **arXiv**: [2601.00217](https://arxiv.org/abs/2601.00217)

