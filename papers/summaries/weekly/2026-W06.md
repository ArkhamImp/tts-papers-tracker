# TTS 论文周报
**周期**: 2026-01-27 至 2026-02-02 (第 6 周, 2026 年)
**生成时间**: 2026-02-04 09:52

## 概览
- **论文总数**: 15
- **主题分布**:
  - `合成`: 8
  - `表现力`: 6
  - `多语言`: 3
  - `其他`: 2
  - `流式`: 1
  - `LLM 基础`: 1
  - `零样本`: 1
  - `编辑`: 1

## 重点论文（分析摘要）
- **LipSody: Lip-to-Speech Synthesis with Enhanced Prosody Consistency** (2026-02-02)
  - **作者**: Jaejun Lee et.al.
  - **arXiv**: [2602.01908](https://arxiv.org/abs/2602.01908)
  - **标签**: expressive, synthesis
  - **TLDR**: LipSody improves lip-to-speech synthesis by explicitly modeling prosody consistency using speaker identity, linguistic content, and emotional context from facial video.
  - **核心贡献**: Introduces a novel visual-only prosody estimation method and integrates it into a diffusion-based lip-to-speech framework to enhance prosody consistency while maintaining intelligibility.
  - **方法**: Builds on diffusion-based LipVoicer architecture, adds explicit prosody modeling using speaker identity, linguistic content, and emotional context from face video; uses conformer-based lip-reading and HiFi-GAN vocoder; trained with DDPM and CFG guidance.
  - **关键发现**: Significantly improves prosody-related metrics (pitch deviation, energy consistency, speaker similarity) over LipVoicer while maintaining high intelligibility (WER); subjective tests show better naturalness and preference.
  - **评估**: medium (评分: 7/10)

- **EmoAra: Emotion-Preserving English Speech Transcription and Cross-Lingual Translation with Arabic Text-to-Speech** (2026-02-01)
  - **作者**: Besher Hassan et.al.
  - **arXiv**: [2602.01170](https://arxiv.org/abs/2602.01170)
  - **标签**: expressive, multilingual, synthesis
  - **TLDR**: EmoAra is an end-to-end pipeline that preserves emotional context when translating English speech to Arabic speech for banking customer service.
  - **核心贡献**: Integration of emotion-preserving components (SER, ASR, MT, TTS) into a unified cross-lingual spoken communication system.
  - **方法**: Uses CNN for emotion detection, Whisper for ASR, fine-tuned MarianMT for translation, and MMS-TTS-Ara for speech synthesis; employs transfer learning and domain-specific fine-tuning.
  - **关键发现**: Achieved 94% F1-score for emotion classification, BLEU 56 and BERTScore F1 88.7% for translation, and 81% human evaluation score on banking-domain translations.
  - **评估**: medium (评分: 7/10)

- **EmoShift: Lightweight Activation Steering for Enhanced Emotion-Aware Speech Synthesis** (2026-01-30)
  - **作者**: Li Zhou et.al.
  - **arXiv**: [2601.22873](https://arxiv.org/abs/2601.22873)
  - **标签**: expressive, synthesis
  - **TLDR**: EmoShift introduces a lightweight activation-steering framework for emotion-aware TTS that learns emotion-specific steering vectors to enhance expressiveness while preserving naturalness and speaker similarity.
  - **核心贡献**: Proposes EmoShift, a parameter-efficient activation-steering method with an EmoSteer layer that learns emotion-specific steering vectors to control emotional expression in TTS without full fine-tuning.
  - **方法**: EmoShift inserts an EmoSteer layer into an LLM-based TTS model to learn steering vectors for each emotion, capturing latent offsets in the output embedding space. The model is trained with only 10M parameters (less than 1/30 of full fine-tuning) using negative log-likelihood minimization.
  - **关键发现**: EmoShift outperforms zero-shot and fully fine-tuned baselines in both objective (Word Error Rate, Speaker Similarity, Emotion Recognition Accuracy) and subjective (Mean Opinion Score, Emotion MOS) evaluations while maintaining naturalness and speaker similarity.
  - **评估**: medium (评分: 7/10)

- **Evaluating and Rewarding LALMs for Expressive Role-Play TTS via Mean Continuation Log-Probability** (2026-01-30)
  - **作者**: Yong Ren et.al.
  - **arXiv**: [2601.22661](https://arxiv.org/abs/2601.22661)
  - **标签**: expressive, synthesis
  - **TLDR**: A paper introducing Mean Continuation Log-Probability (MCLP) as both an evaluation metric and reward signal for improving expressive role-play TTS using Large Audio Language Models.
  - **核心贡献**: Proposes MCLP to quantify stylistic consistency in role-play TTS and uses it as a reinforcement learning reward to enhance style alignment with instructions.
  - **方法**: Leverages in-context learning of pre-trained LALMs to compute MCLP via continuation log-probability prediction, measuring likelihood of ground-truth speech conditioned on generated speech. Employs MCLP as RL reward using GRPO to align RP-TTS model with role-play instructions.
  - **关键发现**: Significant improvements over strong LALM baselines on both objective (WER, CER, Pinyin Error Rate) and subjective metrics (MOS approaching ground truth at 3.646), with exceptionally low CER and Pinyin error rates.
  - **评估**: strong (评分: 8/10)

- **Erasing Your Voice Before It's Heard: Training-free Speaker Unlearning for Zero-shot Text-to-Speech** (2026-01-28)
  - **作者**: Myungjin Lee et.al.
  - **arXiv**: [2601.20481](https://arxiv.org/abs/2601.20481)
  - **标签**: zero-shot, synthesis
  - **TLDR**: TruS introduces a training-free speaker unlearning method for zero-shot TTS that suppresses specific speaker identities at inference time without retraining.
  - **核心贡献**: The first training-free speaker unlearning framework that generalizes to both seen and unseen opt-out speakers by steering identity-specific hidden activations in TTS models.
  - **方法**: TruS works by steering internal representations in TTS models to suppress identity-related activations using an ID-prototype computed from intermediate features and a pretrained speaker verification model, operating entirely at inference time on pretrained models like F5-TTS.
  - **关键发现**: TruS effectively prevents voice generation of both seen and unseen opt-out speakers while preserving other attributes like prosody and emotion, achieving comparable unlearning performance to retraining-based methods without the computational cost.
  - **评估**: medium (评分: 7/10)

- **ASR for Affective Speech: Investigating Impact of Emotion and Speech Generative Strategy** (2026-01-28)
  - **作者**: Ya-Tse Wu et.al.
  - **arXiv**: [2601.20319](https://arxiv.org/abs/2601.20319)
  - **标签**: expressive
  - **TLDR**: This paper investigates how emotional speech and generative strategies affect ASR performance, introducing two fine-tuning approaches that improve emotion-aware ASR without degrading clean speech recognition.
  - **核心贡献**: Introduces two generative strategies (transcription correctness and emotional salience) for constructing fine-tuning subsets that improve ASR performance on emotional speech while maintaining neutral speech recognition.
  - **方法**: Analyzes speech synthesized from three emotional TTS models (EmoVoice, MaskGCT, CosyVoice2), identifies substitution errors as dominant issue, and proposes two generative strategies for fine-tuning Qwen2-audio-7B model using supervised fine-tuning with LoRA adapters.
  - **关键发现**: Fine-tuning with generative strategies achieves consistent WER improvements on real emotional datasets (MSP Podcast Test1/2, IEMOCAP) without degradation on clean LibriSpeech; combined TTS-EMO-G strategy shows strongest gains, particularly for expressive speech.
  - **评估**: medium (评分: 7/10)

- **T-Mimi: A Transformer-based Mimi Decoder for Real-Time On-Phone TTS** (2026-01-27)
  - **作者**: Haibin Wu et.al.
  - **arXiv**: [2601.20094](https://arxiv.org/abs/2601.20094)
  - **标签**: streaming, llm-based, synthesis
  - **TLDR**: T-Mimi replaces Mimi's convolutional decoder with a transformer-only architecture to achieve 9x faster on-device TTS with minimal quality loss.
  - **核心贡献**: A purely transformer-based decoder for the Mimi codec that reduces on-device TTS latency from 42.1ms to 4.4ms while maintaining audio quality.
  - **方法**: Replaces convolutional components in Mimi's decoder with transformer layers inspired by TS3-Codec, employs quantization-aware training with mixed precision (8-bit for most layers, 32-bit for final layers), and optimizes for mobile CPU efficiency using XNNPACK.
  - **关键发现**: Achieves 9x latency reduction (42.1ms → 4.4ms), maintains comparable audio quality (PESQ scores near baseline), 8-bit quantization reduces storage by 75% with minimal quality loss, final two transformer layers require 32-bit precision for quality preservation.
  - **评估**: medium (评分: 7/10)

- **Phonological Tokenizer: Prosody-Aware Phonetic Token via Multi-Objective Fine-Tuning with Differentiable K-Means** (2026-01-27)
  - **作者**: Kentaro Onda et.al.
  - **arXiv**: [2601.19781](https://arxiv.org/abs/2601.19781)
  - **标签**: expressive
  - **TLDR**: Phonological Tokenizer fine-tunes phonetic tokens via differentiable k-means with multi-task ASR and speech resynthesis objectives to better capture linguistic and prosodic information while discarding speaker identity.
  - **核心贡献**: Introduces a phonological tokenizer that effectively incorporates prosodic information through multi-objective fine-tuning of SSL models, achieving superior performance in prosody-sensitive tasks compared to existing acoustic and phonetic tokenizers.
  - **方法**: Fine-tunes WavLM-large SSL features using differentiable k-means clustering with weighted ASR and speech resynthesis losses, trained on LibriSpeech-100h subset, evaluated across discriminative (ASR, ER, SID), generative (reconstruction, VC), and speechLM tasks.
  - **关键发现**: Outperforms baselines in prosody-sensitive tasks (ER, VC), achieves competitive ASR performance, demonstrates strong speechLM capabilities (GenPPL, UTMOS), and shows optimal performance with balanced α weighting between ASR and reconstruction objectives.
  - **评估**: medium (评分: 7/10)

## 完整列表（按日期）
### 2026-02-02
- **LipSody: Lip-to-Speech Synthesis with Enhanced Prosody Consistency**
  - **作者**: Jaejun Lee et.al.
  - **arXiv**: [2602.01908](https://arxiv.org/abs/2602.01908)
  - **TLDR**: LipSody improves lip-to-speech synthesis by explicitly modeling prosody consistency using speaker identity, linguistic content, and emotional context from facial video.
  - **核心贡献**: Introduces a novel visual-only prosody estimation method and integrates it into a diffusion-based lip-to-speech framework to enhance prosody consistency while maintaining intelligibility.
  - **关键发现**: Significantly improves prosody-related metrics (pitch deviation, energy consistency, speaker similarity) over LipVoicer while maintaining high intelligibility (WER); subjective tests show better naturalness and preference.


### 2026-02-01
- **Bias in the Ear of the Listener: Assessing Sensitivity in Audio Language Models Across Linguistic, Demographic, and Positional Variations**
  - **作者**: Sheng-Lun Wei et.al.
  - **arXiv**: [2602.01030](https://arxiv.org/abs/2602.01030)
  - **TLDR**: This paper systematically investigates speech bias in multilingual MLLMs across linguistic, demographic, and positional variations, revealing high sensitivity to language and option order but relative robustness to demographic factors.
  - **核心贡献**: The first systematic investigation of speech bias in multilingual MLLMs, introducing the BiasInEar dataset and a unified framework for assessing fairness and robustness in speech-integrated LLMs.
  - **关键发现**: MLLMs are relatively robust to demographic factors but highly sensitive to language and option order; architectural design and reasoning strategy substantially affect robustness across languages; larger models show higher consistency but still exhibit structural biases.

- **EmoAra: Emotion-Preserving English Speech Transcription and Cross-Lingual Translation with Arabic Text-to-Speech**
  - **作者**: Besher Hassan et.al.
  - **arXiv**: [2602.01170](https://arxiv.org/abs/2602.01170)
  - **TLDR**: EmoAra is an end-to-end pipeline that preserves emotional context when translating English speech to Arabic speech for banking customer service.
  - **核心贡献**: Integration of emotion-preserving components (SER, ASR, MT, TTS) into a unified cross-lingual spoken communication system.
  - **关键发现**: Achieved 94% F1-score for emotion classification, BLEU 56 and BERTScore F1 88.7% for translation, and 81% human evaluation score on banking-domain translations.


### 2026-01-31
- **Edit Content, Preserve Acoustics: Imperceptible Text-Based Speech Editing via Self-Consistency Rewards**
  - **作者**: Yong Ren et.al.
  - **arXiv**: [2602.00560](https://arxiv.org/abs/2602.00560)
  - **TLDR**: A novel text-based speech editing method that decouples content editing from acoustic reconstruction using semantic tokens and Flow Matching, achieving superior imperceptibility and robustness.
  - **核心贡献**: Introduces a decoupled architecture that separates semantic editing from acoustic reconstruction, using a pre-trained TTS model as a consistency critic and GRPO for alignment, outperforming existing AR and NAR baselines.
  - **关键发现**: Significantly outperforms state-of-the-art AR and NAR baselines in intelligibility (WER), speaker similarity, and perceptual quality (DNSMOS, Subjective MOS), with robust performance across varying edit durations and contexts.


### 2026-01-30
- **EmoShift: Lightweight Activation Steering for Enhanced Emotion-Aware Speech Synthesis**
  - **作者**: Li Zhou et.al.
  - **arXiv**: [2601.22873](https://arxiv.org/abs/2601.22873)
  - **TLDR**: EmoShift introduces a lightweight activation-steering framework for emotion-aware TTS that learns emotion-specific steering vectors to enhance expressiveness while preserving naturalness and speaker similarity.
  - **核心贡献**: Proposes EmoShift, a parameter-efficient activation-steering method with an EmoSteer layer that learns emotion-specific steering vectors to control emotional expression in TTS without full fine-tuning.
  - **关键发现**: EmoShift outperforms zero-shot and fully fine-tuned baselines in both objective (Word Error Rate, Speaker Similarity, Emotion Recognition Accuracy) and subjective (Mean Opinion Score, Emotion MOS) evaluations while maintaining naturalness and speaker similarity.

- **Evaluating and Rewarding LALMs for Expressive Role-Play TTS via Mean Continuation Log-Probability**
  - **作者**: Yong Ren et.al.
  - **arXiv**: [2601.22661](https://arxiv.org/abs/2601.22661)
  - **TLDR**: A paper introducing Mean Continuation Log-Probability (MCLP) as both an evaluation metric and reward signal for improving expressive role-play TTS using Large Audio Language Models.
  - **核心贡献**: Proposes MCLP to quantify stylistic consistency in role-play TTS and uses it as a reinforcement learning reward to enhance style alignment with instructions.
  - **关键发现**: Significant improvements over strong LALM baselines on both objective (WER, CER, Pinyin Error Rate) and subjective metrics (MOS approaching ground truth at 3.646), with exceptionally low CER and Pinyin error rates.

- **Now You Hear Me: Audio Narrative Attacks Against Large Audio-Language Models**
  - **作者**: Ye Yu et.al.
  - **arXiv**: [2601.23255](https://arxiv.org/abs/2601.23255)
  - **TLDR**: A jailbreak attack on large audio-language models using narrative-style TTS delivery achieves 98.26% success rate, significantly exceeding text-only baselines.
  - **核心贡献**: Demonstrates that prosodic and paralinguistic features in synthetic speech can bypass safety mechanisms in audio-language models, revealing a new attack vector.
  - **关键发现**: Achieves 98.26% success rate on Gemini 2.0 Flash and other models, outperforming text-only baselines; vocal tone variation alone can substantially shift model behavior even without adversarial content.


### 2026-01-29
- **Speech Quality-Based Localization of Low-Quality Speech and Text-to-Speech Synthesis Artefacts**
  - **作者**: Michael Kuhlmann et.al.
  - **arXiv**: [2601.21886](https://arxiv.org/abs/2601.21886)
  - **TLDR**: The paper proposes using utterance-level speech quality predictors with segment-based consistency constraints to improve frame-level interpretability and localization of TTS synthesis artifacts.
  - **核心贡献**: Demonstrates that utterance-level SQA models can be regularized with segment-based consistency constraints to reduce frame-level stochasticity, enabling better localization of synthesis artifacts.
  - **关键发现**: Consistency regularization improves frame-level predictions. Models successfully detect synthesis artifacts with higher precision than random control sets. Listening tests confirm listeners rate detected segments as lower quality.

- **Unit-Based Agent for Semi-Cascaded Full-Duplex Dialogue Systems**
  - **作者**: Haoyuan Yu et.al.
  - **arXiv**: [2601.20230](https://arxiv.org/abs/2601.20230)
  - **TLDR**: A semi-cascaded full-duplex dialogue system using multimodal LLMs and minimal conversational units achieves strong performance without training.
  - **核心贡献**: Introduces a unit-based agent framework that decomposes dialogue into minimal conversational units for independent processing and prediction of transitions.
  - **关键发现**: Achieves second place among all teams on the HumDial test set of the Human-like Spoken Dialogue Systems Challenge (Track 2: Full-Duplex Interaction), demonstrating effectiveness of the framework.


### 2026-01-28
- **ASR for Affective Speech: Investigating Impact of Emotion and Speech Generative Strategy**
  - **作者**: Ya-Tse Wu et.al.
  - **arXiv**: [2601.20319](https://arxiv.org/abs/2601.20319)
  - **TLDR**: This paper investigates how emotional speech and generative strategies affect ASR performance, introducing two fine-tuning approaches that improve emotion-aware ASR without degrading clean speech recognition.
  - **核心贡献**: Introduces two generative strategies (transcription correctness and emotional salience) for constructing fine-tuning subsets that improve ASR performance on emotional speech while maintaining neutral speech recognition.
  - **关键发现**: Fine-tuning with generative strategies achieves consistent WER improvements on real emotional datasets (MSP Podcast Test1/2, IEMOCAP) without degradation on clean LibriSpeech; combined TTS-EMO-G strategy shows strongest gains, particularly for expressive speech.

- **Audio Deepfake Detection in the Age of Advanced Text-to-Speech models**
  - **作者**: Robin Singh et.al.
  - **arXiv**: [2601.20510](https://arxiv.org/abs/2601.20510)
  - **TLDR**: A multi-view detection approach combining semantic, structural, and signal-level analysis outperforms single-paradigm detectors against diverse TTS architectures.
  - **核心贡献**: First empirical characterization of how detection architectures perform against three state-of-the-art TTS models (Dia2, Maya1, MeloTTS), demonstrating that integrated detection strategies are necessary for robust audio deepfake detection.
  - **关键发现**: Single-paradigm detectors show significant variability in performance across TTS architectures, with proprietary model achieving near-perfect separation (EER < 1%) across all models, while multi-view approach demonstrates robust performance across all evaluated models.

- **Erasing Your Voice Before It's Heard: Training-free Speaker Unlearning for Zero-shot Text-to-Speech**
  - **作者**: Myungjin Lee et.al.
  - **arXiv**: [2601.20481](https://arxiv.org/abs/2601.20481)
  - **TLDR**: TruS introduces a training-free speaker unlearning method for zero-shot TTS that suppresses specific speaker identities at inference time without retraining.
  - **核心贡献**: The first training-free speaker unlearning framework that generalizes to both seen and unseen opt-out speakers by steering identity-specific hidden activations in TTS models.
  - **关键发现**: TruS effectively prevents voice generation of both seen and unseen opt-out speakers while preserving other attributes like prosody and emotion, achieving comparable unlearning performance to retraining-based methods without the computational cost.


### 2026-01-27
- **Phonological Tokenizer: Prosody-Aware Phonetic Token via Multi-Objective Fine-Tuning with Differentiable K-Means**
  - **作者**: Kentaro Onda et.al.
  - **arXiv**: [2601.19781](https://arxiv.org/abs/2601.19781)
  - **TLDR**: Phonological Tokenizer fine-tunes phonetic tokens via differentiable k-means with multi-task ASR and speech resynthesis objectives to better capture linguistic and prosodic information while discarding speaker identity.
  - **核心贡献**: Introduces a phonological tokenizer that effectively incorporates prosodic information through multi-objective fine-tuning of SSL models, achieving superior performance in prosody-sensitive tasks compared to existing acoustic and phonetic tokenizers.
  - **关键发现**: Outperforms baselines in prosody-sensitive tasks (ER, VC), achieves competitive ASR performance, demonstrates strong speechLM capabilities (GenPPL, UTMOS), and shows optimal performance with balanced α weighting between ASR and reconstruction objectives.

- **Rethinking Discrete Speech Representation Tokens for Accent Generation**
  - **作者**: Jinzuomu Zhong et.al.
  - **arXiv**: [2601.19786](https://arxiv.org/abs/2601.19786)
  - **TLDR**: This paper systematically investigates how accent information is encoded in Discrete Speech Representation Tokens (DSRTs) and proposes new accent-aware token designs for controllable accent generation.
  - **核心贡献**: First systematic investigation of accent information in DSRTs, proposing a unified evaluation framework combining Accent ABX task and cross-accent Voice Conversion resynthesis, along with new content-only and content-accent DSRT designs that outperform existing approaches.
  - **关键发现**: Accent information is substantially reduced when ASR supervision is used to fine-tune encoders, but cannot be effectively disentangled from phonetic and speaker information through naive codebook size reduction. The proposed content-only and content-accent DSRTs significantly outperform existing designs in controllable accent generation.

- **T-Mimi: A Transformer-based Mimi Decoder for Real-Time On-Phone TTS**
  - **作者**: Haibin Wu et.al.
  - **arXiv**: [2601.20094](https://arxiv.org/abs/2601.20094)
  - **TLDR**: T-Mimi replaces Mimi's convolutional decoder with a transformer-only architecture to achieve 9x faster on-device TTS with minimal quality loss.
  - **核心贡献**: A purely transformer-based decoder for the Mimi codec that reduces on-device TTS latency from 42.1ms to 4.4ms while maintaining audio quality.
  - **关键发现**: Achieves 9x latency reduction (42.1ms → 4.4ms), maintains comparable audio quality (PESQ scores near baseline), 8-bit quantization reduces storage by 75% with minimal quality loss, final two transformer layers require 32-bit precision for quality preservation.

