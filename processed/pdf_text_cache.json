{
  "2601.02073": "=== METHOD ===\nered perceptually acceptable and intelligible. A baseline model\nTTS model was built with VITS. In both subjective and objec-\ntive evaluations, the VITS model outperformed the Tacotron2\nmodel. In terms of tone synthesis, the VITS model showed sig-\nnificantly lower tone errors than the Tacotron2 model. The pa-\ninput for subsequent alignment and modelling. Owing to its\nwe model a baseline Mizo TTS system using Tacotron2 and\ning section, Section 2, describes the methodology of the cur-\n2. Methodology\nand used for modelling the TTS reported in the current work.\ntion. The TTS models are designed utilising the ESPnet2 toolkit\n[14]. Tacotron2 and the VITS models are natively supported in\nsound files. This constituted the metadata for TTS model train-\nTwo neural text-to-speech architectures were investigated in this\nstudy: Tacotron2 and VITS. All models were implemented us-\nspectrograms predicted by the acoustic model were converted\nstage TTS pipeline, consisting of an acoustic model and a neu-\nral vocoder. In contrast, VITS is an end-to-end architecture that\njointly models acoustic representation learning and waveform\nWe trained a Tacotron2 text-to-speech model using the cor-\npus described in Section 2.1. The model uses 512-dimensional\nwaveform text-to-speech model using ESPnet2 on a single-\ntional layers, normalizing flows for latent modeling, and a HiFi-\nwith the best models selected based on training progress.\nwere generated using the Tacotron2 and VITS models and were\nMOS) was used. The metric is modelled from the scores from\nfrom Tacotron2 and VITS TTS models.\nmodels were not highly correlated with the ground truth. How-\noutputs from each model. The results of the t-test are sum-\nTacotron2 model is statistically significant (p<0.0001), in terms\nof MCD, with the VITS model outperforming the other. How-\nthe VITS model marginally outperforms the Tacotron2 model.\noutputs from the Tacotron2 and VITS models.\nthe VITS model are considered better by the native speakers of\nTable 4: Results of Type II Wald χ2 test on the LME model.\nare significantly different from each other. The model used was\nspeech and Tacotron2 and VITS models. Sentence is a unique\nthe model, Type, Sentences and their interaction are consid-\nOnce the model is generated, it is subjected to a Type II\nmodels.\nLME model to a pairwise comparison using the Bonferroni\npost-hoc method. The results of the pairwise comparison are\nwhere the tone was incorrectly generated by the models. The\nThis paper reported an attempt to develop a TTS model for the\nConsidering this, two approaches\nwe decided to build systems using both approaches.\nhowever, the two approaches were able to synthesize the correct\nin a better language model. The results suggest that, without ex-\nmodeling can be achieved. In the long term, we would like to\n“Wavenet: A generative model for raw audio,” in 9th ISCA Work-\nwith a crowdsourcing approach,” ITU-T Rec. P.808 (06/2021),\n[19] R. A. Clark and K. E. Dusterhoff, “Objective methods for evaluat-\nmixed-effects models using lme4,” Journal of Statistical Software,\n\n\n=== EXPERIMENT ===\nof subjective and objective evaluations, the outputs were consid-\ntive and quantitative evaluation. While TTS for high-resource\nTable 1: Portion of the Mizo TTS datasets details, represents\nclipping at 1.0, and length-based batching. Experiments were\n2.5. Objective and subjective evaluation\ndataset prepared for TTS training. However, these 25 sentences\nincluded as part of the evaluation set for subjective and objec-\ntive analyses. This resulted in an evaluation set consisting of 75\nFor objective evaluation, the recent speech quality metric,\nITU-T Rec. P.808 subjective evaluation [15]. This metric pro-\nfine-grained phonetic and prosodic objective evaluation is im-\nIn the subjective evaluation, 35 Mizo native speakers partic-\nand 1 was ‘Bad’. The evaluation was designed as a web appli-\n3.1. Objective evaluation\nThe results of the objective evaluation on 25 sentences are pro-\n3.2. Subjective evaluation\nSubjective evaluation was conducted using the MOS score\nFor inferential statistics, as the dataset for MOS scores is\nThe objective evaluation of the outputs showed that the\nevaluations were conducted, which confirmed that while the\nTTS evaluation in low-resource and tonal languages. While it\nsources and evaluation conference, 2020, pp. 6458–6461.\nStandardization Sector, “Subjective evaluation of speech quality\n\n\n=== RESULTS ===\nframework. The subsequent version, WaveNet2, could result in\neffective in maintaining prosodic information, resulting in more\nvides a description of the results and discussion and finally, Sec-\nTable 2: Results of DNSMOS, MCD, RMSE f0, F0 F0 corr,\n3. Results and discussion\nvided in Table 2. The results show that overall, the VITS system\nof vowels and consonants. The results of the RMSE f0 and\never, in terms of the DNSMOS results, RMSE f0 and F0 corr,\nthe results are not significantly different (p>0.05), even though\nTable 3: Results of the t-tests conducted for MCD, F0 rmse,\neffects. The results of the test are provided in Table 4. The re-\nprovide results interpretable for improvement in the segmental\ntone, possibly due to the variety of text data, ultimately resulting\n\n\n=== CONCLUSION ===\nfollowed by a discussion of the data collection and annotation\n4. Conclusion",
  "2601.03888": "=== METHOD ===\ndation model comprising two core components: a Transformer-based Text-to-\nbackbone of the S2M module with a more efficient Zipformer-based modeling\narchitecture, achieving notable parameter reduction and faster mel-spectrogram\nmodeling strategies-boundary-aware alignment, token-level concatenation, and\ntext-to-speech (TTS) models have made remarkable progress, particularly in model architecture\nand target application scenarios, mainstream large-scale TTS models have diverged into multiple\ntechnical paradigms, with their core differences primarily lying in model architecture design and the\nchoice of intermediate representation modeling.\nAs illustrated in Figure 1, modern large-scale zero-shot TTS architectures typically comprise a set\nof core components: a Transformer-based language model, a generative module based on diffusion\nEarly large-scale TTS models, such as VALL-E [1], pioneered the use of residual vector quantization\nLanguage Model\nModel\nFigure 1: Schematic diagram of the technical approaches for current mainstream large-scale zero-shot\nspeech synthesis models.\n(RVQ) [15, 16] to discretize speech into acoustic tokens and leveraged language models—inspired\nby the success of large language models—to generate these discrete tokens conditioned on text\nlanguage models (LLMs) for zero-shot TTS, helping to establish a scalable generative framework that\nlines of model evolution have emerged. For instance, IndexTTS 1 [6] discards the codec decoder\nfrom discrete tokens [19, 20]. In contrast, models like Seed-TTS [8] and MiniMax-Speech [14]\nadopt a two-stage approach—first generating coarse speech tokens with a language model, then\nacoustic characteristics. The prevailing architecture today employs a language model to generate\nsemantic tokens, followed either by another language model that predicts acoustic tokens [5, 21] or\nparadigm that has become the dominant approach in modern zero-shot TTS systems. Moreover,\nlightweight models [22] such as F5-TTS [4] and ZipVoice [23, 24] adopt a fully non-autoregressive\napproach to directly model the alignment between text and speech, enabling fast inference while\nbetween discrete tokens and continuous representations—in terms of information fidelity, modeling\nmodeling continuous speech representations [25–27]. This paradigm aims to eliminate reliance on\npipeline: it first employs a language model to generate discrete semantic tokens encoding emotion-\naddress these limitations, we present IndexTTS 2.5, a multilingual zero-shot TTS model that not only\nration pipeline. Building on this pipeline, we propose three modeling strategies—boundary-\nHz to 25 Hz and replacing U-DiT with the Zipformer architecture [28] in the flow-matching\n• We apply GRPO [29] to post-train the language model for semantic token generation, leading\nVoice activity detection and segmentation. A voice activity detection model enhanced with event\nfiltering employs a pre-trained audio quality assessment model, while text quality filtering leverages\nfour key components: multilingual text-to-semantic modeling, semantic codec frame rate compression,\nThe text-to-semantic module in IndexTTS 2.5 retains the architecture of IndexTTS 2. Specifically, it\nFigure 3: Illustration of three multilingual modelling strategies: boundary-aware alignment, token-\neffective multilingual modeling strategies, providing clear signals for language boundaries during\ntic supervision, enabling the model to disambiguate pronunciation based on the token’s linguis-\nsystem integration more complex than with the boundary-aware alignment approach.\nTo better exploit the text comprehension capabilities of the underlying language model, we adopt an\ninput text to explicitly condition the model on the target language. To enhance training diversity and\nZipformer architecture [28]. The shortened token sequence lowers memory consumption and compu-\ntational load during both training and inference, while the compact Zipformer architecture further\nreduces model complexity. These design choices collectively improve inference efficiency with\nWe fine-tune the text-to-semantic model using GRPO with a preference-based objective. For each\nWe train a multilingual text-to-speech model covering Mandarin, English, Japanese, and Spanish,\nTable 1: Performance comparison of various multilingual modeling methods (Boundary-Aware\nModel\nASR models: FunASR [40] for SeedTTS test-zh, and Whisper-large-v3 [41] for SeedTTS test-\nspeaker verification model [40], then computing their cosine similarity. Emotional similarity is\nWe compare our model against state-of-the-art zero-shot multilingual TTS systems, including\nComparative Analysis of Multilingual Modelling Methods\nTo ensure a fair comparison, all three multilingual modeling strategies—Boundary-Aware Alignment,\nWe then conduct a comprehensive evaluation of these models across four test sets covering Mandarin,\nModel\nModel\nsimilarity comparable to or better than recent high-performing models such as CosyVoice 3 and\nrefined model denoted as IndexTTS 2.5-RL. This variant demonstrates a measurable reduction in\nFigure 4: Subjective preference comparison of U-DiT and Zipformer backbone architectures in the\nTable 4: Inference speed comparison of IndexTTS 2 series models.\nModel\nSemantic-to-Mel Architecture Ablation\npreference. This consistent preference suggests that the Zipformer-based architecture generates more\nnatural and speaker-consistent speech, likely due to its efficient modeling of long-range dependencies\ncounterpart in zero-shot multilingual TTS, offering a favorable trade-off between model capacity and\nimprovements: semantic codec compression, an optimized S2M architecture based on Zipformer,\ncross-lingual modeling strategies, and reinforcement learning–based T2S optimization. Experimental\nand F. Wei, “Neural codec language models are zero-shot text to speech synthesizers,” CoRR, vol.\nScalable streaming speech synthesis with large language models,” arXiv preprint arXiv:2412.10117, 2024.\nAn efficient llm-based text-to-speech model with single-stream decoupled speech tokens,” arXiv preprint\n“Seed-tts: A family of high-quality versatile speech generation models,” arXiv preprint arXiv:2406.02430,\nD. Grangier, M. Tagliasacchi, and N. Zeghidour, “Audiolm: A language modeling approach to audio\nlanguage models for advanced multilingual text-to-speech synthesis,” arXiv preprint arXiv:2411.01156,\net al., “Xtts: a massively multilingual zero-shot text-to-speech model,” arXiv preprint arXiv:2406.04904,\nDiffusion transformer autoregressive modeling for speech generation,” in Forty-second International Con-\n[26] C. Y. Wu, J. Deng, G. Li, Q. Kong, and S. Lui, “Clear: Continuous latent autoregressive modeling for\ntransformer-diffusion model with representation alignment for speech synthesis,” 2025.\n“Deepseekmath: Pushing the limits of mathematical reasoning in open language models,” 2024.\n\n\n=== EXPERIMENT ===\ning pronunciation accuracy and naturalness. Experiments show that IndexTTS 2.5\nshot cross-lingual TTS. Comprehensive evaluation not only confirms their individual effec-\nExperiments and Evaluation\nTraining and Evaluation Datasets\nMandarin and English data are derived primarily from the Emilia dataset [31], augmented with\nmultiple public sources: Common Voice [32], the Argentinian Spanish Speech Dataset [33], the\nTEDx Spanish Corpus [34], the Crowdsourced High-Quality Spanish Speech Dataset [35], and\nDataset\nemotional speech, including 29 hours from the Emotional Speech Dataset (ESD) [37] and 106 hours\nFor evaluation on Mandarin and English, we adopt the SeedTTS test-zh and SeedTTS test-en\nEvaluation Metrics and Baselines\nWe conduct a comprehensive evaluation across four dimensions: WER, speaker similarity (SS),\ntions: using the same multilingual dataset, batch size, optimizer, learning rate schedule, and hardware\nVoice Cloning Evaluation\nMultilingual Emotional TTS Evaluation\nWe conduct a subjective preference evaluation between U-DiT and Zipformer as",
  "2601.05911": "=== METHOD ===\nWe release Pantagruel models, a new family of self-supervised encoder models for French text and speech. Instead\nmore effectively. Separate models are pre-trained on large-scale French corpora, including Wikipedia, OSCAR and\nfrom the standard French benchmarks such as FLUE or LeBenchmark. Across these tasks, Pantagruel models show\nLeBenchmark 2.0, while maintaining a shared architecture that can seamlessly handle either speech or text inputs.\nKeywords: self-supervised learning, JEPA, data2vec, French language models, speech and text encoders,\nmultimodal representation learning, joint-embedding predictive architecture, predictive modeling\nprocessing. Text models such as FlauBERT (Le\nRecent predictive approaches (LeCun, 2022;\narchitecture (Baevski et al., 2022), an instance of\nthe joint embedding predictive architecture (JEPA)\nleaving limited room for embedding-based methods\nto improve over input-level approaches (Van Assel\ntive with masked language modeling (MLM, Devlin\nspeech and text models are released on the Hug-\nJEPA frameworks, which allows training models\nscale INA-100k broadcast corpus on the models’ per-\nself-supervised architectures to both modalities.\noutperforms multilingual models on downstream\noffs with compact models such as FrALBERT (Cat-\ndecoder and decoder-only approaches have also\nmodels such as CroissantLLM (Faysse et al.,\nken French, providing wav2vec2.0-based models\ncomprehensive ecosystem of French models cov-\ninputs, JEPA-style models learn to predict latent\nFigure 1: Overview of the Pantagruel model architecture. The network starts with a modality-specific\nan additional masked language modeling (MLM) loss is used. The teacher’s parameters are updated as\na hybrid approach that combines JEPA-style rep-\nModels and Pre-training\nOur SSL models for speech are based on\ndata2vec 2.0 (Baevski et al., 2023). For text models,\nmasked language modeling (MLM) objective (De-\nencoder architecture (Vaswani et al., 2017). The\nthis approach can be viewed as an instance of the\npredictive modeling paradigm (LeCun, 2022; As-\neach modality, following the data2vec approach.\nidea, we enhance our non-generative text models\nloss, encouraging the model to capture local se-\nWe explored two approaches to incorporate the\nMLM loss into our text-based model.\napproach follows the original BERT implementa-\nBoth methods improve performance compared to\nlatter approach yielding the best results. We there-\nfore adopt this approach in all our models.\nPantagruel model configurations\ntokenizer on our models.\nModel configurations\nformer architectures, we propose two model con-\nmodels is given in Table 1. For text models, we\nWe would like to note that text models listed in\ndata2vec loss. These were our starting models\ncompound loss, which are our final settings. Model\nof these models when they will be available. As\nModel name\nTable 1: Pantagruel model names and configura-\nrespectively. Models trained with Camembert tok-\nenizer are denoted with “camtok”. For text models,\nfigurations for each model, which will be released\ntrain Pantagruel-B-camtok-Wk model versions. The\ntrain Pantagruel-B-camtok-Osc model versions. We\nHowever, these remain far from SOTA models,\nent tokenizers on our text models and the impact\nmodels. The ablation results are reported on the\ntrained four speech models: two models using the\nbase architecture on 1K hours of LibriSpeech and\nmodels using the large architecture on 14K hours\n8.92 to 8.46). For the large architecture, however,\nthe model capacity being potentially too limited to\nText models evaluation\nModel/Data\nmodel architectures and data sizes\nric used for evaluating the models is the F1-score\nCoref model (Dobrovolskii, 2021; D’Oosterlinck\n10% of the train split for model validation during\nas different models use different tokenizers that\nto different evaluations for different models.\nWe evaluate the text models\nand dependency parsing. The overall methodology\nand downstream model architecture are based on\n(Table 5), Pantagruel models lag slightly be-\nPantagruel models match or outperform the base-\nOverall, models results and analyses suggest\nSpeech models evaluation\nsess SSL models on ASR in two settings: with a\nModel\nModel\nthe SSL encoder, and the whole model is finetuned.\nand without the use of a language model.\nNext, we assess SSL models on ASR on a\ntures. We first benchmark models on the French\nFinally, we assessed the ability of ASR models\nfor fine-tuning the SSL models. The validation and\nmark our models on the French ETAPE cor-\nModel\nModel\nModel and hyperparameters. A 5-layer BiLSTM\none scalar per frame. Models are trained for 200\nassess the ability of SSL models to encode both\nmodalities individually. Downstream SLU models\nare based on the Transformer architecture and use\nlr depending on the models. Results are reported\nlines. The large model Pantagruel-L-114k achieves\ning all other models and Pantagruel-L-14k remaining\nOverall, the Pantagruel model family demon-\nAcross modalities, Pantagruel models exhibit com-\nOverall, Pantagruel models demonstrate that\nto French. Its unified architecture—trained sep-\npractical foundation for cross-modal modeling. To\nmodel capacity and corpus diversity; (iii) extending\nlanguage model aged to perfection.\nembedding predictive architecture.\n2025. V-JEPA 2: Self-supervised video models\nwal, et al. 2020. Language models are few-shot\nmodels for a french question-answering task -\nthe 2009 Conference on Empirical Methods in\nGiuseppe Riccardi. 2009c. Re-ranking models\nence of Empirical Methods for Natural Language\nConference on Empirical Methods in Natural Lan-\nComputational Models of Reference, Anaphora\npretrained french sequence-to-sequence model.\nA truly bilingual french-english language model.\nd’alembert: a large corpus and a language model\n2025. Llm-jepa: Large language models meet\njoint embedding predictive architectures. arXiv\nA method for stochastic optimization. In ICLR\nmodel. In LREC, pages 4275–4284. European\nvised language model pre-training for french.\ntraining data makes language models better. In\nComputational Models of Reference, Anaphora\ntraining approach. CoRR, abs/1907.11692.\nguage model. In Proceedings of the 58th Annual\nand Yannick Estève. 2022. End-to-end model\ndata volume for compact language models. In\njoint-embedding predictive architecture with con-\nembedding predictive architecture.\nA large autoregressive french language model.\nWorkshop on Computational Models of Refer-\ntext for language model pre-training applied to\nLanguage models are unsupervised multitask\nmodel. CoRR, abs/2211.05100.\nsuite of language models and evaluation tasks\nJoint-embedding predictive architecture for audio\ning unlocks robust audio foundation models for\nTable 10: Statistics for the speech corpora used to train SSL models according to gender information\n\n\n=== EXPERIMENT ===\net al., 2025). Our experiments on text-based mod-\nand CroissantLLM (Faysse et al., 2024) datasets.\nformances. Finally, we provide a unified evaluation\nand evaluation resources for speech understand-\nfeature prediction within a self-distillation setup is\nthe CroissantLLM dataset (Faysse et al., 2024) with\ncroissantLLM datasets are still in progress. We will\nwith much bigger datasets such as OSCAR and\nthe last suffix denotes the text dataset names, while\nfor speech it is the size of the datasets in terms of\nDataset\nDatasets and Resources\ngoal was to maximize dataset size, including both\ndatasets, the proprietary INA corpus, and prepro-\nText datasets\ntwo main public datasets: the Wikipedia dump of\nand the French portion of the OSCAR dataset\nFrench portion of the OSCAR dataset, including ap-\nSpeech datasets\nThe complete list of datasets considered in this\nrized in Appendix 10. We used the dataset collec-\ntilingual LibriSpeech (MLS) dataset (Pratap et al.,\ntraining speech dataset, a corpus of 100,000 hours\ndard benchmark evaluation corpora are also based\nlanguage inference (XNLI dataset) for text and auto-\nmatic speech recognit",
  "2601.12480": "=== METHOD ===\nA Unified Neural Codec Language Model for Selective Editable\nNeural codec language models achieve im-\nguage model SpeechEdit that extends zero-\nour approach maintains naturalness and robust-\nern generative modeling, enabling high-fidelity\ntoken modeling (Du et al., 2024; Yang et al., 2025b;\nmodels treat the reference audio as a holistic, black-\nproaches. Text-driven methods rely on textual di-\nspeaker’s voice. Audio-driven approaches use dual\nchallenges necessitate a more precise approach\ntributes through specialized architectures or train-\ntion enables a single model to seamlessly support\ntroduce a Delta-Pairs sampling method to generate\nsion, and style editing within a single model,\nNeural codec language modeling treats speech syn-\nthesis as a sequence modeling problem over dis-\n(AR) and Non-Autoregressive (NAR) architecture.\ntures, codec language models involve clear trade-\noffs. AR models achieve strong perceptual qual-\nity by modeling temporal dependencies, but suffer\nNAR and partially NAR models improve efficiency\nvia parallel generation and duration modeling, of-\nglement strategies. One approach, exemplified by\nglement methods resolve conflicts through mech-\nquire additional model components.\nmodel performs selective attribute editing through data-driven implicit disentanglement with delta pairs.\nProposed Method\nediting specification condition C, the AR model\nprompt, and θAR denotes the AR model trainable\ntions, the NAR model refines acoustic details by\non task-specific architectures or auxiliary disentan-\nmodel multiple speech attributes during generation\ntion is modeled with five predefined classes: Neu-\nprint model1. This embedding is projected into the\nguide the model’s attention to the explicit control\nare sampled from different speakers. The model\nBy conditioning the model on mismatched acous-\nediting tasks within a single model by reorganiz-\nto <fill-in> forces the model to rely entirely on the\nmodel (SER) 3, which predicts an 8-way emotion\nemotion labels via multi-model cross-validation\nsegments agreed upon by at least two models and\nTraining Dataset. We train the SpeechEdit model\nModel Configuration. Both AR and NAR stages\ntual modeling, the first-stage AR model employs\nThe model is optimized using Adam with β =\nstrategy is applied in both stages, with all model\n2025) which are leading open-source models for\nmodel (Gulati et al., 2020) with ground-truth text.\nmodel (Reddy et al., 2021) trained on human rat-\npre-trained-models\nStep-Audio-EditX, our model uses much less train-\nsity of LibriEdit, but overall the model maintains\nTo comprehensively evaluate the model’s capa-\nModel\nParams refers to the number of parameters in the AR model. # / h indicates the amount of training data in hours.\n∗Indicates the amount of task-specific training data used after model initialization.\nthat automatic speech recognition models tend to\ncontinuous-domain modeling in the second stage,\nmodel’s\nproposed model, we conduct a subjective timbre\nFirst, the granularity of speaker modeling remains\nond, the model relies entirely on implicit disen-\nmodels. arXiv preprint arXiv:2406.02430.\nWei. 2024. Vall-e 2: Neural codec language models\nral codec language models are zero-shot text to\naudio language models. In The Thirty-ninth Annual\nmodels. In Proc. IEEE ICASSP. IEEE.\ntorized codec and diffusion models. arXiv preprint\ncodec language model for zero-shot text-to-speech.\nspeech model on 100k hours of data. arXiv preprint\nscalable pipeline for human-like speech modeling\nlanguage modeling with alignment-guided sequence\ntts: An efficient llm-based text-to-speech model\nthesis in the era of large language models: A system-\non Empirical Methods in Natural Language Process-\nspeech model with freestyle text prompting. In Pro-\nautoregressive neural codec language models for effi-\nmodelling. In Proceedings of the 32nd ACM Interna-\nanced optimization signal, causing the model to\nablated model yields a non-negligible improvement\n\n\n=== EXPERIMENT ===\nstructed LibriEdit dataset, which provides delta\nLibriHeavy. Experimental results show that\nsettings, this setup requires fine-grained, attribute-\nels (LMs)—trained on large, diverse datasets span-\na new dataset, LibriEdit, by labeling the speech\ntion. Experimental results conducted on various\nnotated LibirEdit dataset, enabling promising\nLibriEdit Dataset\nHeavy dataset (Kang et al., 2024) which is cho-\nriEdit dataset comprises 2566 speakers with a total\nDataset Construction Pipeline\nExperiment Setup\nfollowing the VALL-E setup for 800k updates, and\nthen further trained on the target training dataset for\nBaselines and Evaluation Metrics\nEvaluation Results\nObjective Evaluation\nevaluation protocol by performing five times sam-\nimental setups based on the relationship between\nriEdit dataset, with five target emotions roughly\nriEdit dataset, where Sad is most frequent. While\nSubjective Evaluation\nattribute controllability, with objective evaluations\nsimilarity evaluation with four speakers, including\nidentity. Figure 7 summarizes evaluation results.\nFurthermore, we constructed the LibriEdit dataset\nized architectural modules. Experiments across\nnition challenge: Dataset, baseline framework, and\nanechoic fullband speech dataset benchmarked for\nSubjective Evaluation\nting for most experiments reported in this paper,\nTable 2: Comparison of open-sourced speech datasets in terms of fine-grained style control speech synthesis.\nDataset\nTable 3: Evaluation criteria for SMOS and SSIM.\ninternal emotional speech dataset, and the Expresso\ndataset (Nguyen et al., 2023). The internal dataset\nexpressions in the internal and Expresso datasets,\nspeech, whereas the internal and Expresso datasets\nbined dataset using same-speaker delta pair sam-\n\n\n=== RESULTS ===\nhighly competitive performance on naturalness and\nperformance in selective speech editing. Our main\nspeaker-consistent style mining. The resulting Lib-\nand the resulting distribution of emotions in LibriEdit.\nTable 1 summarizes the objective results, showing\nzero-shot TTS performance in the upper section\nperformance on the LibriSpeech test clean set, with\nplings per utterance and reporting the final result by\nFigure 4: Emotion editing performance on the easy task.\nports the results for Step-Audio-EditX iterative\ning the best performance across all metrics except\nTable 1: Overall objective performance comparison including zero-shot TTS results on the LibriSpeech test-clean\nset and emotion editing results under different task settings, with the best-performing values highlighted in bold and\nFigure 6: Result of a CMOS-style subjective test on\nFigure 7: Result of a CMOS-style subjective test on\nfurther enhance emotion editing performance.\npitch and energy manipulations result in slightly\nDespite the promising results, SpeechEdit has sev-\nresults. Development, 10(9,290):4–54.\nemotion representation. In Findings of the Associa-\nReturn the result strictly as a JSON object with two\ntion results across three tasks. Contrary to expec-\nresults suggest that naive emotional data augmen-\n\n\n=== CONCLUSION ===\nConclusion",
  "2601.05699": "=== METHOD ===\nBenchmarking large language models (LLMs)\napproaches, culturally grounded pretraining,\nmodal large language models (MLLMs) particu-\net al. (2021b) tested models on verifying statements\nmarks is that they query models only through text,\na consistent finding is that models perform poorly\non African languages, with open-weight models\nModels:\nlected models based on two criteria: a) support\nmodel sizes within each family to assess scaling\nclosed-source models, we include Gemini-2.5 Pro\nevaluation, models receive the written version of\nparison shows whether model performance gen-\nguages affect models’ performance and whether\nEnglish. This setup allows us to compare model\nfects model performance (RQ4), we evaluate mod-\nsame prompt templates across models and lan-\nWe evaluated all settings and models\nWe evaluate models on visually-grounded cultural\ntasks, models are provided with an image and must\ntwo evaluation formats: (1) MC-VQA: Models are\nended VQA: Models receive an image and a text\nmodels are queried through African-accented En-\nspeech format without answer options and models\nimate measure of the models’ baseline linguistic\nassessing whether models can accurately capture\nthe model’s ability to recognize spoken languages.\nWe evaluate models in a zero-shot setting using\nperforming model per family.\nspeakers rated whether model outputs matched the\nOpen-weight models consistently perform better\nand Open-ended QA, where all models, including\nFigure 3: Performance comparison of models on text-based question answering tasks: (a) Text MC-VQA (Multiple\nFigure 4: Performance comparison of models on audio-based question answering tasks: (a) Audio MC-VQA\nWe also observe that increasing model size\namong open-weight models does not necessarily\nguage QA, indicating that model scaling alone is\ntably, some smaller models achieve near-zero ac-\nweight models across both tasks while maintain-\ntween proprietary and open-weight models. Hu-\nweight models achieve higher performance when\nacross both tasks. For open-weight models, au-\n(Section 6.2.2). Open-weight models demonstrate\nperformance models in downstream tasks, such as\nfind that scaling up model size shows little improve-\nVQA. Accuracy across best-performing models for En-\nglish and Native. We observed that, while most models\nof the tested models and provide pointers on why\nmodels fail on Afri-MCQA.\ndition to Afri-MCQA. All models showed per-\nmodels show a big drop, with Qwen variants\nopen-weight models show considerably higher\nscores, suggesting that while models possess gen-\ncross-lingual gaps, particularly for Qwen models,\nmodels. In contrast, correlations between Afri-\nplay a large role in explaining why models fail in\nModel\nshow moderate performance. Qwen models per-\ntive ASR, whereas Gemma models exhibit substan-\ntial degradation. Qwen models produce high error\nmodels score near zero. In light of these results,\nit is likely that models fail on open-ended VQA\ncome from errors at each step: (1) Open models\nmodels receive wrong transcriptions, as they cannot\nserved across models, modalities, and languages.\nand 4, even the best performing model (Gemini-2.5-\nOpen-VQA (text-based, English). Smaller models\nparticularly for smaller models. As shown in Fig-\nures 3 and 4, on MC-VQA, Qwen models drop by\napproximately 1–2% while Gemma models show\nfor non-Gemini models).\nform native language queries across all models and\ning is the dominant limitation. However, models\nFigures 3 and 4, models perform significantly bet-\n(a 40% gap). Smaller models show even larger rel-\nModels struggle to generate culturally grounded\ntions: (1) speech-first approaches: Many African\nweight models lack basic LID and ASR capabili-\nalone is insufficient; models need explicit exposure\ncultural transfer: Models may “know” cultural\nited set of open- and closed-source models, so the\nlanguage models. In Proceedings of the 2025 Con-\nof the 2025 Conference on Empirical Methods in\nConference on Empirical Methods in Natural Lan-\nlanguage models for cultural understanding. arXiv\ngood are large language models on african languages?\nlarge multimodal models across multilingual and\nmodels for ethiopian languages with task evaluation.\nInkubalm: A small language model for low-resource\nFor each QA format, we use the same prompt templates to ensure consistency across models and languages.\nWe evaluated models under two distinct prompt conditions to assess the impact of visual and contextual\n- Geography, Buildings, Landmarks: Popular/common landmarks, local architecture/buildings. \n●​ Verification Method: Follow along with text while listening \n\n\n=== EXPERIMENT ===\ntence, we include control experiments meant\n1https://huggingface.co/datasets/Atnafu/Afri-MCQA\nAlthough most evaluation datasets for low-\nevaluations are designed for text-only tasks. In\nmultilingual cultural VQA dataset supporting\nThe dataset consists of ≈7.5k Q&A pairs (in En-\nevaluate multiple MLLMs across various setups to\ncultural and multilingual multimodal evaluation\nDespite this progress, all of these evaluations\ndatasets (Romero et al., 2024; Vayani et al., 2025b)\nglish audio, and (3) diagnostic control experiments\nDataset\nDatasets\nTable 1: Data statistics for Afri-MCQA compared to existing VQA datasets that include African languages.\nDataset collection\nfinal dataset.\nDataset Composition\nFigure 2: Image categories in our dataset and their dis-\nExperimental setup\nOur experimental design is organized to address\nmodal evaluation, and (b) availability of different\nthe question. For audio evaluation, we use na-\nEvaluation\nThis section describes our two evaluation setups\nCultural VQA Evaluation\nControl Experiments\nevaluations provide evidence to understand the ex-\nText-based experiments:\nAudio-based experiments:\nEvaluation metrics\nhuman evaluation for open-ended VQA (text).\nAutomatic Evaluation: We report accuracy\nHuman Evaluation:\nevaluation type, beginning with cultural VQA per-\ntasks, followed by control experiments.\nWe show evaluations on MC-VQA and Open-ended\nman evaluations on Open-Ended VQA (in Fig-\nthe trends observed in automatic evaluations, with\nlanguages. Similar to text-based evaluation, open-\nOpen-ended VQA. Similar to text evaluations, we\nFigure 5: Human Evaluation for Text Open-ended\nResults for Control Experiments\nWe present the results on control experiments on\nText-based Experiments\nels on the control-experiment benchmarks in ad-\ntrol datasets (see Appendix D). We observe strong\nAudio-based Experiments\nFigure 6 shows three evaluations on native African\nfor meaningful evaluation of cultural reasoning in\nels. Control experiments for audio show that this\non MC-VQA). Control experiments on AfriXNLI\ntext and speech modalities. Our evaluation shows:\nstep toward more inclusive evaluation by fore-\nmarks. Although the dataset spans 15 languages\nmost human-curated datasets, potential biases in\nhuman-curated dataset created without the involve-\ntime-consuming, the dataset is of moderate size and\nintended as an evaluation benchmark rather than\nthe intended use of the dataset for research and\nevaluation purposes, and their right to withdraw\nthat our dataset may still reflect certain biases or\nA simple, inclusive, and big evaluation dataset for\nLLM evaluation challenges for low-resource lan-\ngenerative datasets. In Findings of the Association\ndatasets for African languages. In Proceedings of\nbah Adamu Kakudi. 2023. Havqa: A dataset for\nautomatic mt evaluation. In Proceedings of the tenth\nand linguistic biases in multilingual evaluation. arXiv\nResources and Evaluation (LREC-COLING 2024),\ndetection and slot-filling dataset for 16 African lan-\nFigures 9 & 10 show the location-aware prompts we used in both setups for audio and text modalities.\nAdditional Experiments results\nThe following shows detailed requirements and rules for dataset creation when annotating the \ndataset.  \nAfri-MC-VQA Dataset Review Guidelines \nconsistency across all submissions for the Afri-MC-VQA dataset. Reviewe",
  "2601.02444": "=== METHOD ===\nExisting purification and denoising approaches, mostly de-\noutperform ASR-oriented denoising methods, these approaches\npurification model that learns a latent mapping from perturbed\ndiffusion models that rely on transcripts or external language\nprompts, our approach operates entirely in the acoustic domain\n(TTS) and voice conversion (VC) models [1]. These technolo-\nmethods to detect and mitigate malicious uses of modern\nproactive voice protection methods that make a user’s speech\nunlearnable for synthesis models. The key idea is to introduce\nsynthesis models to accurately mimic the identity of a speaker,\nquestion of whether current methods can remove protective\nof existing purification approaches and the need for more ro-\nVocalBridge is a diffusion-bridge purification model that\nresentations within a compact encoded domain. The model\nemploys a time-conditioned 1D U-Net denoiser that models\nmethods in both attack effectiveness and performance\nincorporates recent synthesis models, multiple defense\npurification model that operates in the speech latent space\nligibility compared to recent purification methods, while\nto these, modern deep learning approaches can generate re-\nan acoustic model converts the input text into a low-level\nboth stages is what allows these models to capture the complex\nThis is achieved by training or fine-tuning a TTS model on a\nfrom these samples, the model captures the unique vocal\npatterns, and accent [17]–[19]. This allows the trained model\ndata level, these techniques prevent a generative model from\nlearned by TTS synthesis models. In their follow-up work,\nversarial noise added to the input of VC models, an important\nprotection. Adversarial purification methods operate directly\nmodel itself, purification does not require access to or control\nover the target VC model. This makes purification an appeal-\nmust defend against external VC models without altering them.\nMoreover, purification methods can generalize to unseen or\nRecent research shows that generative model-based de-\nnoising approaches, especially diffusion models, are highly\nDDPM [34] based diffusion model to denoise it and restore\nas ASR models often focus on the low frequency components\npre-trained guided diffusion model. Tan et al. [37] employ\na one-shot unconditional Mel spectrogram diffusion model\ndomain signal through interpolation methods, then the sam-\nples’ Mel spectrograms are purified with a diffusion model.\nadded to compromise ASR models, Fan et al. [38] propose\na two-step method to remove protective perturbations whose\ngoal is to make speech un-learnable by generative models.\nunconditional pre-trained diffusion model. In the second step,\nan Ornstein–Uhlenbeck SDE–based refinement model [39],\nthe limited robustness of purification methods based on sim-\nple pre-trained diffusion models. Specifically, they evaluate\napproaches [47]. To address these limitations, Li et al. propose\nADBM, an approach that constructs a reverse bridge from the\ndistribution. Their method assumes that adversarial noise ϵa\nduring training. The model is trained to predict the original\nIII. THREAT MODEL AND SECURITY OBJECTIVES\nWe consider the threat model shown in Fig. 1, where a user\nFig. 1: This figure illustrates the threat model, demonstrating\nor prompt few-shot or zero-shot TTS/VC models.\nmodel Gθ (parameterized by θ) to produce the imitation\nIV. AUDIO DIFFUSION BRIDGE MODEL\ndio Diffusion-Bridge Model, named VocalBridge, a purification\nWe build on the Adversarial Diffusion-Bridge Model\non a more compact representation and enables the model\nThe model is a diffusion-based purification framework de-\ncreases with smaller t, enabling the model to progressively\nenabling the model to better preserve speech content while\ntruth transcripts, our approach operates purely on acoustic\nprior rather than a strict constraint, making the method robust\nD. Network Architecture\nThe purifier network adopts a 1D U-Net architecture with\nfor model training and purification. For lighter inference\nrification method evaluated, we generate 4,526 purified test\nbased voice defense methods aim to protect speech data by\nTTS and VC models. In this work, we select a representative\nEffectiveness: each method is explicitly designed to degrade\nfense). (2) Availability: We require that each defense method\nprovides publicly released model checkpoints along with the\npublicly available perturbation-based speech defense methods\nlearning in TTS models. Its central mechanism, Speech\nerative model to guide the creation of perturbations that,\nthe protected data during model training. Rather than\nclear, voice conversion models cannot accurately clone\nmodels; accordingly, we use the embedding attack in our\nmethod that adds small imperceptible adversarial pertur-\nthe data unlearnable for TTS voice cloning models. The\nloss is shared across nearly all TTS models, the approach\narchitectures. They report that POP generates protected\nTTS models are trained on this protected data, the result-\ntwo optimization strategies: a threshold-based method\noriginal, and a target-based method that moves embed-\nity to unknown TTS models, it optimizes perturbations\nmain using a generator-discriminator architecture (GAN),\nVC model is used during training to provide gradient\nstrate high performance and collectively cover methods that\nvarying degrees of transferability across synthesis models.\nturbations guided by a surrogate generative model. Attack-VC\nVC models for synthesizing/cloning voice, each representing\nspeech synthesis. We select six representative models\nmodel that predicts discrete acoustic tokens conditioned on\nsive transformer-based acoustic model with a diffusion\n• StyleTTS2: StyleTTS2 [54] is a text-to-speech model\nlarge speech language models (SLMs) to generate natural-\nsounding audio. It models speaking style as a latent random\nmodel that leverages vector quantization for content encod-\ntional autoencoder integrating text-to-semantic-unit model-\ning two diffusion models, DiffPitch and DiffVoice, for se-\nusing pretrained models from SpeechBrain [58] and Resem-\npredicted by the NISQA model [60]. NISQA estimates speech\nnunciation clarity. A pre-trained Whisper-small model [46] is\nremovers that represent a diverse range of recent approaches.\n(1) Purification Stage : Uses a diffusion-based model to\nASR systems. It uses a hierarchical diffusion model that\ntiple ASR models and attacks, WavePurifier outperforms\nsystems using off-the-shelf diffusion models. It uses\ndiffusion models to generate noise, which is added to\nrecover the clean audio. It is a plug-and-play method,\ndefense method against adversarial perturbations. First,\nditional diffusion model is used to purify the features\nmodel including Purification and Refinement), AudioPure,\nthreat model. Nevertheless, we include them as baselines to\nTTS and VC models from purified datasets across the three\nover the best existing method (DualPure at 37.4%) by a margin\nThe only setting where our models do not lead is AntiFake,\nwhere the specialized De-AntiFake method remains higher\nfunction without any privileged model access, making them\ning methods, effectively removing protective perturbations and\nnisms, ASV backends, and synthesis architectures.\npurification framework, we use a Mono model that is trained\nAdaptive model across all protection types.\nTABLE I: Authentication Restoration Rate (%) for selected TTS models.\nTTS models\nTABLE II: Authentication Restoration Rate (%) for selected VC models.\nVC models\nbility and helps the model preserve speaker-relevant phonetic\nW improves encoder-averaged ARR of VC models from\naverage MOS values ranging from 2.95 to 3.27. Our method\noutperforming all prior purification approaches. Fig. 6 reports\noutperforming all competing approaches.\ndistortions present in baseline methods, allowing attacker-\nused VC/TTS models to replicate speaker characteristics more\ntion model, including its gradients, and can therefore optimize\nmethodology used in D",
  "2601.02914": "=== METHOD ===\nnerabilities: 1) modern voice cloning models\nize across different methods of audio synthesis,\ncloning (deepfake) models, which can synthe-\nities in state-of-the-art models.\nupdated training corpora and architectures that\nModel\nhours. Commercial models, by contrast, require\nclassifiers, and end-to-end models that operate di-\napproaches typically use LFCC, MFCC, or CQCC\nto-end models exploit raw waveform representa-\net al. (2025) proposed post-training SSL models\narchitecture with spectral features to improve de-\ngeneralization, where models trained on one cor-\nification models, anti-spoofing detectors, and di-\nverse speech synthesis approaches1.\nSpeaker Verification Model\nThe model uses a time delay neural network\nDeepfake Detection Model\nart architecture combining XLS-R (Zhang et al.,\nsentation model pretrained on 436k hours of multi-\nSynthesis Model\nduring model training.\nverification model, revealing a key vulnerability:\nexceeds 0.55, approaching the typical range of\nModel\nTable 4: Comparison of deepfake detection models with\ncan train the required models in less than 2 hours.\ndeepfake models, XLS-R + AASIST achieves an\ntional methods and demonstrating its potential as\nwide range of deepfake models, leading to out-of-\nated by models with synthesis patterns unseen dur-\nSynthesis Models\nmodels3. These systems span diverse synthesis\nand prompt-conditioned architectures, enabling an\nable features rather than model-specific signatures.\n3See Appendix C for the full list of models.\nfrom in-domain to out-of-domain (model variation) and\nmethods, particularly diffusion-based and prompt-\nand flow-based architectures, exposing fea-\nseen synthesis methods.\nsynthesis methods absent from public training cor-\nNew architectures emerge monthly, while retrain-\ncorpora, but, most importantly, model architectures\ntection model on two large-scale English datasets,\nMandarin-trained models to English deepfakes and\nto 16.24%, highlighting the model’s high sensitiv-\ndeepfake models trained on very small datasets,\nand the deepfake detection model, despite strong\nmands a fundamental shift in how we approach au-\non deepfake models trained with a small\nison of modern deep learning models for speaker\ncodec and diffusion models. In Proceedings of the\ntional and contemporary approaches used in text to\ntwo-stage conditional diffusion probabilistic model\n2: Latent diffusion models are natural and zero-shot\nbased architecture. Knowledge-Based Systems, page\nA raw data boosting and augmentation method ap-\ntrained model and code will be publicly released to\nBaseline Deepfake Detection Models\nline deepfake detection models spanning the evo-\nthe state-of-the-art XLS-R + AASIST architecture,\nGaussian Mixture Models (LFCC + GMM) pro-\ncrafted features and generative modeling, widely\nan end-to-end model operating on raw waveforms\nextends RawNet2 by jointly modeling spectro-\nited generalization to unseen attacking models, a\nDeepfake Model\nstage conditional diffusion model for binaural\nend TTS model achieving human-level quality\nand expressivity by jointly modeling the entire\ndiffusion model, improving zero-shot quality\nPromptTTS with prompt-driven style model-\nThese models collectively span diverse synthe-\nand prompt-conditioned approaches, ensuring that\n\n\n=== EXPERIMENT ===\nsis dataset, revealing two major security vul-\n• We present the first systematic evaluation\nberation (Tak et al., 2022), as well as cross-dataset\nsystematic evaluation across diverse synthesis ar-\nExperiment Setup\nCeleb (Nagrani et al., 2017), a large-scale dataset\nBenchmark Dataset\nmale) from the AISHELL-3 dataset in Chinese\n1Code and dataset will be released upon acceptance.\nTable 2: Overview of the benchmark dataset, with total\nTable 2). The dataset is split by speaker: 30 for\nEvaluation Metric\ning training. This necessitates further evaluations\nincremental dataset expansion cannot solve. Since\nCross-Lingual Evaluation\ndataset achieves 1.53% EER. These results con-\nThis paper presents a systematic evaluation of\nDespite the comprehensive evaluation presented in\n• Training data scale: Our evaluation focuses\n• Complexity of cross-lingual evaluation:\nthese experiments highlight the promise of\ntification dataset. In Interspeech 2017, pages 2616–\nsupport efficient and reproducible evaluation.\nclosely reflects a deployable real-world setup, en-\nout-of-domain evaluation captures realistic and\n\n\n=== RESULTS ===\nperformance and real-world robustness. These\nfindings call for a reconsideration of security\nsecurity level and robustness. Our results reveal\nfindings are amplified by the rapid evolution of\npus often suffer significant performance drops on\nproven performance on various benchmarks (Yam-\nget speaker. For deepfake detection, performance is\nperfect performance.\nResults and Discussions\nterances. Our findings reveal an alarming insight:\nTogether, these results expose a serious and action-\nble 4). However, we argue that such performance\nstudies that result in the misalignment between per-\nPerformance Gap\nTable 5: Performance degradation of deepfake detection\nAs shown in Table 5, a 30× performance degra-\nfacts, detector performance collapses. Manual in-\nTable 5 shows modest performance degradation,\nFinally, we evaluate performance under environ-\ntunately, negative results that expose critical vulner-\nin-domain performance, fails to generalize to un-\nResults show minimal performance variance (EER\nperformance. Below, we provide further details\ntations, achieving strong performance on the in-\n\n\n=== CONCLUSION ===\nConclusion",
  "2601.15621": "=== METHOD ===\nhttps://modelscope.cn/collections/Qwen/Qwen3-TTS\ncontrollable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-\ndual-track LM architecture for real-time synthesis, coupled with two speech tokenizers:\nboth tokenizers and models under the Apache 2.0 license.\nFigure 1: Qwen3-TTS is a multilingual, controllable, robust, and streaming text-to-speech model. Based\nto AGI. Modern neural text-to-speech (TTS) models, trained on large-scale datasets, already deliver\net al., 2022; Zeghidour et al., 2022; Kumar et al., 2023) combined with autoregressive language modeling\nTable 1: Overview of the Qwen3-TTS model family.\nModel Name\nQwen3-TTS, the first text-to-speech model in the Qwen series. Qwen3-TTS exhibits the following\nnatural and expressive speech. Our 1.7B model, in particular, delivers state-of-the-art, human-like\nquality, demonstrating our approach successfully maximizes perceptual quality without overfitting\nto ASR-related metrics. 4) Multilinguality: The model is trained across more than 10 languages and\nfor our model to integrate seamlessly with Large Language Models (LLMs) and achieve extremely\narchitecture and introduce two tokenizers in the Qwen3-TTS family: 1) Qwen-TTS-Tokenizer-25Hz employs\nmodeling and leads to long-horizon error accumulation. To balance these factors, Qwen-TTS-Tokenizer-\nmodels (Du et al., 2024b; Zhang et al., 2025a). To further support ultra–low-latency streaming, we designed\na dual-track autoregressive architecture for streaming text input and audio output. This architecture\nincorporates a Multi-Token Prediction (MTP) module to effectively model the multi-codebook sequence,\nGPT-4o-mini-tts in target speaker manipulation. Furthermore, the model exhibits remarkable stability\nfacilitate community research and development, we release the complete family of Qwen3-TTS models\nstage (Stage 2), we fine-tune the entire model by incorporating a convolution-based mel-spectrogram\narchitecture (Défossez et al., 2024), speech is decomposed into two discrete code sequences: a semantic\ncodebook capturing high-level semantic content and an acoustic codebook modeling acoustic detail,\nMethod\nArchitectures\nacoustic tokens along the channel axis. Upon receiving a textual token, the model immediately predicts\nthat these adjustments enhance the model’s ability to process extended and complex inputs and\net al., 2023) to align model outputs with human preferences. Specifically, we construct preference pairs\nmodel’s capabilities and stability across tasks. Finally, we introduce lightweight speaker fine-tuning on\nthe base model, enabling Qwen3-TTS to adopt specific voices while further improving the naturalness,\nprosody. For voice design, built upon the Qwen3 text model foundation, Qwen3-TTS inherits robust\nlanguage model (LM) time to first token group for the first speech packet, and (ii) the tokenizer decoding\nModel\nuntil sufficient future tokens are available. With a chunk size of 8 set in Qwen3-TTS, the model must\nor near-lowest WER across multiple datasets. In the S2 stage, where the model is further fine-tuned to\nModel\n2,620 utterances. To enable fair comparison across models, we report key configuration parameters,\nverification model. We compare Qwen-TTS-Tokenizer-12Hz against prior semantic-aware methods,\nefficiency underscores the advanced capabilities of our method in speech representation learning and\nModel\n• Zero-Shot Speech Generation: We evaluate the model’s ability to clone unseen voices by\n• Cross-Lingual Speech Generation: We investigate the model’s capacity for cross-lingual voice\n• Controllable Speech Generation: We verify the effectiveness of models’ instruction-following\nfine-tuned (SFT) model variants on the multilingual test set (Zhang et al., 2025a), focusing on\nmodel to better model long-term dependencies for stable speech generation.3): Scaling the model size\nModel\nadopting the ChatML format, Qwen3-TTS treats voice control as a language modeling task, allowing for\n(Creation): In this scenario, the model generates novel voices based on text descriptions. As shown\nin Table 8, Qwen3-TTS-12Hz-1.7B-VD establishes a new state-of-the-art among open-source models.\nNotably, it outperforms commercial systems like Hume and specialized models like VoiceSculptor in\nModel\nboth Chinese and English, with lengths varying from 200 to 2000 words. Following the methodology of\nModel\nspeech models designed for real-time speech synthesis. Through a novel dual-track design two types\nExtensive evaluations confirm that our models achieve state-of-the-art performance across a wide spec-\nstability issues in autoregressive models, enabling the seamless generation of over 10 minutes of fluent\nthis architecture to support versatile audio generation, further scale our multilingual coverage beyond\nthe current 10 languages, and explore more granular stylistic controls. By open-sourcing both the models\nmodels. arXiv preprint arXiv:2406.02430, 2024.\nGrave, and Neil Zeghidour. Moshi: a speech-text foundation model for real-time dialogue. arXiv\nlanguage models. arXiv preprint arXiv:2412.10117, 2024b.\nand diffusion models. arXiv preprint arXiv:2403.03100, 2024.\nspeech: Leveraging large language models for advanced multilingual text-to-speech synthesis, 2024.\nfor generative modeling. In ICLR 2023.\nDirect preference optimization: Your language model is secretly a reward model. In NeurIPS, 2023.\nNaturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing synthesizers.\nHuaming Wang, Jinyu Li, et al. Neural codec language models are zero-shot text to speech synthesizers.\nZheng, Rui Wang, Xiaoqin Feng, et al. Spark-tts: An efficient llm-based text-to-speech model with\nSheng Zhao, Jiang Bian, Xixin Wu, et al. Uniaudio: An audio foundation model toward universal audio\ncodec for audio language model. In AAAI-25, Sponsored by the Association for the Advancement of Artificial\nGuo, Weiji Zhuang, et al. Mimo-audio: Audio language models are few-shot learners. arXiv preprint\ntokenizer for speech large language models. CoRR, abs/2308.16692, 2023a.\ntokenizer for speech large language models. arXiv preprint arXiv:2308.16692, 2023b.\nlanguage modelling. In Jianfei Cai, Mohan S. Kankanhalli, Balakrishnan Prabhakaran, Susanne Boll,\n\n\n=== EXPERIMENT ===\nConvNet. Extensive experiments indicate state-of-the-art performance across diverse\nfrom 8,192 to 32,768 and upsample long speech in the training data. Experimental results indicate\ntable are computed based on the above setup.\nExperiments\nWe conduct a comprehensive evaluation of Qwen3-TTS. The evaluation is divided into two main cate-\nEvaluation of Speech Tokenizer\nquality is assessed using Short-Time Objective Intelligibility (STOI), Perceptual Evaluation of Speech\nIn this section, we conduct a comprehensive evaluation of the speech generation capabilities of Qwen3-\nTTS. To ensure a robust assessment across diverse scenarios, we categorize our experiments as follows:\ntecture, we evaluate content consistency on an internal dataset consisting of generated speech\nEvaluation of Zero-Shot Speech Generation\nDatasets\nEvaluation of Multilingual Speech Generation\nEvaluation of Cross-Lingual Speech Generation\nEvaluation of Controllable Speech Generation\nnuanced manipulation of speech attributes. The evaluation covers two distinct scenarios: Voice Design\nEvaluation of Target-Speaker Speech Generation\nEvaluation of Long Speech Generation\nprosodic discontinuity. We evaluate this capability on a curated internal dataset comprising 100 texts in\nDatasets\n\n\n=== RESULTS ===\nresulting in outputs of greater richness and diversity (Du et al., 2024b; Lyth & King, 2024; Zhou et al.,\nTrained on over 5 million hours of speech data, Qwen3-TTS achieves impressive performance across\nspeech token through a linear head. The resulting sequence is then processed by a chunk-wise DiT\ncompetitive performance. Qwen-TTS-Tokenizer-25Hz in the S1 stage (trained with ASR supervision)\nachieves ASR performance c",
  "2601.13802": "=== METHOD ===\nparticularly from a unified modeling perspec-\ninformed curriculum learning. Our approach\ncommitted to open-sourcing the model, along\nFigure 1: Our open-source unified-dialectal model\nto few models for specific or limited dialect sup-\npected as achieved by recent zero-shot TTS models\nunderwhelming performance of these models is un-\nit unsuitable for training high-quality TTS models.\nspeech model covering to date more than 20 lan-\ncommercial model on all 6 major dialect test sets.\neral to dialect-aware training, our model enables\ntraining a single, unified model versus separate,\nspecialized models for each dialect. We further\nspecific ASR models for more comprehensive\nMethodology\nthat contribute to the effectiveness of our model in\nduring transmission. This approach, while sim-\nsource separation model (Luo and Yu, 2023) for\nof this approach involves a nuanced trade-off, as\nficient quality and reliably support the model to\nscale model training and inference. As part of\ntialize the model with weights from the F5-TTS\nmodel (Chen et al., 2025b), which is pre-trained on\nfavors the model to grasp fundamental grammat-\nsuite of models through two parallel strategies, de-\ntailed in the ablation studies: specialized models re-\nand unified models, enhanced by fine-tuning on all\nstep learning approach, our model demonstrates\nCurrently, mainstream speech synthesis models ex-\nmodels can implicitly retrieve and effectively lever-\nproves the model’s inference ability overall—even\nthe curriculum-trained base model inherently pos-\nmodel better understand and internalize certain\nof our open-source Habibi model suite, with full\nkey factors underpinning the model’s efficacy.\nmodels (Gao et al., 2023; Le et al., 2023; Eskimez\nraw text character sequences. This approach is not\ntrain all models to 200K updates, where a single\ning ASR models, speaker similarity (SIM) leverag-\ning the speaker verification model WavLM (Chen\nASR-LLM-7B model (Omnilingual et al., 2025)\nderived from dialect-specific ASR models, most\ntuned models on Hugging Face6,7 are employed).\nmodel risks cross-dialect recognition bias (e.g., in-\ntures); conversely, specialized models often suffer\nTable 3: Comparing unified models trained on D1 (Uni.D1) and on D2 (Uni.D2) with specialized (Special.)\nmodels across different training updates (Upd.).\nondary validation using a large language model to\nResults of Dialect-Specific Models\nSpecialized dialect models are trained on D1 (in\nTable 2). For SAU-specialized model, we use the\ncialized models reasonably outperform the ground\nriority of the model design and the fact that GT\nwhich challenge ASR models. In the subsequent\nResults of Unified-Dialectal Models\nAccording to Table 3, our unified dialect model\nmodels. This result represents a significant break-\neral training approaches: training from scratch, di-\nbase model, and our proposed two-stage method,\nEnglish pretrained model initialization, first-stage MSA-\ngual base model enables successful convergence.\nCrucially, our curriculum approach—first learn-\nmodeling perspective. Even when compared with\na model receiving twice the training updates but\nTable 5), our curriculum approach demonstrates\nof our model, we conducted an ablation study in\nior suggests that the model depends critically on\nduring Saudi-specialized model training. Evaluated on\nenhancement model to the high-noise portion of our\nto train the Saudi-specialized TTS model. Specifi-\nmixed-sampling approach with a fixed probabil-\nTable 8: Comprehensive comparison between unified dialectal models trained with or without regional identifiers\nMoroccan-Specialized Model\nEgyptian-Specialized Model\nUnified Model (avg. score)\nsample length is beneficial to the TTS model’s per-\ngional identifiers facilitates the model’s learning\nmodel exhibits strong robustness to different infer-\nO reflects the model’s generalization capability to\ncally, our unified model attains performance compa-\nmance. Finally, we release the model checkpoints,\ndation for future research on unified dialect model-\nmodeling setting, more effective checkpoint fusion\nacross dialects. Additionally, the current model\nfied modeling shows promising results, the trade-\nand computational cost, as well as the model’s ca-\nstrain model performance. Incorporating more data\ndesign, without exploring more tailored model\nmodeling. Finally, this study does not analyze neu-\nral model behaviors to understand how linguistic\ndistinctions can be observed from the model.\nveloped from open-source model weights, aiming\ngarding a unified dialectal modeling of the Arabic\ntication models should be directly applicable or\ntransferred and applied to our model. Additionally,\nquality versatile speech generation models. arXiv\nsion transformer autoregressive modeling for speech\ntinuous audio language models.\nlanguage models are zero-shot text to speech synthe-\nTTS: An efficient llm-based text-to-speech model\nwith generative speech models.\nv3 (alpha) with our models.\nvice, we use these prompt pairs during our models’\nElevenLabs models, voice IDs already available as\nand our models are exhibited in Table 11.\nModel\nmodel suite: the dialect-specialized models (Special.)\nand two unified models trained on D1 and D2 with\nsource models surpass the proprietary ElevenLabs\nin gray to indicate a methodological caveat:\ntribution on model performance. The MSA and\noutperform unified models trained on heteroge-\nmodels leverage cross-dialect training to gener-\non MSA and EGY. In contrast, specialized models\npha) model shows markedly lower SIM scores,\nAdditional Ablations of ASR Models\nOmnilingual-ASR Model Suites\nOmnilingual-ASR-LLM-7B v1 model with a fixed\nASR-LLM-7B model in Table 12.\nASR-LLM-7B model on Ground Truth (GT) samples.\nAs shown by the results, the updated v2 model\nPerformance of Moroccan ASR Models\nBoumehdi’s ASR model (Boum.)10 outperforms\nOmnilingual-ASR-LLM-7B v1 model is abbrevi-\nmodels, evaluating MAR-specialized model across differ-\n\n\n=== EXPERIMENT ===\ndata, benchmarks, and evaluation guidelines,\nand establishing evaluation standards for the\nmajority of the datasets are designed and curated\nsamples. Almost no ready-made clean datasets\nbic speech datasets. For instance, ArVoice (Toyin\nand reliable evaluation.\ntraining dataset, while Section 2.3 describes how\nularity. We emphasize that the terminology setup\nDataset Name\nTable 1: Training dataset statistics. †: expanded data for\nisting open-source ASR datasets:\nable datasets: UAE-100K, UAE-Nexdata, Darija-\nFor all datasets, we apply the speaking rate\nunfiltered by the original datasets or from unusu-\ndataset. The guiding principle is consistent: the\nestimated corruption level of a dataset dictates the\nTo tackle low-SNR datasets, we employed a\nThe statistics of our final usable dataset are pre-\nA systematic and standardized evaluation bench-\nstandard zero-shot TTS evaluation as it needs both\nfor evaluation.\nExperiments\nwe detail the experimental setup, present our main\nBackbone Choice and Training Setup\nfollow the default setup of F5-TTS v1 base training.\nEvaluation Setup\nAll experiments adhere to the benchmarking stan-\nfor comparative evaluation. If no suitable voice\nexperiments, we selected checkpoints with the low-\nablation study on SADA, which is the dataset used\ncorresponding denoised version; cleaner datasets,\nlength diversity, and overall dataset size (Table 9).\na primary evaluation metric in the current stage of\nmore comprehensive evaluation perspectives, as\ning, evaluation, and robust deployment within the\nwe emphasize that the terminology setup in this\nSADA: Saudi audio dataset for arabic.\nlearning evaluation of universal representations of\n2025. ArVoice: A multi-speaker dataset for arabic\nEvaluation Details in Comparison with\nWe elaborate on the evaluation details in this sec-\nTable 11: Evaluation results of ElevenLabs’ Eleven v3\nzero-shot TTS evaluation prohibitively expensive\n\n\n=== RESULTS ===\nand pronunciation conventions, resulting in rela-\nthe resu",
  "2601.23255": "=== METHOD ===\nAudio–Language Models\nLarge audio-language models increasingly op-\nfollowing text-to-speech (TTS) model to ex-\nRecent advances in large language models (LLMs)\naudio-enabled models such as GPT-4o and Gem-\nmodels (LALMs) fall largely into two categories.\nmethods, either by optimizing directly over audio\nrestricted outputs from highly aligned models. Our\npathy, narrative pacing, our method leverages the\ndelivery, successfully induces the model to produce dan-\nWe evaluate our method on three state-of-the-art\nderscoring the importance of modeling content,\nLarge Audio-Language Models\nLarge audio-language models (Zhang et al., 2024;\nmodels by incorporating audio inputs, enabling spo-\nWhile effective, this approach discards non-textual\non end-to-end large audio–language models that\naudio–language models do not treat speech as a\nsignal that systematically influences model behav-\nto divergent model responses even when lexical\ncally influence model outputs in both ASR-LALM\nrestricted outputs. These methods generally fall\nWhite-box methods exploit model gradients or\nHuang et al., 2023). Black-box approaches in-\nstead rely on model feedback, using iterative self-\nWhile these methods remain effective for purely\nto manipulate model compliance.\ndio language models are susceptible to jailbreaks\nsocially conditioned responses from models.\nthat elicit specific model behaviors. This perspec-\nMethod\nThreat Model\naudio-language model solely through audio input.\nerated spoken inputs to bias the model’s internal\nthat contravene the model’s alignment policy, such\nmodel’s output y = M(a) explicitly contains or\ntions that systematically bias the model toward pro-\nmodel’s internal representation of speaker intent,\nto-speech (TTS) model gϕ renders the two into a\nthose most likely to override the model’s alignment\nlow-level signals, one can steer model behavior by\nIntegrative Vocal Affect (CIVA) model (Vaughan-\ncrete control template within the TTS model gϕ,\napproach transforms abstract psychological con-\n2024b) , we evaluated our methods on three bench-\nTo thoroughly evaluate our method,\nRate to assess the effectiveness of our method\njudge model to determine whether the generated\nmodel can be found in Appendix B.\nwhether our approach can enhance both text-based\ndio–language models.\nting without access to model parameters, DeepIn-\nincrementally condition the model toward policy-\nadapts to model behavior via iterative prompt para-\nrefinement for each model–benchmark pair to en-\nModel\nModel\nmethod in modulating model compliance, reveal-\nmethod on the audio-based jailbreak method, we\nated using the GPT-4o Mini TTS model. We main-\nOur method consistently outperforms original Ad-\nvWave across all models and benchmarks. On GPT-\nstruct. For Qwen2.5-Omni, our method attains sub-\nThe advantage stems from our method’s design,\nTTS model to incorporate instruction-style tonal\nmethod advances the design of high-fidelity, behav-\ncues shape model susceptibility, we analyze five\naudio–language models. The results reveal that de-\nlivery style alone can substantially shift model be-\nModel\nModel-wise Observations.\nAveraged across models, psy-\nAcross models, two trends stand out. First, topics\n• Premature termination (43.0%). The model\nModel\nbut in the model’s internal sequence stability when\nparatively compact end-to-end model. Its smaller\nditional 10–20% boost across models, confirming\nModel\nModel\ntematically biases model compliance, even when\nour approach generalizes to human speech, we\nend-to-end audio-language models to delivery style,\nModel\nmodels under this alternative TTS setting. In par-\ntied to a particular TTS model, but instead reflects\nhow large audio-language models interpret and re-\ndio–language models. By translating social and\nalone can bias model compliance, achieving no-\nmust jointly model linguistic content, prosody, and\non Qwen 2.5-Omni, our attack method can be less\ning instability. In such cases, the model tends to\nond, the current approach relies on a small set of\ntrolled accounts and model instances to ensure that\nsecurity research and model evaluation. The intent\nvulnerabilities in large audio–language models, not\nour findings to relevant model developers prior to\n2023. Using large language models to simulate mul-\ntion models for natural interaction between humans\nmodels. Preprint, arXiv:2404.01318.\n2024b. Jailbreaking black box large language models\nend-to-end large audio-language models. Preprint,\nmodels. Preprint, arXiv:2501.13772.\nnot as i do’: A semi-automated approach for jailbreak\nspeech language models: A survey.\nfer in speech-to-speech models. In Proceedings of\nthe 2024 Conference on Empirical Methods in Nat-\nmodel for audio tasks. Preprint, arXiv:2305.11834.\n2024. Gama: A large audio-language model with ad-\nrobust audio jailbreaks in audio-language models.\nA survey of spoken dialogue models.\naudio-language models. Preprint, arXiv:2412.08608.\nFrozen large language models can per-\n2025b. Frozen large language models can perceive\nHypnotize large language model to be jailbreaker.\n2024a. Advancing large language models to capture\nguage modeling of spoken dialogue.\nprompts on aligned large language models. Preprint,\nOpenAI. 2024. Gpt-4o: Openai’s new flagship model.\nOpenAI. 2025. Gpt-4o mini tts (text-to-speech) model.\nOnline model documentation.\nopenai.com/docs/models/gpt-4o-mini-tts.\nFine-tuning aligned language models compromises\nOptimizing speech language models for acoustic con-\non language models in one gpu minute. Preprint,\n2024. Cognitive biases in large language models:\nour new ai model for the agentic era.\naffect in persuasion: The civa model. Journal of\nrelated tasks with large language models. Preprint,\nuating large language model safety refusal. Preprint,\nguage models can hear, talk while thinking in stream-\nteaming audio large multimodal models. Preprint,\nlanguage models via generative comprehension.\nmodel jailbreak evaluation for comprehensive safety\n2024. Defending large language models against jail-\nlanguage models. Preprint, arXiv:2307.15043.\nmodels. In this setting, we remove all adversarial or narrative jailbreak prompts and vary only the\ndelivered with an authoritative demand style. Across all evaluated models, tone-only manipulation yields\nModel\nfilters of modern end-to-end LALMs. The models already exhibit near-zero compliance for direct malicious\nThe following style prompts were provided to the text-to-speech (TTS) model to control vocal delivery\nthey were specifically designed for research on adversarial prompting and model safety evaluation.\nWe employed these datasets solely for academic jailbreak experiments to study model robustness and\n\n\n=== EXPERIMENT ===\nevaluation or search to refine candidate prompts (Li\nExperimental Setups\nDatasets\nmark datasets: Advbench (Zou et al., 2023), Jail-\nEvaluation Metrics\nthority. For our experiments, we adopt the original\nsure fair and consistent evaluation. For the origi-\nThe evaluation covers three representative end-to-\nPrior evaluations rely on synthetic speech gener-\nconduct a small-scale experiment using human-\nas in our synthetic audio experiments. We evalu-\nsynthetic evaluations due to the limited sample size,\nOur primary experiments synthesize audio using\nserved in our main experiments. This suggests that\ncross-accent evaluations are needed to fully assess\nously. All experiments were conducted using con-\nexposed to generated harmful content. The datasets\n2025. Jailbreak-audiobench: In-depth evaluation and\nA survey and mitigation experiments.\nWe conduct an additional control experiment to evaluate whether vocal tone variation alone, without any\n\n\n=== RESULTS ===\ntext-only baselines. These results highlight the\nSun et al., 2024). These findings mirror insights\nlayer with a summary of what their discussion results \ntion(ASR), then passing the result to an LLM.\nfindings, each corresponding to distinct prosodic\nour enhanced version consistently results in higher\nOverall, these findings",
  "2601.16023": "=== METHOD ===\nframework leveraging a multilingual Large Language Model\n(LLM). The architecture integrates a Whisper speech encoder, a\ntency. We further evaluate three projection architectures (Linear,\nmodels, Speech tokenization, Timbre control.\nsystems typically adopt a cascaded architecture composed of\nexplored end-to-end Speech-to-Text (ST) models that directly\nreduce error propagation compared to cascaded architectures,\nWhile this approach enables direct inference from source\ngle model [12]. These approaches initially relied on attention-\nbased sequence-to-sequence architectures, with later work\nDespite these advances, current end-to-end S2ST models often\n• We systematically evaluate three projection architectures:\nmodel checkpoints to support reproducibility and future\nmodels to current large-scale, end-to-end neural architectures.\nmodels based on sequence-to-sequence architectures; and (iii)\nrecent approaches that leverage LLMs for speech understand-\ndirect and LLM-based methods.\ndriven and neural architectures [30]. Early demonstrations by\napproaches. End-to-end speech-to-text (ST) models demon-\narchitectures [44]. Conformer-based encoders later improved\nrobustness by better modeling local speech features [45]. More\nrecently, LLM-based approaches have been introduced into\nbased encoder–decoder architectures could map source speech\nstudies strengthened these models through self-supervised\ntextless modeling approaches replaced continuous spectro-\nmultilingual direct S2ST via unit-to-unit modeling, allowing\ntional autoencoder (VQ-VAE) models are used to reconstruct\nward frameworks built around large language models (LLMs).\nmodels to retain strong semantic reasoning capabilities ac-\nX [17] explored a cross-lingual neural codec language model\ndirect S2ST that jointly models speech understanding, cross-\nunified architecture. We first describe GigaS2ST-1000, a large-\nspeech translation models. Each audio recording is paired\nmodel is trained on large multilingual text corpora, includ-\nmultilingual datasets, the model supports 17 languages and\nfrom speech (left) and a text-to-token LLM-based approach from text (right).\ntokens are derived from self-supervised speech models that\nHowever, self-supervised tokenization methods may exhibit\na supervised semantic tokenization approach is developed to\nconditioning the model on the target text, a speaker embedding\nLarge Language Model\naccent speech, which enables robust modeling of phonetic and\ning modeling capacities: linear, hybrid Conv1D–Linear, and\nissue, we adopt the semantic group modeling strategy from\nthe model is trained using a weighted sum of audio-token and\nmodels and converted into waveforms using a neural vocoder.\ntional flow-matching model to generate mel-spectrograms con-\nLarge ASR model [23] for evaluation.\nmodel. The cascaded pipeline follows the conventional three-\nstage architecture comprising ASR, MT, and TTS. To keep\nthe model computationally affordable, the DS2ST-LM frame-\nPERFORMANCE COMPARISON OF DS2ST-LM AND BASELINE MODELS USING LEXICAL AND SEMANTIC EVALUATION METRICS (BLEU, METEOR,\nModel /\nFor model training, the Whisper-small encoder is employed\nmensionality of the LLM. The Qwen2–0.5B model serves as\nadapt to the speech translation task. The model is optimized\nWhisper-large ASR model.\nWavLM-based [66] speaker verification model to extract dis-\nmodel components and training data. Unlike these cascaded\napproaches, DS2ST-LM jointly trains the speech encoder,\nIMPACT OF DIFFERENT PROJECTION ARCHITECTURES ON THE\nAs LLM-based models may generate meaning-preserving\n2) Effect of Projection Architectures: We further analyze\nthe influence of different projection architectures on model\nThe analysis focuses on learning stability, modeling capacity,\nNevertheless, the Linear projection approach may exhibit\nlimitations for certain tasks, as it does not explicitly model\nthe text-driven approach, the text-to-token LLM may generate\nExperimental results in Table III show that models condi-\nFig. 3. BLEU score comparison of DS2ST-LM with cascaded models across\nbe leveraged to train direct S2ST models with only a modest\ndegradation in performance. This approach substantially re-\nthe extension of direct S2ST models to language pairs where\ncilitates a rigorous assessment of the model’s robustness\nS2ST models designed for speaker-identity retention. The\nFig. 4. COMET score comparison of DS2ST-LM with cascaded models across\nACROSS DIFFERENT MODELS TO EVALUATE TIMBRE PRESERVATION AND\nModels / Speech Quality\n3.54, outperforming all comparison systems and approaching\na zero-shot timbre-control mechanism enables the model to\nperforming prior direct S2ST models in both speaker similarity\nDespite these advances, the proposed approach remains\non scaling to alternative LLM architectures, jointly modeling\n“Neural machine translation: A review of methods, resources, and tools,”\nspeech synthesis: a systematic review of approaches, challenges, and\nsequence models can directly translate foreign speech,” in Interspeech,\nmodel,” in Interspeech, 2019, pp. 1123–1127.\nI. King, “Recent advances in speech language models: A survey,” in\nwith your own voice: Cross-lingual neural codec language modeling,”\ndour, and H. Zen, “Audiopalm: A large language model that can speak\nlanguage models for end-to-end speech translation leveraging synthetic\nmassively multilingual zero-shot text-to-speech model,” in Interspeech,\nmodels,” in ICML, 2023.\nA generative model for raw audio,” CoRR, vol. abs/1609.03499, 2016.\n[52] P. K. Rubenstein et al., “Audiopalm: A large language model that can\n[62] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: A method for\nSIMILARITY (MOS SIM) ACROSS DIFFERENT MODELS ON THE FR–EN\nModels / Speech Quality\nselected samples per model. All evaluations were conducted\nSimilarity. For this evaluation, 20 samples per model are\nproposed DS2ST-LM model is consistent with the objective\nevaluation results and outperforms the existing models in terms\nADEQUACY AND FLUENCY SCORES ACROSS MODELS FOR EVALUATING\nModels / Score\nWhisper-large ASR model.\nEach annotator evaluates 20 samples per model, and the\nModel /\nModel /\n\n\n=== EXPERIMENT ===\nbilingual corpus by extending the GigaST dataset with high-\nachieves higher performance. Extensive experiments demonstrate\nBhasaanuvaad dataset [29]. Finally, we incorporate a timbre-\ndataset, enabling large-scale research on direct S2ST.\n• We release all training recipes, evaluation pipelines, and\nshot prompt setup. Building on these ideas, subsequent work\nscale dataset constructed for direct S2ST training, followed\nThe GigaS2S-10002 dataset is constructed using the Gi-\nmore reliable dataset for large-scale direct S2ST training.\n2https://huggingface.co/datasets/Lalaramarya/GigaS2S-1000\nwith mono-channel audio. The dataset is publicly released on\ndatasets remain limited. In contrast, a large amount of ST data\nS2ST systems. However, ST datasets typically do not include\nIV. EXPERIMENTS AND RESULTS\nIn this section, we describe the datasets and baseline\nsystems used in our study, followed by the training setup,\nevaluation metrics, and a comprehensive evaluation of the ex-\nA. Dataset\nWe utilize four datasets to train and evaluate the DS2ST-LM\nGigaS2ST-1000 dataset developed in this work comprises\nguages; in our experiments, we use the zh–en subsets, totaling\nWe further incorporate the CVSS dataset [28], a multilingual\ndatasets. CVSS includes two variants: CVSS-C, which pro-\nto Indic languages, we utilize the Bhasaanubad dataset [29]\nsummary of statistics for all training datasets is provided in\nFor evaluation, we employ both internal and external test\nsets. Internal evaluation sets are constructed by randomly\nbenchmarking, we use the Few-shot Learning Evaluation of\nUniversal Representations of Speech (FLEURS) dataset [60],\nation datasets are summarized in Table IX in Appendix C.\nFor system-level evaluation, we develop a cascaded speech\nBLEURT, COMET) ACROSS CHINESE (ZH)–ENGLISH (EN) DATASETS.",
  "2601.10629": "=== METHOD ===\ncloning model to enable high-fidelity timbre transfer for downstream speech synthesis.\nis fully open-sourced, including code and pretrained models, to advance reproducible\nIn recent years, the rapid evolution of large-scale multimodal foundation models has fundamentally\ngeneration models, including Veo 3 2, Wan 2.6 3, Seedance 1.5 Pro (Seedance et al., 2025), and Kling 2.6\n1https://platform.openai.com/docs/models/gpt-4o-mini\nEarlier controllable TTS methods typically relied on explicit prompt engineering or learned latent\nrepresentations, rather than directly modeling instruction semantics from natural language, such as\nPromptStyle (Liu et al., 2023) and PromptSpeaker (Zhang et al., 2023). While these approaches laid\ndifferences, these approaches all compress rich voice attributes into a single continuous vector, leaving\nwork has begun to explore direct instruction understanding via large language models to achieve more\nRecent advances in audio-centric foundation models, such as MiMo-Audio (Zhang et al., 2025a) and\nStep-Audio2 (Wu et al., 2025), demonstrate that scaling both model capacity and training data can\nsubstantially enhance representation quality and instruction-following capability in speech models. Their\nstrong performance shows that large-scale models can better capture the semantics of textual instructions\nand generalize across speakers and styles, laying the groundwork for unified text-and-audio modeling.\nThese developments suggest that LLM-like modeling with sufficient data and parameters is a promising\napproach to achieving fine-grained, natural-language-driven voice control, motivating the design of our\nmethod.\nTo address these limitations and build on insights from recent audio-centric foundation models, we\nboth open-source accessibility and instruction-driven, fine-grained controllability, our method closes a\nclosed-source systems, which offer limited transparency and reproducibility. Our method closes a key\net al., 2022), fine-grained attribute modeling mechanism that explicitly decomposes high-level natural\nattributes. By modeling this reasoning process as auxiliary attribute tokens, the model is guided to\nArchitecture\nAs shown in Figure 1, VoiceSculptor adopts LLaSA-3B (Ye et al., 2025) as the voice design model and\nCosyVoice2 (Du et al., 2024) as the voice cloning model. LLaSA is built upon the open-source LLaMA\nand sequence modeling capabilities of large language models. To enable speech generation within an LLM\nallows the model to jointly reason over text and speech tokens using a unified autoregressive generation\nvoice design. Second, we introduce CoT–based fine-grained attribute modeling, which enables precise,\nmodels such as CosyVoice2.\nadjacent tokens. Based on these fine-grained temporal cues, a trained punctuation-prediction model is\nstyle and vocal characteristics. To mitigate hallucinations introduced by large language models, we apply\na cross-validated emotion labeling process. Specifically, we employ multiple complementary models,\nthese models are then cross-validated to resolve inconsistencies and improve label reliability, yielding a\nand human verification. Specifically, we employ the DataSpeech (Lyth & King, 2024) model to estimate\nCoT–based Fine-grained Attribute Modeling\nvocal style. However, directly conditioning speech synthesis models on explicit attribute tokens often\nleads to brittle control and over-reliance on structured inputs, limiting the models’ ability to generalize to\nTo address this challenge, we introduce a CoT–based fine-grained attribute modeling strategy that explic-\nitly guides the model to reason over acoustic attributes through intermediate semantic representations.\nThis design enables the model to interpret high-level textual descriptions, decompose them into attribute-\nBased on this formulation, VoiceSculptor jointly models instruction text, CoT-based fine-grained at-\nmodel to explicitly control multiple acoustic dimensions during synthesis, supporting precise prosody\nthe model is trained to predict not only the acoustic attribute tokens and intermediate linguistic tokens,\noptimizing CE loss across both modalities, the model learns a unified latent space that tightly couples\nIn particular, the text-token CE loss encourages the model to capture semantic intent and attribute\nas 0.2, forcing the model to infer the intended acoustic attributes solely from natural language instructions\nand contextual cues. This training strategy serves as an effective regularizer, improving the model’s\nTo improve the model’s generalization ability and robustness when handling out-of-domain natural\nthe model to leverage prior knowledge encoded in semantically related in-domain instructions, thereby\nsimilar to those observed in the training data, using the Qwen3-Embedding-0.6B model (Zhang et al.,\nfirst converted into a dense vector representation using the same embedding model. A cosine similar-\nquently injected into the model’s input, guiding it toward a more stable and accurate interpretation of the\napproach enhances both the robustness and controllability of the model under open-ended instruction\nperformance under different model sizes, data scales, and training strategies. In addition, we perform\nevaluation on the model’s instruction-following and controllability performance in Chinese, enabling a\nnot imply any inherent limitation of the proposed method to Chinese. Instead, Chinese is adopted as a\nrepresentative language to demonstrate the feasibility and effectiveness of our approach, which can be\ndesigned to measure how well a model follows natural-language instructions in speech synthesis.\nevaluation protocol with a large language model as the evaluator.\nTable 1: Performance Comparison Across Different Models on InstructTTSEval-Zh Benchmark\nModel\n* indicates commercial models, while the others are open-source. All metrics follow InstructTTSEval. For ElevenLabs\nmodels, and evaluated with Gemini 2.5 Pro, from which the reported results of the compared models are obtained.\nproprietary models that rely on closed-source infrastructure. In contrast, VoiceSculptor achieves state-of-\nthe-art (SOTA) performance among open-source instruction-following TTS models, while maintaining a\nincluding fine-grained attribute modeling, instruction-aware training, and robust scaling strategies.\nule, along with the corresponding test text, is fed into the CosyVoice2 model for downstream speech\nmodel. Experimental results indicate that the stylistic attributes encoded in the prompt waveforms\nintroduction of an additional synthesis stage and a different model architecture, the generated speech\nScaling Study on Model Size and Data Size\ncalls to proprietary models such as Gemini, which significantly increases evaluation cost and latency. To\nTable 3: Scaling Study on Model Size and Data Size\nModel\ndemonstrate that model performance consistently benefits from both increased model capacity and\nthe model from 1B to 3B parameters yields clear improvements across all metrics, indicating stronger\nrepresentation and generalization capabilities. For a fixed model size, expanding SFT data from in-\nfavorable initialization and enables the model to better exploit downstream supervised data for the voice\nvalidation loss for larger models, and richer training data throughout training.\nIncreasing model capacity and enriching the quality and diversity of training data are fundamental to\ndemonstrating the effectiveness of explicit chain-of-thought modeling for voice design tasks. These\ngains are observed without altering the model architecture, indicating that CoT primarily enhances\ncontrollability and attribute understanding rather than relying on increased model complexity.\nAcross both model scales, models equipped with CoT-based attribute tokens achieve better overall\nperformance, suggesting that the proposed approach generalizes well to different model capacities.\nFigure 3 provides supporting evidence from validation loss tre",
  "2601.12966": "=== METHOD ===\ndata during training. Our approach leverages style embed-\nincorporate them into our TTS model to generate speech at\nmethod preserves naturalness and speaker identity, enhances\nin interactive systems [5, 6].However, typical TTS models\nPrevious approaches to incorporate Lombard or hyper-\non SNR feedback [11]. More recent methods leverage con-\noften fail to generalize to unseen speakers. The method most\nAlthough this approach achieves\nbard TTS approaches by leveraging F5-TTS [14] as the base-\ndata. Experiments show that our method preserves speaker\n2. METHODOLOGY\nTTS model that, during training, receives masked mel-\nthe original F5-TTS model, while green blocks indicate modules introduced in the proposed version.\nlowing the model to implicitly learn the alignment between\ncharacters and mel frames. At inference, the model takes both\nduring inference. The overall architecture of the proposed\n2.1. Model Adaptation\nSince our approach does not rely on the in-context learn-\ning ability of the base model, we inject speaker information\nnetwork (TDNN) architecture [16].\nThe F5-TTS Base model consists of 22 Diffusion Trans-\nrived from the style embeddings. To encourage the model\nence text are omitted, and the model generates speech con-\nwith the model, the style encoder captures prosodic variabil-\ntain manipulated embeddings. This approach enables contin-\nWe first evaluated our model against the F5-TTS baseline on\nacross different methods. To account for this, we define a\nModel\nspeaker verification model 3.\nground-truth (GT) samples. Interestingly, our model often\nlevels (SNR = 10, 5) and higher Lombardness, our model\nmodel achieved slightly better scores in intelligibility.\ncorrelated components, our approach enables fine-grained\nmodels for hyperarticulated speech,”\npensating for hyperarticulation by modeling articulatory\n[5] Bernhard Suhm, Brad Myers, and Alex Waibel, “Model-\n“Vocal effort modeling in neural tts for\n“Gradual modeling of the lombard effect by modifying\nspeaker embeddings from a text-to-speech model,” in\n\n\n=== EXPERIMENT ===\ndings learned from a large, prosodically diverse dataset and\nEvaluations demonstrate that our\nare trained on read-speech datasets that lack such variability,\nLombard datasets [7, 8, 9], controlling spectral tilt to vary vo-\nprosodically diverse dataset.\nspace using PCA on Lombard and articulation datasets to\nPreliminary experiments indicated\nity present in the Emilia dataset [18], which includes diverse\nand articulation, was analyzed using the ALBA dataset [20],\n3. EXPERIMENTS AND RESULTS\nthe lombardness levels of AVID dataset by shifting relevant\nFor subjective evaluation, we conducted a user study on\nbased and empirical evaluation of multimodal interac-\ngual, and diverse speech dataset for large-scale speech\n\n\n=== RESULTS ===\nresult in higher diversity in the formant space [1, 2]. Hu-\ntorted speech can result in higher error rates [3].Conventional\nreasonable results, it still requires some Lombard data for\nBased on these findings, we control lombardness by pro-\nthese results highlight that style embeddings enable stronger\nThese results confirm that manipulating style embeddings to\nTable 2: WER results across different SNR levels (%).\nTable 3: ∆WER results. Lower is better.\nthese results demonstrate that manipulating style embeddings\nTable 5: SSIM results across Lombardness levels.\nour results demonstrate that leveraging large-scale prosodic\n\n\n=== CONCLUSION ===\n4. CONCLUSION\nzero-shot Lombard TTS. Future work could explore more",
  "2601.02944": "=== METHOD ===\nspace models (SSMs) offer linear complexity,\npure causal SSMs architectures often strug-\nerties of hybrid architectures by proposing\nHydra’s native bidirectional modeling, which\nflow-matching-based synthesis methods. Cru-\nlower models. These results demonstrate the\ntive method for ADD.\nthough such methods offer interpretability and com-\nmodels have shifted to end-to-end architectures,\na self-supervised learning (SSL) model based on\nto other pre-trained models (PTMs) such as Whis-\nConformer architecture (Gulati et al., 2020), which\ncombines the global context modeling of Trans-\nstate space models (SSMs) (Gu et al., 2022) with\nmodels like Rawformer (Liu et al., 2023), vali-\narchitecture in audio forensics. Following this,\nAlthough pure SSM models excel in inference\nsults from cutting-edge hybrid architectures such\nlying solely on one mechanism limits the model’s\nbrid SSM-Attention architecture as the back-end\nMethods\nThe original Mamba model performs causal com-\nments in large language models (LLMs) such as\ncompared to pure SSM architectures. This hy-\nhybrid SSM-Attention architectures.\nquence modeling) and Attention mechanisms (pre-\nings while maintaining robust sequence model-\nlayer architectures. Each structurally resembles\nnative to self-attention in speech modeling. This\nby the pre-trained XLSR model, which extracts a\nMamBo Architectures\ninstantiation of the SSM component. Four variant configurations of the MamBo architectures include: (a) replacing\nproposed method via cross-dataset evaluation, we\nand flow-matching-based TTS models (subsets\ntrained separate models for the ASV21LA (21LA)\nmodel applied linear and non-linear convolutive\nnoise, while the 21DF model used stationary signal-\ntuned the models using the AdamW (Loshchilov\narchitecture, the projection dimension was set to\nmodel) and two NVIDIA RTX 3090 24 GB GPUs.\nModel\nTable 1: Comparison of MamBo-1 and MamBo-2 architectures across varying stacking depths N on the ASV21LA,\nASV21DF, ITW, and DFADD evaluation sets. All models were trained on ASV19LA. Results are reported as \"Best\nMamBo series architectures across SSM variants\nmodels) and F1-F2 subsets (generated by distinct\nflow-matching models).\nD3, demonstrating the model’s efficacy in ro-\nFor the MamBo-2 architecture, performance re-\nunknown generative models.\nplex hybrid architectures, MamBo-3 and MamBo-4.\nshow that increasing model capacity contributes to\nModel\nTable 2: Comparison of the MamBo-3 architecture across varying stacking depths N and MamBo-4 under different\nsets. All models were trained on ASV19LA. Results are reported as \"Best / Avg\" across the top-5 checkpoints.\n(L = 7), deeper architectures generally improved\nof shallow architectures remain sensitive to unseen\ndemonstrate that increasing model depth mitigates\nvariance prevalent in shallower models.\nMethod\nmodels.\nMamBo-3-Hydra-N3 hybrid model yields the low-\nof Hydra’s native bidirectional modeling. Theo-\nmodel to capture more complex, holistic non-causal\nflow-matching-based synthesis methods. Notably,\nbility often observed in shallower models. Deeper\nSSM-Attention architectures are promising alter-\nadvancing these hybrid architectures to keep pace\nity, and the theoretical upper bound of the model’s\nmodel’s robustness unverified against deepfakes in\ngence phenomenon during training. Models fre-\nthe best-performing model for unseen scenarios.\nprivacy-preserving architectures such as on-device\near attention language models balance the recall-\nmodels. arXiv preprint arXiv:2504.03624.\nzero-shot text-to-speech model. In Interspeech 2024,\nEnd-to-end bidirectional state space model for audio\nGeneralized models and efficient algorithms through\nkanformer: A kan-intergrated model for synthetic\nssm hybrid model. arXiv preprint arXiv:2405.16712.\nsequence modeling with selective state spaces. arxiv\n2023. In The Conference On Language Modeling\nciently modeling long sequences with structured state\nstate space models through generalized matrix mixers.\ntransformer-mamba language model. In The Inter-\nbrid state space models for efficient unlimited context\nlanguage modeling. In The International Conference\nA raw data boosting and augmentation method ap-\nTemporal-channel modeling in multi-head\nA dual-column bidirectional state space model for\nnormalization in the transformer architecture. In In-\nPre-Trained Model\n\n\n=== EXPERIMENT ===\ndra, and Gated DeltaNet. Experimental results\nevaluations on the DFADD dataset demonstrate\non massive cross-lingual datasets. Benefiting from\n(ASV21DF) (Yamagishi et al., 2021) dataset at\nefficiency and ADD performance, experimental re-\net al., 2022), and DFADD (Du et al., 2024) datasets,\nevaluation of depth scaling in SSM modules and its\nExperimental Setup\nDatasets and Metrics\ndevelopment, and 71,237 evaluation utterances. To\nconducted experiments on four challenging test\nITW dataset contains real-world deepfake audio\nas evaluation metrics.\nand ASV21DF (21DF) evaluations. The 21LA\nwas used in experimental runs.\nthe DFADD dataset are reported separately for\nIn the evaluation of the MamBo-1 architec-\nevident on the ITW dataset, where Mamba and\nout-of-domain datasets.\nRegarding the more challenging DFADD dataset,\nthan in-domain datasets, all variants produced\non ASV21LA and DF. On the ITW dataset, how-\nIn the detailed evaluation on the DFADD dataset,\nFigure 2: Evaluation results of the MamBo-3 architec-\nThe checkpoint EERs trends for the F2 dataset are\nthe lowest EER of 3.02% on the F1 dataset with the\nThe experimental evaluation of the MamBo-4 ar-\nthe ASV21DF and ITW datasets, Hydra demon-\nDFADD evaluation, the four SSM variants showed\nbackbone depths (L = 5 vs. L = 7, with fixed N = 1) on the ASV21LA, ASV21DF, ITW, and DFADD evaluation\nFigure 3: Evaluation results of the MamBo-4 archi-\nants on F1-F2 datasets. Notably, Mamba-2 and\nization on the F1 dataset, achieving EERs of 8.49%\nMamBo-3 and MamBo-4 experiments on F1-F2\n21LA, 21DF, and ITW evaluation sets.\nExperimental data indicates that the\n(4.97%) datasets.\nMamba (L) on the ITW dataset. Based on these\ninconsistencies. Through a systematic evaluation\nfor ADD. Experimental results indicate that the\nITW datasets, comparable to or surpassing several\nmore, evaluations on the DFADD dataset suggest\ntions. First, the current evaluation relies exclusively\nASV19LA dataset. While this serves as a stan-\ndatasets utilized in this work are predominantly\nWhile this study utilizes public datasets, the real-\nand flow-matching based audio deepfake dataset. In\nevaluation of state-of-the-art voice spoofing counter\nDatasets\ndatasets/isjwdu/DFADD\n\n\n=== RESULTS ===\nfiguration achieves competitive performance\nbenchmarks. This performance benefits from\nachieved state-of-the-art (SOTA) performance on\nDat, 2025), further pushed the performance bound-\nand the results demonstrate its robustness and ef-\nimpact on detection performance in audio deepfake\nThe resulting projected features H ∈RT×D serve\nResults and Analysis\nTables 1 and 2 summarize the performance of the\nagainst advanced generative algorithms, results on\nachieve the best generalization performance. These\ndistinct inductive biases, resulting in varying de-\nmaintained competitive performance in shallower\ntions in MamBo-1. These findings suggest that\nTable 2 further presents results for the more com-\nOn the ASV21DF, performance trends diverged\n(4.45% and 4.97%, respectively). These results\nresults on D1-D3 aligned closely with ITW trends.\nants exhibited improved performance through ap-\ncomparable performance of 3.47% at N = 2.\ncomparable performance on ASV21LA, whereas\nnegligible performance differences across the D1-\ndisplay performance with backbone depths L = 5 and\nvergent trade-offs emerged; Mamba performance\npoint performance reveals persistent low bias ac-\ngenerative algorithms. Consistent findings from the\ntency and effectively attenuate the performance\nTable 3 presents the performance of the proposed\nempirical findings, and considering its parameter\npetiti",
  "2601.11329": "=== METHOD ===\nF-Actor: Controllable Conversational Behaviour in Full-Duplex Models\nconversational speech model that can be trained\nand finetuning only the language model, our\nmodel requires just 2,000 hours of data, with-\nThe model can follow\nsign choices. Both the model and training code\napproaching human-like communication (Cheng\nmodelling using a variety of architectural choices:\n(Ruede et al., 2017; Chen et al., 2025b), modelling\nlatter approaches enable models to handle inter-\nmodel them explicitly on the system side. That is,\nmodels are trained to robustly handle overlapping\nmodels offer limited customization.\nEmb. Model\nModel\nFigure 1: Overview of our controllable full-duplex model, which can be prompted to control (i) speaker voice,\nduplex models, for instance by specifying a target\n2025). To date, these models and their code have\nmodel that behaves like an actor following con-\napproach for training it under typical academic re-\nfrozen and finetuning only the LLM, our approach\nmodel that can follow explicit instructions regard-\nmodel. Our contributions are three-fold:\nduplex model.\nyielding good models on an academic budget.\nWe release both the model and the training code\nfull-duplex speech language model; alternative ap-\nFull-Duplex Modelling. Full-duplex interaction,\nthe model vocabulary. Each speech stream then\nthis approach is limited by the chunk-size for in-\nmodels (Défossez et al., 2024; Hu et al., 2025).\nThe codec models represent each audio frame\nhow these codes are modelled. Dependent code-\nerarchical prediction: the language model predicts\net al., 2024, FSQ), which simplify modelling by al-\nFull-duplex models have gained significant atten-\ntion, resulting in a diverse set of architectures.\nArchitectures. Most models rely on text-based\net al. (2023). Cascaded architectures (Chen et al.,\nmodel turn-taking and overlapping speech. This is\nEnd-to-end architectures typically support these\nstreams, and codec-based models for speech input\n2022; Zeghidour et al., 2021). Other approaches\net al., 2024). Some architectures avoid multiple\nInstruction-Following. While recent models em-\nOne such model is BeDLM (Lee et al., 2025),\nBeDLM is primarily a dialogue generation model,\ncaded speech model with duplex capabilities, can\nit unclear whether they extend to the duplex model.\nduplex model, is trained for instruction following\nnot release their code or models, making direct\nModel\nF-Actor, our full-duplex model, is based around an\nding model and an audio encoder. Unlike previ-\n2Voila released the base model, but not the full-duplex.\nArchitecture. We use Llama3.2-1B-Instruct3\nwithout requiring a depth-transformer architecture\nour model to adhere to explicit instructions. An\nDuring inference, the model begins by append-\nOur model processes two\nstreams allow the model to reliably distinguish\nLLM’s original language-modeling head to predict\nWe evaluate our full-duplex models along two cri-\nlogues between two instances of our model, al-\n2024), assigning each model instance its respective\naudio streams are. Note, that the model is con-\nall alignment methods are reported in Tab. 4, with\nand evaluation protocol of our full-duplex model.\nserve our needs for instruction-following models.\nand a public model8 trained on Librispeech (Panay-\nInstruction Following Prefix. To train the model\nvosk-model-en-us-0.22\nkaldi-model-m13\nTable 1: General modeling results for full-duplex models trained to predict both system and user (s/u) or only the\nsystem role (s). Models are compared with and without a text stream, using word- or utterance-level alignment, and\nconversation, which biases the model to rely on\nspeaker set, we prevent our model being used for\nTo maximize efficiency, our approach utilizes a sin-\ntext pretraining as required by prior models such\nper model. We use a maximum sequence length of\ntraining choices for our full-duplex model.\nabling the model to predict not only audio but also\ntions (BC/I tok.) to help the model generate the\nDSU prediction architecture, we also evaluate an\nTable 2: Instruction-following results for full-duplex models trained to predict both system and user (s/u) or only the\nsystem role (s). Models are compared with and without a text stream, using word- or utterance-level alignment, and\nThe only available instruction-following models,\nrelease code or model checkpoints, making direct\nmodel against Moshi (Défossez et al., 2022) and\nmodels in terms of general modeling abilities, in-\nmodels and the influence of sampling parameters.\nGeneral Modeling Abilities\nAll results for general modeling abilities are shown\nmodels do not match these oracle values, the re-\nTab. 2 summarizes our models’ performance across\nerally, the model trained on predicting both the\nwhether the model should initiate the conversation\nstrongly depends on whether the model also gener-\nates text. Such models, in combination with audio\ndelay, achieve over 99% accuracy, whereas models\nBased on manual inspection, we find that the model\nbecause the model does not explicitly represent\nated by the model, particularly for word-level align-\nComparing Turn-Taking to SOTA Models\nturn Gap and Overlap, our model exhibits behavior\nmodel’s pauses are generally longer than the gaps,\nModel\nTable 3: Cumulated durations per minute across models\nmodel behaviour. Results for different tempera-\ntures (0.6–1.0) using our best-performing model\nmodel codebook dependencies and instead predict\nrable to that of the FSQ-based models. However,\nlable full-duplex speech models that can be trained\nmodel F-Actor produces coherent conversations\nour model controllable with respect to backchan-\ndesign choices and release our model and code\ninstruction-following full-duplex speech models.\n1. While the model’s predictions of backchan-\nbers specified in the prompt, the model con-\nthese phenomena may allow the model to bet-\nhow such behaviors can be modeled in full-\n2. The codec model we use, NanoCodec, does\npabilities of our full-duplex model in English\nSpoken conversational models pose inherent risks,\nFirst, the model is restricted to a small fixed pool of\nthe model is trained exclusively on text-to-speech\nFinally, the model is released for research pur-\nmodels. arXiv preprint arXiv:2406.02430.\nguage models: A comprehensive survey. Preprint,\nmodel for seamless voice interaction.\nken language models. Preprint, arXiv:2509.14515.\nin full-duplex speech language models with planning-\nspeech-text foundation model for real-time dialogue.\nmodels. Preprint, arXiv:2407.21783.\nburg. 2025. Efficient and Direct Duplex Modeling for\nSpeech-to-Speech Language Model. In Interspeech\naware generative spoken language modeling. In Pro-\nerative spoken language modeling from raw audio.\ngeneration with large language models. In Proceed-\ning overlap handling for full-duplex speech models.\nspoken dialogue language modeling. Transactions\nterleaved spoken and written language model. Trans-\nVoice-language foundation models for real-time au-\nmodels based on gemini research and technology.\n2024 Conference on Empirical Methods in Natural\ncodec language models are zero-shot text to speech\nmodel. In Advances in Neural Information Process-\nspeech language modeling for dual-channel spoken\nspeech dialogue model with frozen llm. Preprint,\nOmniFlatten: An end-to-end GPT model for seam-\nTo enable Large Language Models (LLMs) to pro-\nunits, allowing the model to be trained using stan-\nbased language modeling. However, the properties\napproaches.\nend-to-end modeling such as in Moshi and VALL-E\ninformation density of DAUs can make modeling\n(SSL) models like HuBERT or WavLM (Hsu et al.,\nrecent work chooses to model DSUs with addi-\nto-fine generation process. In this latter approach,\na separate model generates continuous representa-\nQuantization Methods\nThe choice of quantization method fundamentally\ndictates the architecture of the Speech Language\nModel, specifically regarding how it models the\nFSQ). In RVQ-based models like EnCodec or Mimi\nquires",
  "2601.11141": "=== METHOD ===\nModel with Personalized Voice Cloning\nthese models often exhibit limited speaker iden-\nend spoken dialogue model that achieves both\nities. Our code and models are publicly avail-\na downstream large language model (LLM), and\naudio language models (LALMs) emerged follow-\nstanding and speech generation. Early models such\nWhile more recent audio-understanding models\ntime dialogue models (Défossez et al., 2024; Xie\nto-end spoken dialogue model that achieves both\n• A streaming architecture that tightly couples\ngeneration model on audio embeddings from\npipeline, and pretrained model weights to support\nCascaded approaches remain widely deployed due\nend-to-end speech modeling. Moshi (Défossez\narchitecture for real-time dialogue, while Spirit\nFigure 2: Overall architecture of Chroma 1.0. The Reasoner outputs text tokens and hidden states. These form an\ndual-stream (Thinker-Talker) architectures that de-\nModels focused primarily on audio understand-\nneural codec language models (NCLMs), which\nmodeling EnCodec codes with a large conditional\nlanguage model yields natural, expressive zero-shot\napproaches further advance fidelity and control.\nModel Architecture\nlogue architecture designed to achieve high-quality,\nbone for acoustic modeling, the Chroma Decoder\nenables the model to leverage prosodic and rhyth-\ntual modeling for subsequent speech synthesis.\nthe model on the target speaker’s acoustic character-\narchitecture with approximately 100M parameters.\ndialogue data that meet our model’s requirements\nvoice cloning and acoustic modeling.\nrepresentations. This objective enables the model\nWe evaluate our model on multiple\nerated and reference audio. To evaluate the model’s\nsize of 4. The model is trained for 100K steps on\nsentation model fine-tuned on speaker verifica-\nModel\nTable 1: Performance comparison of speech models on\nSeed-TTS (Anastassiou et al., 2024). Our model\ncurrent state-of-the-art speech models and a human\nmodels achieve comparable speaker similarity\nother models.\nmodels, resulting in 30 comparative samples evalu-\n8https://elevenlabs.io/docs/overview/models\nstage approach: first creating a voice profile from\nIn contrast, Chroma uses an end-to-end approach\nextraction. Chroma’s end-to-end architecture pre-\nThe current Chroma architecture does not support\ntion phase, the model avoids reprocessing prompt\nModels\nTable 5: Task accomplishment scores for end-to-end spoken dialogue models across understanding, reasoning, and\nall models.\nCritically, Chroma is the only model in this com-\nAll other models focus exclusively on dialogue\nover larger models (7B-9B) and delivers superior\nperformance compared to smaller models (0.5B)\nreal-time end-to-end spoken dialogue model that\ndiverse domains. We release our code and models\nmodels. arXiv preprint arXiv:2406.02430.\nlanguage models. arXiv preprint arXiv:2311.07919.\ntext foundation model for real-time dialogue. arXiv\nlanguage models. arXiv preprint arXiv:2412.10117.\nsive large audio language model.\nenizer for audio language modeling. arXiv preprint\ntorized codec and diffusion models. arXiv preprint\nlanguage models. Advances in Neural Information\n2024. Advancing large language models to capture\nguage model. Transactions of the Association for\nothers. 2022. Training language models to follow in-\nmodel is secretly a reward model. Advances in neural\ncient foundation language models. arXiv preprint\ncodec language models are zero-shot text to speech\nguage models can hear, talk while thinking in stream-\nend-to-end spoken dialogue models. arXiv preprint\nand 1 others. 2022. Scaling autoregressive models for\nSpeechgpt: Empowering large language models with\nlingual neural codec language modeling.\nIntegrating such methods could further improve di-\nonly architecture aligning with recent trends in\nspeech language modeling, encoder-decoder ar-\nThese architectures may offer distinct advantages\nmentary benefits to our current approach, particu-\nmodels.\nThis balanced weighting encourages the model to\nzation layers, enabling the model to capture fine-\nAs a result, the final model achieves improved voice\n\n\n=== EXPERIMENT ===\nTraining Datasets\nPublicly available datasets lack high-quality speech\nExperiments\nExperimental Setup\nDatasets.\nbenchmark datasets to assess different aspects\nformance evaluation, we use the CommonVoice\ndataset (Ardila et al., 2020), which provides di-\ncapability, we conduct subjective experiments mea-\nUnless otherwise specified, all experiments were\nEvaluation Metrics.\nevaluation using both objective and subjective met-\nevaluation, we employ Speaker Similarity (SIM),\ntor (RTF). For subjective evaluation, we conduct\nthrough human listeners. Our SIM evaluation is\nObjective Voice Cloning Evaluation\nVoice dataset, following the protocol established by\nTable 2: Comparative evaluation between Chroma and\nSubject Voice Cloning Evaluation\nWe conducted comparative experiments between\ncial voice cloning system. The experiments mea-\nditional experiment comparing ElevenLabs outputs\njective evaluations. Consequently, Chroma’s com-\native evaluations demonstrate competitive perfor-\nparticipated in our human evaluation experiments.\ntwelfth language resources and evaluation confer-\n\n\n=== RESULTS ===\nperimental results demonstrate that Chroma\npetitive performance across understanding,\ninteractive performance, with lower values indicat-\nonly audio, we assess its performance in a zero-shot\nTable 1 presents the comparative results against\nThis result suggests\nTable 2 presents the comparative results. For NC-\nthe results were remarkably close: ElevenLabs re-\npetitive SCMOS performance suggests stronger\nperformance.\noral conversation capabilities. Bold values indicate Chroma’s performance, which remains competitive across all\nstrating strong performance for streaming applica-\nChroma demonstrates strong performance across\ntently achieves second-best performance: 71.14%\npetitive performance particularly noteworthy: it\nwhile maintaining real-time performance. Compar-\nchitectures have demonstrated strong performance\n\n\n=== CONCLUSION ===\nConclusion\nLimitations and Future Work",
  "2601.09239": "=== METHOD ===\nof discrete Speech Large Language Models\nspeech modeling. Audio samples are avaial-\nmodel will be made publicly available after\nThe rapid advancement of large language models\nisting architectures, fully discrete Speech LLMs\nand acoustic information atop mixed architectures.\nin this task. Some models bias towards linguis-\nSpeech Large Language Models\nThinker-talker architectures (e.g., MinMo (Chen\n(2) Fully discrete architectures (e.g., GLM-4-\nment. Notable recent disentanglement methods\ninclude: SpeechTokenizer uses SSL model distilla-\n(Gong et al., 2025) equips RVQ architecture with\nMethod\nthen fused via distinct condition injection methods\npre-trained HuBERT model as the semantic en-\nconstruction objectives. This approach inevitably\n(a) DSA’s Main Architecture\n(b) Self-Reconstruction Mode: The model learns to predict the velocity field of the full Mel-spectrogram based on\nthe complete acoustic and semantic tokens. (c) Recombination (Contextual Inpainting) Mode: The model learns\ntic tokens) are provided as conditions. The model\nmodel to infer global acoustic style from partial\nattention. This allows the model to flexibly cap-\nthe model captures long-range linguistic context\ninherent capability for cross-sequence modeling.\nsquared error (MSE) between the model-predicted\nprotocols are adapted to three categories of model\narchitectures:\nTraining and architecture details of DSA-Tokenizer\nFour representative types of baseline models\nModel\nFSQ layers. Baseline models with similar bitrate are selected for fair comparison, annotated with their codebook\nshown in Table 1, our model achieves a superior bal-\ntures without effective fusion, our model surpasses\nmodels, the ASR-supervised CosyVoice2 S3 Tok-\nModel Family\ntic tokens capture style (high SC ACC), the model\nthe separate modeling of semantics and acoustics.\ndisentanglement designs, our model’s superiority\nModel\nthis enables LLMs to independently model the two\nacoustic modeling. In conclusion, both compo-\nWhile the model demonstrates robust performance\nthe DSA-Tokenizer to general audio modeling in\nmodels for natural interaction between humans and\nquality versatile speech generation models. Preprint,\nlanguage models to see, hear and speak with vivid\nmodel for seamless voice interaction.\nand masked language modeling for self-supervised\nsynthesis with large language models.\nspeech-text foundation model for real-time dialogue.\nnoising diffusion probabilistic models. Advances\nmodeling. In The Thirteenth International Confer-\native spoken language modeling from raw audio.\nmatching for generative modeling.\nInterleaved spoken and written language model.\ndiffusion models with transformers.\napproach for tonal language speech synthesis. arXiv\ndio language models are few-shot learners. arXiv\ndiffusion models. Preprint, arXiv:2302.05543.\nspeech tokenizer for speech large language models.\nModel Architecture\narchitecture with dimensions of [512, 1024, 1024,\narchitecture with dimensions of [1024, 1024, 1024,\nwill hurt the performance of our model, therefore,\nwe utilize speaker-diarization model 3 (Bredin and\n430M. We trained the models on the Ascend plat-\nAll models are trained for 30 epochs, with the\nmodel achieving the lowest validation WER (for\nis computed. The model is trained for 5 epochs\nFour representative types of baseline models\nthe model with six codebook layers that operates\nshares a similar architecture\nmodel (An et al., 2024), which discretizes inter-\nModel\ntrained model available at 4. Speaker similarity\nThe speaker-diarization model(Bredin and Laurent,\n2021) and the model4 used to calculate the UT-\nAnd the model6 calculating CER is released under\nmodel-license license.\n\n\n=== EXPERIMENT ===\nof the latter. Our experiments demonstrate that\nThe experimental results demonstrate that our\nExperiment Setup\nEvaluation Tasks\nTwo evaluation scenarios are considered: (1) Re-\nused to assess the acoustic information. Evaluation\nFor evaluation, we use UTMOS for naturalness,\nDatasets\nChinese-English subsets of the Emilia dataset (He\nspeaking styles; we clean this dataset to boost per-\ntively. Waveform Evaluation: We use the mul-\ntilingual SeedTTS datasets (Anastassiou et al.,\nused for ASR-based content retention evaluation,\nFigure 3: Disentanglement probing evaluation results. (L0) means the first layer, (L1-7) means the second to eighth\ntwelfth language resources and evaluation confer-\ndiverse english speech recognition dataset for com-\ndataset for large-scale speech generation. Preprint,\nconversational (ramc) speech dataset. arXiv preprint\nDataset Cleaning\nWe find that some samples of Emilia Dataset (He\nThe dataset used for the training of the semantic\nIn the disentanglement probing experiments, to-\nFor the ASR experiment, English character se-\nFor dataset configurations: the LibriSpeech train-\nHuBERT. In our experimental setting, for a fair\nTable 4: Disentanglement probing evaluation results\nEvaluation tools\nThe Emilia dataset, LibriSpeech, MagicData-\n\n\n=== RESULTS ===\ntion—yet their performance hinges heavily on the\non reconstruction quality or ASR performance, we\ning performance in acoustic-related LLM tasks\nadded to the noisy input mt, resulting in the fused\nTable 1: Performance comparison of different tokenizers on speech reconstruction and cross-utterance recombination\nResults and Discussion\ntic integrity—resulting in high error rates—DSA-\nperformance across layers, suggesting semantic\nperformance of acoustic-related task?\nTable 2: LLM-based voice cloning performance.\nablation results verifying the contribution of the\nresults in slight SIM degradation during reconstruc-\ntion but leads to a severe performance collapse in\nbalanced performance across all tasks.\nwhere [·; ·] denotes concatenation resulting in a\nRDout×2D, bproj ∈R2D. The resulting h serves\nsize 2,048, at a 12.5 Hz frame rate, resulting in a\nchannels and 3 levels per channel, resulting in a to-\nDisentanglement Probling Result\nThe detailed result of disentanglement probing is\n\n\n=== CONCLUSION ===\nConclusion\npared to GAN-based counterparts. Future work\nLicense Discussion",
  "2601.02753": "=== METHOD ===\nlack of TTS-quality audio-visual corpora, previous approaches suffer\nknowledge transfer scheme. This paper proposes a new approach called\non Voxceleb testset. The proposed method then uses a retrieval-based\nimage generation methods. Of particular interest is the face imagery\ngeneration model [17] to produce probable speaker embeddings given\nbaseline method [19]. Using this system as guidance, we demonstrate the\nthe state-of-the-art approaches. Self-Lifting employs an iterative learning\n[4] trains TTS models with paired speech and face input on LRS3 [6],\npretrained language-visual matching model with broad vision-related ap-\n3. METHOD\nWe propose a two-part method for face-based speaker generation. An\noverview of this method is illustrated in fig.1. In the first part, a matching\nGMM-based speaker generation module. This method considers both\nFig. 1. Overview of the proposed method. First a Vclip and speaker\ngeneration model is trained (red, steps 1-2). Then image features extracted\nhas been shown in [13] that a modality gap exists for multi-modal models:\nThe trained Vclip model contains necessary information to link voices\nWe start by modeling an unconditional distribution p(e) for speaker\nding space using gaussian mixture models (GMM) following [17]. To\nthe downstream TTS model (use TTS speaker encoder as fv), a na¨ıve\napproach is to score the candidate samples {e} using the Vclip-projected\napproach ignores the speaker mismatch between the input and output of\nincurs too much cost. We note, given Vclip model, the composition\nWe use a separate Vclip model as an automatic evaluator for assessing\nGiven a pair (xi,xv) of face-voice sample, our method generates a set of\ntem [2], we model the statistical characteristics of known TTS speakers’\n[19] are used for perforamance comparisons among FVA methods. For\nspeaker verification model [31] trained on Voxceleb2 is used as the\n(ϕv). All Vclip models were trained on a single NVIDIA V100 GPU with\non large mini-batches [26]. The models are trained until AUC scores\nassess the FVA performance of our Vclip model. Table 1 reports AUC\nshow that the supervised face recognition models used in our baselines out-\nthe proposed Vclip model.\nTTS module complements our method and helps to retrive voices that\nbaselines and the proposed method attains comparable results to the\nWe propose a two-part method for face-based TTS by formulating the\nFVA method called Vclip is proposed. Vclip utilizes the sematically-rich\ngeneration model to produce highly probable novel speaker embeddings\ndownstream TTS model, achieves superior matching results to a feature-\nexpressive downstream TTS methods to mitigate the attribute mismatch\nvoice: Face-styled diffusion model for text-to-speech,” in ICASSP\nChao Weng, Helen Meng, and Dong Yu, “Instructtts: Modelling\nmodels from natural language supervision,” in ICML 2021, pp.\nidentities with mixture models for speaker anonymization,”\naudio-visual model for speech separation,”\n\n\n=== EXPERIMENT ===\nreference images. Experimental results demonstrate that the proposed\nspeech synthesis is the quality of available datasets. While synthesis\ntask requires high quality audio for training, major audio-visual datasets\nfashion. Experimental findings demonstrate that a retrival step is crucial\nevaluation studies.\nan audio-visual dataset for lip-reading. But the synthesis quality of\nthis setup is severely limited by the poor quality of the corpus. Given\n3.3. Automatic evaluation of generated voices\n4. EXPERIMENTS\n4.1. Experimental setup\nDatasets. For FVA learning, unless otherwise stated, we use videos from\nclosed dataset\nopen dataset\na vanilla multi-speaker VITS [32] for all our experiments in this chapter.\nsame scale but has less identities. The evaluation results demonstrate the\nder both data settings. Consistent with [27], our preliminary experiments\nTable 3. Automatic evaluation results for generated voices.\n4.3. Automatic evaluation on face-based speaker generation\nSetup. We sample M = 500 positive face-voice pairs from Vox1\n4.4. Subjective evaluation on face-based speaker generation\ndirectly trained on LRS3 for a quality comparison. The evaluation results\nsynthetic voice. Preliminary experiments suggests that generated voices\n“Lrs3-ted: a large-scale dataset for visual speech recognition,”\n& billion image datasets,” arXiv:2301.07315, 2023.\n\n\n=== RESULTS ===\ncontribute to performance degradation for TTS [14]. Moreover, it is\nresult of the downstream TTS into account. However, it scales poorly\nco-occuring audio-visual data, achieving SOTA results on Voxceleb [18].\ndirect replacement for speaker embedding does not lead to optimal result\nTable 1. Face-voice AUC results for Vox1-test\na batch-size of 1024, as we observe a performance increase when training\n4.2. FVA performance of Vclip\nproposed Vclip surpasses Self-Lifting and achieves best performance un-\nperform CLIP in plain face recognition. We attribute the performance gain\nResults. The results are reported in table 3. We note all entries of\nv2v values are low compared to the true voice clone result on the same\nscoring into the procedure (w/ winformed), a higher f2v result is achieved.\nTable 4. Subjective naturalness results.\nTable 5. Subjective face/voice matching results.\nresults are reported in table 5. To aid interpretation, multiple reference\nfactors that are lacking in reading-style TTS data. As a result, although\n\n\n=== CONCLUSION ===\n5. DISCUSSIONS\n6. CONCLUSION",
  "2601.06560": "=== METHOD ===\nframework that explicitly models and aligns multi-resolution spectral rep-\nlike conventional single-resolution or implicit feature-fusion approaches, the\nproposed method enforces agreement across complementary time–frequency\ncol. The method achieves near-perfect performance on ASVspoof LA (EER\nThe proposed model remains lightweight and\nthe model learns resolution-consistent and semantically meaningful spectral\nplicit cross-resolution modeling provides a principled, robust, and scalable\nmachine learning models that imitate the acoustic characteristics, linguistic\ndeep neural architectures such as neural vocoders [5, 6], autoregressive gen-\nerators [7, 8], and diffusion-based models [9], enabling highly natural and\nBy conditioning these models on\ntroduced by synthesis pipelines [21, 22, 23, 24]. These approaches relied on\nWhile effective against early-generation synthetic speech, such methods were\nsumptions about specific synthesis artifacts. As synthesis models improved,\nmodels to learn discriminative representations directly from time-frequency\nrepresentations are modeled. Most existing approaches operate on a single\nexplicit cross-resolution modeling. To this end, a resolution-aware detection\ncomplementary resolution-specific features, enabling the model to adaptively\nacross resolutions, encouraging the model to focus on resolution-invariant\nstrate that the method achieves near-ceiling performance on standard bench-\ncross-resolution modeling is a critical factor for building generalizable and\n2. Proposed Method\nmodels and aligns these representations to exploit resolution-complementary\nIn conventional multi-feature approaches, these embeddings are often con-\nstrategies do not explicitly model the relationships among resolutions and\nThe proposed method is evaluated on three publicly available audio deep-\nducted on both the LA and PA subsets to assess the proposed method under\nsubsets are used for model optimization and validation, respectively, follow-\nmethods, including both open-source and commercial systems. The spoofed\nneural TTS models such as DeepVoice 3, Google Cloud TTS, WaveNet,\nmodel’s ability to generalize to unseen speakers and recording conditions.\n2.4. Model Architecture\nThe proposed resolution-aware audio deepfake detection model is de-\ning computational efficiency. The architecture consists of three main compo-\nresolution-agnostic classification heads. An overview of the model complex-\nmodel to dynamically emphasize resolution-invariant and discriminative spec-\nTable 4: Parameter count and computational complexity of the proposed model.\n2.4.4. Model Complexity\nThe proposed architecture remains compact and computationally effi-\nwith a model size below 1 MB (0.62 MB) in single-precision format.\nThe proposed model contains 159,875 trainable parameters, correspond-\ndemonstrate that the model achieves strong detection performance while\n2.5. Model Training and Evaluation\nThis section describes the training procedure and evaluation methodol-\nThe proposed model is trained as a binary classifier to distinguish be-\nresented by multiple spectral resolutions, the model outputs a scalar logit\nAll models are optimized using the Adam optimizer with a fixed learning\nTraining is conducted for 15 epochs, with the best-performing model selected\n2.5.2. Model Selection\nModel selection is performed using the validation sets corresponding to\nin the audio spoofing literature. For each experiment, the model achieving\nexperiments to characterize the overall discriminative capability of the model\nThe proposed method achieves near-perfect perfor-\nsion and recall, indicating that the model effectively captures algorithmic\nIn this more challenging replay-based scenario, the proposed model achieves\nThe proposed method achieves consistently strong results, with near-perfect\nconditions are standardized. The proposed method achieves an accuracy of\npared to the clean subsets, the proposed method still achieves a strong ac-\nTable 8 summarizes the test-set performance of the proposed model. The\nmethod achieves an overall accuracy of 95.70%, an ROC-AUC of 0.9800, and\nmodel adapts its attention to artifacts introduced by the playback–recording\nNotably, the model does not rely on isolated spectral peaks but instead\ndistortions. All ablation models follow identical training and evaluation pro-\nFour model variants are evaluated:\n• Full Model: The complete proposed framework with multi-resolution\nModel Variant\nFull Model\nFull Model\nFull Model\ning cues are distorted by channel effects. Second, restricting the model to\nThis study investigates the role of explicit cross-resolution modeling in\nworld datasets consistently demonstrate that modeling interactions among\nventional single-resolution or implicitly fused approaches.\nern TTS and VC systems. More importantly, the method maintains strong\nthe proposed model does not rely on isolated spectral peaks or narrow fre-\nthat (i) explicitly models interactions among multiple spectral resolutions\ndedicated regularization objective. This combination encourages the model\nability and replay effects. Furthermore, the proposed architecture achieves\nthe model demonstrates that robustness and efficiency need not be mutu-\nmethod is its consistent performance across a wide spectrum of spoofing\nprovides confidence that the model learns meaningful and generalizable cues\nFinally, although the model generalizes well across the evaluated datasets,\nthe rapidly evolving nature of generative speech models may introduce new\nthrough cross-scale attention and consistency learning. By jointly modeling\nmultiple spectral resolutions, the proposed approach addresses a key limi-\naudio deepfake benchmark demonstrate that the proposed method delivers\nefficient model design.\nmodel to dynamically integrate coarse temporal context with fine-grained\n[1] O. A. Shaaban, R. Yildirim, A. A. Alguttar, Audio deepfake approaches,\nFrom generative adversarial networks to diffusion models, International\nlearning methods on deepfake audio detection for digital investigation,\naudio detection with a hybrid mfcc and spectral contrast approach,\ndeep learning models, IEEE Access (2025).\nusing deep learning methods: A systematic and comprehensive review,\napproaches for deepfake content detection, Expert Systems 41 (8) (2024)\ntechniques: architecture, detection and datasets, IEEE Access (2024).\ntime-frequency approaches: coding, classification, fingerprinting, and\n\n\n=== EXPERIMENT ===\nmarks: ASVspoof 2019 (LA and PA), the Fake-or-Real (FoR) dataset, and\nthe In-the-Wild Audio Deepfake dataset under a speaker-disjoint proto-\ncelerated progress by standardizing evaluation protocols and datasets. On\ncurated datasets frequently degrade when exposed to replay attacks, channel\nmany existing detectors implicitly rely on dataset-specific artifacts or nar-\nevaluated on a diverse set of benchmark datasets spanning controlled syn-\ncluding speaker-disjoint evaluation protocols. Experimental results demon-\nIn-the-Wild dataset. Low-, mid-, and high-resolution representations reveal complemen-\nfrom the In-the-Wild dataset. While some synthesis artifacts are more pro-\n2.2. Datasets\nfake detection datasets that collectively cover controlled, rerecorded, and\nreal-world conditions. These datasets are widely used in the literature and\nprovide complementary evaluation scenarios for assessing robustness and\n2.2.1. ASVspoof 2019 Dataset\nThe ASVspoof 2019 dataset [40] is a widely used benchmark for audio\nThis subset represents a controlled experimental setting in which spoofing\nThe dataset is partitioned into training, development, and evaluation\nsubsets following a predefined protocol. In this work, experiments are con-\nTable 1: Summary of ASVspoof 2019 LA and PA dataset statistics used in this study\ning established evaluation practices in the literature.\ndataset statistics used in this study is provided in Table 1.\n2.2.2. Fake-or-Real (FoR) Dataset\nThe Fake-or-Real (FoR) dataset [41] is a",
  "2601.19786": "=== METHOD ===\ntext-based Large Language Models (LLMs), dis-\nherently compatible with LLM architectures. As a\nwith an accent ABX method we proposed that eval-\ntation model, or using ASR supervison) discards\nrior performance to existing approaches.\nof-the-art (SOTA) models utilise a hierarchical ap-\ninternal ASR model encoder; how ASR pretraining\nDSRTs along with powerful LLM architectures\nThe ABX error rate is a distance-based, model-\nMethod\nmodel using HiFiGAN (Polyak et al., 2021). To\nspeech models with DSRTs from a source speaker\nWe choose three speech representation models\nand Encoder-Decoder architectures, respectively.\ntise the speech representations. The model consists\nof discrete tokens. The model is trained to recon-\nto gradually update the codebook. For model de-\nHiFiGAN model for each DSRT configuration, fol-\nWe train unit-to-speech models on data cover-\nmodel on DSRTs from a source speaker and a tar-\nSpeaker Verification (SV) models are used to cal-\nWe choose ABX as a model-free method to es-\n2025b), a supervised accent identification model\nels share the same Encoder architecture (24 Trans-\nlarger-scale pretraining data/model to future work.\nunit-to-speech HiFiGAN models8 for 100,000 steps\nwhen transfer learned from SV models (Zuluaga-\nrecognition model (Churchwell et al., 2024). It is\nfect than changing representation models or layers.\ntenuated, encouraging the model to guess accent\nmodels are needed to better reveal subtle accent in-\nmodels. Third, our experiments are limited to En-\nglish SSL models and the VCTK dataset, which\non additional large-scale model training.\navailable datasets and pretrained models for re-\nmodels are used under the following licenses: Cre-\nA Language Modeling Approach to Audio Genera-\nand Masked Language Modeling for Self-Supervised\nguage Models: A Survey. In Proceedings of the\nEffect Gradual? A Computational Modelling Ap-\ntext Foundation Model for Real-time Dialogue. arXiv\nEnglish (US) ARPA Acoustic Model\nTechnical report, https://mfa-models.\nEnglish(US)ARPAacousticmodelv3_0_0.html.\nspoken dialogue language modeling. Transactions\nRepresentation Model.\nTokenizer for Speech Language Models.\ning Large Acoustic Pretrained Models for Accent\ncessfully captured by our data-driven method. First,\n\n\n=== EXPERIMENT ===\nin DSRTs. We propose a unified evaluation\nevaluation and provides practical guidance for\nis largely overlooked in the design, evaluation, and\nbenchmarks or evaluation settings that consider\nExisting DSRT evaluation frameworks focus on\n• Evaluation pipeline and proposed DSRTs will\nABX Evaluation\nagnostic evaluation metric, assessing whether the\nusing a range of ABX setups (see Section 3.3).\nAmerican English datasets, with evaluation focus-\nduring evaluation, following Zhong et al. (2025a),\ncontrastive evaluations. For detailed triplet selec-\nExperiments\nEvaluation Dataset\nFor objective evaluation of the converted speech,\nprevious DSRT evaluation frameworks to reflect\n(a) Cross-accent VC evaluation results for information recoverability.\n(b) ABX evaluation results for information accessibility.\nset is used for ABX evaluation to prevent speaker\nin HiFiGAN experiments for simplicity. Accent\nTest experiments used all utterances in the split,\nevaluation when assessing DSRTs for accent gen-\nFuture work will extend our evaluation to ZS-TTS\ntations and datasets involving L2 English accents\nin VC. We haven’t included subjective evaluation\nreleasing derived datasets. All data and pre-trained\nrin Richmond. 2025a. Pairwise Evaluation of Ac-\nFull Evaluation Results\nevaluation results across codebook sizes are also\n\n\n=== RESULTS ===\nfrom a variety of speech encoders. Our results\nreduction. Based on these findings, we propose\nresult, they are rapidly becoming established as a\nobserve the following key findings: (1) Accent in-\n• Based on our findings, we propose quantisa-\n2025). Motivated by such findings, many state-\nnot only contradicts the findings of Yeh and Tang\ntion, evidenced by improved performance in AID\n(See Figure 4 in Appendix D for additional results from cross-accent VC.)\na practical lower bound (chance-level performance)\n(See Figure 5 in Appendix D for additional results from cross-accent VC and ABX.)\nResults\nTable 1: Results of proposed content and content-accent tokens, compared with content and content-style tokens\nfrom Vevo (Zhang et al., 2025). Differences in DSRT design choices are marked in red, with best performances\nBased on the above findings, we highlight the limi-\naccent tokens achieve better performance than\nFinally, our findings provide potential explana-\non the results using this framework on HuBERT-\ning with subjective listening tests results.\nOur results further suggest that accent control can\ning Performance Bias in ASR Systems: A Study on\n20%, and we reported results at 10%, which is the\nlow accent accessibility results shown in Figure 4\nDue to page limits, we report additional results\n\n\n=== CONCLUSION ===\nDiscussion\nConclusion\nuseful discussion.",
  "2601.20319": "=== METHOD ===\nsynthesized from three emotional TTS models and find that\ning across models. Based on these insights, we introduce two\nconventional ASR systems, underscoring the need for models\nemotional speech recognition method that detects emotions\nOne practical approach to addressing this challenge is\nthese, controllable text-to-speech (TTS) models have emerged\nRecent advances in large language model-based TTS sys-\ndifferent TTS models influence these error patterns has not\nby an ASR model. Based on these observations, we design\ncontrollable TTS models. An overview of the analysis work-\nmodels to generate the speech data. For each model, the\ncrete speech tokens. Its architecture comprises three compo-\nmodel, and a chunk-aware flow-matching module. The system\nmodel with 24 Transformer layers and approximately 0.49\nTTS model that enables fine-grained emotional synthesis\npretrained EmoVoice 1.5B model to synthesize speech from\nTTS model that uses masked generative transformers and\nmodeling. Unlike LLM-based models, MaskGCT is built on a\nLLaMA-style Transformer architecture and generates speech\nments, we use the pretrained MaskGCT model released by the\nB. Synthesized Data Analysis Methods\n1) TTS Correctness Analysis: One standard approach to\nanalysis. This Speech-LLM architecture integrates Whisper-\nthe language model, jointly modeling acoustic and linguistic\nploy an emotion regression model that estimates Arousal (Act),\nThe regressor is implemented as a multitask model based on\nthe WavLM architecture, following the configuration used in\ntion. To prioritize acoustic over lexical features, the model is\nin training set using the NISQA model, a non-intrusive speech\nacross all models are narrow with limited variability, although\nthis combined approach as TTS-EMO-G in our experiments.\nB. ASR Model Fine-tuning Details\nwe fine-tune the pretrained Qwen2-audio-7B model using\nframework for tuning large language model-based systems. We\nadopt a supervised fine-tuning approach in which only a small\nthe rest of the model remains frozen. Specifically, we allow\nand a maximum gradient norm of 1. The model is trained\nimpact on the pretrained Qwen2-audio model without any\nsiveness, introducing additional complexity for ASR models.\nsynthesized test sets from each emotional TTS model, as well\nmodels,\nto real emotional speech, we evaluate the ASR models fine-\nsection. Specifically, we assess models trained using the three\nus to determine whether models trained solely on synthetic\nidentify which emotional regions benefit from our approach.\nour fine-tuned ASR models. This dataset contains emotion-\n2) IEMOCAP: We also evaluate model performance on\nacross all benchmarks. Notably, our model was not trained\nAmong the three TTS methods, MaskGCT achieves the best\ntrained model and the best fine-tuned system. Red indicates\nTTS models shows that emotional variability mainly causes\nThis approach relies on well-trained emotional TTS systems\nmodel adaptation for these high-risk words.\n[4] P. Laukka, P. Juslin, and R. Bresin, “A dimensional approach to vocal\ntional speech recognition method based on word transcription,” Sensors,\nsynthesis with large language models,” arXiv preprint arXiv:2412.10117,\ntext-to-speech model with freestyle text prompting,” arXiv preprint\nrecognition combined with acoustic-to-word asr model,” in Interspeech\nself-attention model for multidimensional speech quality prediction with\n\n\n=== EXPERIMENT ===\ndatasets without noticeable degradation on clean LibriSpeech\nanalytical experiments to examine how synthetic emotional\nspeech datasets. Through this analysis, we aim to clarify\nresulting dataset comprises 30,000 utterances for training,\nbillion parameters. In our experiments, we use the instruction-\ndataset constructed using GPT-4o and GPT-4o-audio, allowing\nmore nuanced emotional control. In our setup, we use the\ndataset [15] that shares the target emotion (e.g., Angry for\n1https://huggingface.co/datasets/yhaha/EmoVoice-DB\ntrained on the BIIC-Podcast dataset [22], a 157-hour Chinese\nTable I shows that all emotional TTS datasets lead to sig-\nthe synthesized datasets, CosyVoice2 yields the lowest WER\nacross the synthesized dataset.\noriginal synthesized dataset (Vanilla), these strategies yield\nthe TTS-EMO-G setting. Evaluations are conducted on the\nIV. REAL EMOTION DATA EVALUATION\nthree benchmark datasets: MSP Podcast Test1, Test2, and\nIEMOCAP. These datasets are used only for evaluation, and no\nreal emotional speech is seen during training. This setup allows\nA. Real Speech Emotion Datasets\non a scale from 1 to 7. Following the official evaluation\ndataset consisting of dyadic conversations recorded in a con-\nthis study, we use the entire corpus for evaluation, comprising\n0.57% on each dataset, respectively. Among the three strate-\non any of these datasets and had no exposure to their emo-\ndatasets, particularly in highly expressive regions, while main-\ntransfer for voice conversion with a new emotional speech dataset,” in\n“Odyssey2024 - speech emotion recognition challenge: Dataset, baseline\ncrowdsourced datasets,” in Interspeech 2021, 2021, pp. 2127–2131.\ndyadic motion capture database,” Language resources and evaluation,\n\n\n=== RESULTS ===\ngenerative strategies affect ASR performance. We analyze speech\nResults show consistent WER improvements on real emotional\nparticularly for expressive speech. These findings highlight the\nthat emotional speech negatively impacts the performance\nlanguage [7]. These findings suggest that emotional speech\nimprove ASR performance for dysarthric speech [10], suggest-\nhelp improve ASR performance under affective conditions.\nspeech affects ASR performance. Starting from synthesized\nunder emotional variability. Our main findings are:\nemotional speech without harming neutral performance.\nability on ASR performance, we control two key aspects in our\nart performance on conversational English benchmarks such as\nASR performance under affective conditions [20].\nC. Result and Discussion\nsplits. This supports prior findings that emotional utterances\nresults confirm that synthesized emotional speech presents\nwith MaskGCT achieving particularly strong results (4.40).\nThese findings suggest that the elevated WERs are unlikely\nThese results suggest that only a portion of synthesized\na substantial portion of utterances. These findings highlight the\nassess their impact on ASR performance.\nMotivated by the findings in the previous section, we de-\nretained only if it results in a higher number of substitutions\nC. Results and Discussion\nfine-tuning. Table III presents the WER results on subsets\nsplits. Notably, TTS-EMO-G consistently results in higher\nTable IV further reports the results after fine-tuning under\nwhen tested on the MaskGCT set. Crucially, performance\nobserved for CosyVoice2. These results demonstrate that the\ngeneral ASR performance.\nachieves the best overall performance, with an average WER\nconditions. These findings highlight the effectiveness of the\nB. Result and Discussion\nTable V reports the performance of all generative strategies\ngies, TTS-EMO-G consistently achieves the best performance\nvariability rather than lexical differences. These findings con-\nperformance on IEMOCAP, consistent with earlier findings\nshown in Table I) lead to better performance. As an additional\nTo further understand where performance gains occur, we\nacross the same three 2D planes to visualize ASR performance\nperformance under emotionally expressive conditions.\nspeech on ASR performance. Analysis of three emotional\ntaining performance on neutral speech. These results demon-\nperformance,” in Proceedings of the 2019 Conference of the North\nframework, and results,” in Odyssey 2024: The Speaker and Language\n[26] A. Paszke, “Pytorch: An imperative style, high-performance deep learn-\n\n\n=== CONCLUSION ===\nV. DISCUSSION AND CONCLUSION\nmultilingual and low-resource scenarios. Future work could",
  "2602.00560": "=== METHOD ===\ncontext. Prevalent methods operating in the acoustic space suffer\nPreserve Acoustics’. Our approach relies on two core compo-\nOptimization. By leveraging a pre-trained Text-to-Speech model\ndemonstrate that our method significantly outperforms state-of-\n(NAR) approaches [3], [8]–[10] offered inference stability\nbut often failed to model long-range dependencies, result-\nsive (AR) models based on Neural Codec Language Mod-\n(SOTA) naturalness. However, these methods typically operate\nthat text-based speech editing is best approached by modify-\nModels (LLMs) [19]–[21], with initial explorations in speech\nText-Speech Language Model\nPretrained Text-to-Speech Model\nFig. 1. The overall framework of our proposed method. The pipeline consists of two stages: (1) Structural Foundations (Left), where we employ a semantic-\nmechanism using a pre-trained Text-to-Speech (TTS) model\nmodel captures the distribution of natural speech, we utilize the\narchitecture to decouple content editing from acoustic re-\nlize a pre-trained TTS model as a consistency critic within a\n‚ Empirical evaluations demonstrate that our method sig-\nII. METHOD\nA. Overall architecture\nin Figure 1. Our approach orchestrates this principle through\nleverage a pre-trained TTS model as an implicit critic to\nAR approaches lies in the inherent entanglement within acous-\nartifacts. To address this, we adopt a decoupled architecture\nWe employ a decoder-only transformer as the policy model\nThis formulation ensures structural integrity: the model\nstrategy. We leverage the pre-trained TTS model itself as an\nspeech editing model, we compute the average log-likelihood\nunder the frozen TTS reference model πtts:\nTheoretical Justification: The pre-trained TTS model πtts\nicy’s generation and the TTS model’s prior:\nsilence or simple repetitions), the model may converge to\nwhere WER is computed by an ASR model on the waveform\nrepetitive or unintelligible outputs, forcing the model to bal-\nModel\nsentative models covering diverse editing paradigms:\n‚ FluentSpeech [28]: A NAR diffusion-based model generat-\nwas computed by the pre-trained TTS model CosyVoice3 [24],\nModel\nIntelligibility and Robustness. Our method, based on seman-\nfrom acoustic rendering, which simplifies the modeling task.\n‚ Insertion: AR methods generally outperform NAR meth-\nhighest WER. As NAR models rely on mask prediction, the\nAR models (e.g., VoiceCraft), which suffer from severe\nConversely, NAR models perform better by simply predict-\ning silence. Our method achieves the best performance;\n‚ Substitution: While NAR methods excel here due to similar\ndurations between original and edited text, our method still\nSpeaker Similarity and Perceptual Quality. Our method\nof AR-based approaches over diffusion-based NAR methods\nRegarding DNSMOS and Subjective MOS, our method\n‚ Our method significantly outperforms both the acoustic AR\nmethod—especially with GRPO alignment—as the most\nImpact on Intelligibility. Our method consistently achieves\nour method exhibits a much slower rate of degradation. Even at\ndemonstrating robust long-context modeling.\nImpact on Speaker Similarity. Our method maintains the\nNAR method shows a drastic decline in similarity as duration\ndegrades, albeit more slowly. Our method, however, maintains\ndirectly from our decoupled architecture: the Flow Matching\nImpact on Naturalness (DNSMOS). Our method consis-\nwidens significantly. While baseline methods and our non-\naligned model show little improvement or plateau, Ours (with\nTTS model to approximate the natural speech distribution, RL\noptimization guides the model to generate coherent prosody\nmethod significantly outperforms SOTA AR and NAR base-\nand the GRPO alignment method to freeform speech editing.\nspeech editing method for text-based speech editing, one-shot tts and\nbased speech editing by modeling multi-scale acoustic and prosody\nediting,” in Proceedings of the 2025 Conference on Empirical Methods\net al., “Uniaudio: An audio foundation model toward universal audio\nhigh-quality speech editing model without hallucinations,”\nand Matthew Le, “Flow matching for generative modeling,” in The\nlanguage models,” arXiv preprint arXiv:2402.03300, 2024.\n“Seed-tts: A family of high-quality versatile speech generation models,”\nspeech editing with context-aware diffusion models,”\nunderstanding and generation foundation models for natural interaction\n\n\n=== EXPERIMENT ===\ntoken sequence with the original context. Empirical evaluations\nIII. EXPERIMENTS\nA. Experimental Settings\nDatasets. We conduct training on the Libriheavy [25]\ndataset, a large-scale corpus comprising approximately 50,000\nhours of English speech from LibriVox. For evaluation, we\nEvaluation Metrics. We employ both objective and subjective\nmetrics for a comprehensive evaluation: (1) WER: Calculated\nare set to 0.2. Our experiments were conducted on 8 NVIDIA\nROBUSTNESS EVALUATION ON THE SUBSET OF SEED-TTS TEST SET\nsetup (where mask duration equals generated text duration),\nExtensive evaluations on two benchmarks demonstrate that our\n\n\n=== RESULTS ===\nfrequently resulting in hallucinations or boundary artifacts.\nresulting in perceptual discontinuity. To achieve impercepti-\nbaseline directly from the collective performance of multiple\nPERFORMANCE COMPARISON ON THE TEXT-BASED SPEECH EDITING BENCHMARK. THE SYMBOL ’˛’ INDICATES THAT THE RESULTS ARE CITED\nDIRECTLY FROM THE ORIGINAL PAPER [14]. RED INDICATES THE BEST RESULT, AND BLUE INDICATES THE SECOND BEST.\nPerformance\nperformance. The first, adapted from the Ming-Freeform-\nC. Main Results Analysis\nTable I presents the performance comparison on the first\nTHE BEST RESULT, AND BLUE INDICATES THE SECOND BEST.\nperformance. The NAR baseline (FluentSpeech) exhibits the\nresults\nTable II illustrates the performance stability as the masked\nthe main results, GRPO has a negligible effect on SIM. Among\nhigh similarity with minimal decay. This robustness results\nin Findings of\n\n\n=== CONCLUSION ===\nIV. CONCLUSION",
  "2601.22873": "=== METHOD ===\ncluding large language model (LLM)-based designs, rely on scaling\nto model emotion-specific latent characteristics. To address this gap,\ngenerative modeling driven by prosodic cues [8] or natural-language\nthese approaches typically scale a fixed emotion embedding and\nshare all model parameters. This limits interpretability and hinders\nand interpretable paradigm for controlling generative model be-\nsets—steering vectors—into a model’s latent space at inference time\ncontrol without altering or retraining the base model. The design is\nmodel-agnostic and can be seamlessly integrated into LLM-based\napproach performs explicit control by modulating emotion embed-\ncent work leverages generative modeling to guide emotional control.\nprompt-driven frameworks [9, 8, 10] use large language models\nThese existing methods either scale a fixed emotion embed-\nIn contrast, our approach\nlearns an emotion-specific steering vector that directly models a\n2.2. Activation Steering Methods for Controlled Applications\nas a lightweight and interpretable approach for controlling LLMs\ntional methods such as LM-Steer [14], Style Vectors [15] and\nrameters within the TTS model.\n3. METHODOLOGY\nproposed system first models TTS as a conditional auto-regressive\n3.1. LLM-based Problem Formulation and Modeling Setup\ntoken generation task. The model is conditioned on three sources\nModel\nand the model is trained by minimizing the negative log-likelihood\nin the model’s parameters, limiting interpretability and direct con-\ncontrollable, without changing the core architecture or retraining the\nbase LLM-based TTS model.\nEmoSteer layer is inserted to construct our EmoShift model. For\na learning rate of 1 × 10−4, and train the model for 5 epochs. Base-\nthe proposed EmoShift model with the baselines.\nASR model [29] to the synthesized speech, the Speaker Similarity\n(SpkSIM), measured with the WavLM-Base model, and the overall\nemotion2vec model [31], which perform speech emotion recognition\nstronger ability of the TTS model to generate emotion-aware syn-\nEmo-MOS, the focus is solely on whether the model has learned to\nModel\nBase Model\ntheir base models, reported for MOS and Emo-MOS.\nduct a human evaluation using two base models—CosyVoice and\nlayer serves as the model variable. 10 listeners participate in pref-\nversions with the EmoSteer layer over their respective base models.\nmetrics and base models demonstrate that the EmoSteer layer effec-\nbased SER model. To test this hypothesis, we scale α from its train-\nthe overall emotion recognition accuracy using the same SER model.\nAs shown in Fig. 3, models with EmoSteer outperform those without\nemotional control in a parameter-efficient and architecture-agnostic\nwards controllable speech synthesis in the era of large language models:\nmodeling for conversational speech synthesis,” in ICASSP 2025 - 2025\nmodeling via spherical emotion vector for controllable emotional text-\n“Emovoice: Llm-based emotional text-to-speech model with freestyle\nlanguage models: A simple approach to controlled text generation,” in\ntivation steering: A tuning-free llm truthfulness improvement method\nlanguage models,”\nvectors for steering generative large language models,”\nmodels: Versatile steering vectors through bi-directional preference op-\nlarge language models,” 2025.\nguage models through activation steering,” in The Thirteenth Interna-\n\n\n=== EXPERIMENT ===\nfully fine-tuned baselines in objective and subjective evaluations, en-\n• Extensive objective and subjective evaluations demonstrate\nTable 1. Overall Results: Objective evaluation results of EmoShift and baselines in terms of both speech generation quality and emotion\n4. EXPERIMENTS\n4.1. Experimental Setup\nDataset: We use the English subset of the ESD dataset [28], con-\ncluding neutral as one of the emotions) based on the dataset. During\n4.2. Evaluation Metrics\nWe employ both objective and subjective evaluations to compare\nevaluation, speech generation quality is assessed using the Word\nthetic speech. For subjective evaluation, we consider the Mean\n4.3. Experimental Results\nTable 2. Subjective evaluation scores obtained from 10 listeners,\nWe also conduct a human subjective evaluation with 10 listeners,\nthe objective evaluation results in Table 1.\nmanner. Extensive evaluations indicate that EmoShift delivers con-\n\n\n=== RESULTS ===\nneutral baseline, and apply the resulting signal at the utterance [4]\nBi-directional Preference Optimization [16]; to task performance\nand per-emotion performance (except for the Angry category), but\nalso achieves comparable overall results to CosyVoice-SFT-Shift,\nTTS performance by amplifying emotion-specific offset subspaces,\nby Sad (61.24%) and Neutral (55.84%). These results indicate that\ntional speech synthesis,” in Findings of the Association for Compu-\nin Findings\ning improves task performance and safety in llms through correlation-\nfor speech emotion representation,” in Findings of the Association for\n\n\n=== CONCLUSION ===\n6. CONCLUSION\nfidelity. Future work will extend EmoShift to more emotional cate-",
  "2602.01908": "=== METHOD ===\nbased models such as LipVoicer have demonstrated impressive\nsimilarity—compared to prior approaches.\nart LipVoicer [1], our model explicitly estimates prosodic fea-\n• We introduce a novel visual-only prosody estimation method,\nour approach and uncovering a compelling correlation be-\nsilent video frames. Early works trained CNN-RNN architectures\nspeech decomposition methods [6]. Notably, recent work such as\ndiffusion-based speech generation [7]. Their method leverages both\nenhance conditional generation quality. Specifically, the model uses\na pretrained lip-reading model [10] to generate pseudo ground-truth\n(ASR) model [11] as the guiding classifier.\nHowever, most lip-to-speech models still struggle to capture\n3. METHODS\nOur model is built upon the state-of-the-art diffusion-based lip-to-\nDenoising Diffusion Probabilistic Model (DDPM) [12], where con-\nASR model to compute the guidance signal: ∇xt log p(l | xt),\nand l is the predicted text label from a pretrained lip-reading model.\nLipSody is trained using a DDPM-based architecture with CFG.\nHowever, beyond utilizing s and c, we explicitly model prosodic\nbining this with our proposed prosody modeling, the inferred noise\nLipSody lip-to-speech model, while only the pitch and energy pre-\n3.4. Model architecture\nLip Reader: Conformer-based lip-reading model architecture [15]\nVocoder: HiFi-GAN architecture [16].\nmouth-region cropping, we follow the approach in [18]. A 96×96\nwork LipVoicer [1] (WER 21.9%), evaluating our method against\nThe first is the official model reported in the original paper,\nLipVoicerrecon. Our proposed model, which incorporates explicit\nprosody modeling, is referred to as LipSody.\nFor preprocessing and the base lip-to-speech model, we follow the\nWe evaluate our models using both objective and subjective metrics.\nevaluate the model’s ability to capture prosodic information, such\nWER: Calculated using an ASR model [15] to assess the intelligi-\nABX test: Measuring subjective preference between two models,\nLipVoicerrecon shows that the reconstructed model achieves perfor-\nrectly evaluate the main objective of our proposed method. In all\ning, indicating improved modeling of frame-wise speech dynamics\nof 0.5 using a one-sample t-test, and found that our model achieved\nour method in enhancing prosody consistency while preserving\nfull LipSody model, it shows a statistically significant degradation in\nand energy modeling into a diffusion-based generation pipeline. Un-\nlike prior approaches that mainly emphasize intelligibility, LipSody\nfrom face video—for prosody modeling.\nOur method achieves significant improvements in prosody-\nmore, we found that enhanced prosody modeling also contributes to\nCatanzaro, “Diffwave: A versatile diffusion model for audio\n[8] Prafulla Dhariwal and Alexander Nichol, “Diffusion models\nmodels,” in ICASSP 2021-2021 IEEE International Confer-\nfusion probabilistic models,” Advances in neural information\nvision-language method for zero-shot video facial expression\nbased non-intrusive speech intelligibility assessment model,”\n“Rmvpe: A robust model for vocal pitch estimation in poly-\n\n\n=== EXPERIMENT ===\ntext inferred from face video. Experimental results demonstrate that\n• We conduct extensive evaluations using a diverse set of\nTable 1. Evaluation results of conventional metrics\n4. EXPERIMENTS\n4.1. Datasets\nWe used LRS3 dataset [17], which comprises 5,502 TED and TEDx\ndefined pretrain and train splits, and use the test split for evaluation,\nTable 2. Evaluation results of prosody-related metrics\nofficial LipVoicer implementation [1], including the diffusion setup\nThe objective evaluation consists of two types: (1) conventional met-\nas pitch and energy consistency. For subjective evaluation, we con-\nTable 3. Evaluation results of subjective metrics\n5.1. Evaluation with objective conventional metrics\nThe results of the conventional metric evaluation are presented\n5.2. Evaluation with objective prosody-related metrics\n5.3. Evaluation with subjective metrics\nThese subjective evaluation results reinforce the effectiveness of\nman, “Lrs3-ted: a large-scale dataset for visual speech recog-\n\n\n=== RESULTS ===\nperformance in reconstructing linguistic content, they often lack\nprimarily evaluating performance through word error rate (WER).\nLipVoicer [1] has shown strong results in speech intelligibility using\nresulting in less natural and personalized speech synthesis.\ngraded WER performance (33.9% in [4], 29.8% in [5], and 28.5% in\n5. RESULTS\nThese results demonstrate that LipSody retains high intelligibility\nstrategy does not compromise baseline performance.\nTable 2 presents the results for prosody-related metrics, which di-\nassociated with prosody. Finally, the results of Resem and Resemtv\nparametric pitch extractor. To ensure that our findings are not tied\nLipVoicer. Due to space constraints, detailed results are omitted.\nTable 4. Results according to prosody information settings\nTable 3 presents the results of the naturalness and ABX test. For\n5.4. Performance According to Prosody Information\nTable 4 compares the performance of three variants of the LipSody\nresult implies that more accurate prosody estimation not only en-\n\n\n=== CONCLUSION ===\n6. CONCLUSION",
  "2601.03403": "=== METHOD ===\nimplementation.1 Evaluation of frontier large language models (LLMs) reveals significant gaps in their ability to\nlanguage modeling, speech synthesis, and accessibility applications targeting Tigrinya-speaking communities.\n(TTS) synthesis, automatic speech recognition (ASR) language modeling, and accessibility technologies. While number\nWe also evaluate frontier large language models on this task to assess their current capabilities.\n5. Evaluation of Large Language Models\nTo assess whether current large language models (LLMs) have internalized Tigrinya number verbalization rules, we con-\nWe evaluated six frontier models from three major providers. Each model was prompted with the verbalization task in-\nresults (Table 3, Figure 1) reveal substantial deficiencies. While models achieve moderate accuracy on simple cardinals and\ncurrency, performance degrades significantly for other categories. Performance depends on the models’ familiarity with\nModel\npreprocessing component. Automatic Speech Recognition (ASR) language models benefit from expanded text corpora that\nfor Language Modeling. Similarly, the rules documented in this work can serve as structured knowledge for fine-tuning\nLimitations: (1) The LLM evaluation assumes basic support for Tigrinya by the models, but it should be noted that the\nmodel providers do not oﬀicially claim to support Tigrinya. The evaluations are indicative using a limited test set and should\nbe expanded in future work as the models improve. (2) There are regional dialects of Tigrinya in Eritrea and Ethiopia with\nTesfamariam, I. Let’s Speak Tigrinya: A Multidimensional Approach to the Teaching and Learning of Tigrinya as a For-\n\n\n=== EXPERIMENT ===\nstructed an evaluation set with 100 examples spanning six categories: cardinals (50), ordinals (15), currency (10), dates\nA. Evaluation Set\nTable 4: The evaluation set used for LLM assessment. Entries shows the input and ground truth answer(s), separated by semicolons.\nCardinal Number Evaluation Examples (50 entries)\nOrdinal Number Evaluation Examples (15 entries)\nCurrency Evaluation Examples (10 entries)\nDate Evaluation Examples (10 entries)\nTable 4: The evaluation set used for LLM assessment. Entries shows the input and ground truth answer(s), separated by semicolons.\nTime Evaluation Examples (10 entries)\nPhone Number Evaluation Examples (5 entries)\n\n\n=== RESULTS ===\nTigrinya. Strikingly, GPT-5 Mini struggled to give correct results in almost all cases within two token budget settings (2048\nrect application to teens; and (4) failure to distinguish simple vs. compound multipliers with scale words. These findings\nTable 3. Performance of LLMs on Tigrinya Number Verbalization. GPT-5 Mini runs out of max tokens (2048 & 4096) for most requests.\nFigure 1. LLM performance comparison across categories. Best overall: Opus 4.5 (65%), followed by Gemini 3 Flash (44%).\n\n\n=== CONCLUSION ===\n7. Conclusion\nfor speakers with disabilities, and reduced technological disparity for low-resource language communities. Future work",
  "2601.05329": "=== METHOD ===\nCapability from Zero-Shot Text-to-Speech Models\nediting model adapted from CosyVoice through task-specific fine-\nour 400M-parameter model achieves reliable speech editing\nlanguage model baselines but also matches the performance of\nstate-of-the-art cascade approaches. These results demonstrate\nfrom a zero-shot TTS model, yielding a novel and cost-effective\nIndex Terms—automatic speech editing, end-to-end modeling,\nmodel\nto be edited (step (iii)). Finally, zero-shot synthesis methods,\nsuch as autoregressive (AR) generative models [2], [3] or\nnon-autoregressive (NAR) diffusion-based models [4]–[6], are\nto-end models (Fig.1(b), step (i)) inherently avoid these by\nzero-shot TTS models [6], [12]–[14] now possess human-\nCOMPARISON OF DIFFERENT SPEECH EDITING MODELS. THE DASHED LINE SEPARATES PREVIOUS BASELINES FROM RECENT END-TO-END MODELS.\nMethod\nArchitecture\nspecific training and inference strategies, these models could\nzero-shot TTS models. As an instantiation of this strategy, we\na model from scratch. Our contributions are threefold:\n• We extend AR+NAR zero-shot TTS models, exempli-\ning CosyEdit, a truly end-to-end speech editing model\nthe RealEdit [2] benchmark demonstrate that our model\nA. Non-Autoregressive Speech Editing Models\nNAR speech editing models formulate speech editing as\nacoustic feature space, and the model reconstructs it based on\nsolvers to achieve efficient, high-quality infilling. NAR models\nB. Autoregressive Speech Editing Models\nAR speech editing models formulate speech editing as\nbidirectional acoustic context. AR models naturally capture\ntemporal structure and implicitly model output duration, which\nC. Speech Language Model-Based Speech Editing Models\nend speech language models (SLMs), which are increasingly\nreinforcement learning approaches, while also demonstrating\ncurrent SLM-based editing approaches may not yet match\nthe stability of cascade systems, their end-to-end architecture\nmodels show greater potential for general speech editing tasks.\nIII. PROPOSED APPROACH\nlanguage model (LLM), and a NAR conditional flow-matching\n(CFM) model. We retain the original text encoder and S3\ndecoding in the reasoning stage. (c) provides an enlarged view of our flow matching model conditioning on a speaker embedding v, semantic tokens µZ\nA. Large Language Model for Speech Editing\nUnlike conventional cascade speech editing approaches that\nAs illustrated in Fig. 2(b), we adapt the TTS model to the\nand the original speech. Specifically, the model is trained to\naligned regions. Accordingly, we design the LLM to model\na pretrained speaker-verification model. The text encoding\nfor the AR token language model is:\nshot TTS models are typically optimized for global timbre\nmodel with a reference-guided design (GOT-CFM). Specifi-\ndition the model on both the fully revealed original mel-\ndistinct input sequences for the token language model in\nZero-shot in-context training conditions the model only on\nmodel to predict the target speech tokens. This design serves\nfrom the original speech, which assist modeling and prediction\neasily cause the model to under-attend to the sparse, localized\nlanguage model proceeds to autoregressively predict target\ning the AR models VoiceCraft and SSR-Speech and the\nNAR model FluentSpeech, as well as end-to-end approaches\nto-end models, we apply an alignment-based postprocessing\nstep and report the replaced results for all end-to-end models\nMethod\nPERFORMANCE COMPARISON OF THE END-TO-END SPEECH EDITING MODEL AFTER REPLACEMENT OPERATIONS.\nMethod (Replaced)\nMethod\ngenerated and ground-truth speech. For end-to-end models,\nand the flow model were trained for 16 epochs, with learning\nto-end models on RealEdit benchmark. CosyEdit surpasses\nall baseline methods on both WER and EMOS metrics,\nlevels close to the best-performing cascade approaches. For\nand after editing among end-to-end models, indicating that the\nmodels’ ability to preserve overall consistency. As shown\nin Table III, CosyEdit outperforms other end-to-end models\nsemantic modeling remains largely unchanged, but prosody is\ntask-specific flow training forces the model to shift from\nto modeling richer acoustic details in in-the-wild recordings\nediting model that eliminates external alignment modules and\nscale speech language models from scratch, we introduce\napplicable to AR+NAR zero-shot TTS models, enabling ef-\ndiffusion models,” in Findings of the Association for Computational\n[10] L.-C.-T. Xiaomi, “Mimo-audio: Audio language models are few-shot\nH. Wang, J. Li et al., “Neural codec language models are zero-shot text\nof spoken language models: A comprehensive survey,” arXiv preprint\native models with minibatch optimal transport,” in ICML Workshop on\n\n\n=== EXPERIMENT ===\nhours of supervised data from our curated GigaEdit dataset,\nperformance. Experiments on the RealEdit benchmark indicate\nTraining Dataset\npervised speech editing training datasets from existing\nediting dataset derived from GigaSpeech [8].\n• Comprehensive subjective and objective evaluations on\n(a) Four tasks for constructing the GigaEdit dataset \nFig. 2. (a) is an example of four editing tasks for constructing the speech editing training dataset GigaEdit. (b) is a schematic diagram of CosyEdit.\nIV. EXPERIMENTS\nA. GigaEdit Dataset\ning datasets covering insertion, deletion, and substitution sub-\ntasks. Using this procedure, we construct the GigaEdit dataset\ndataset to simulate real-world editing conditions.\nC. Metrics & Experiment Settings\ndataset introduced in VoiceCraft [2]. Objective metrics include\nFor subjective evaluation, we randomly sample 10 examples\nWe trained CosyEdit on the GigaEdit dataset at a 16 kHz\n2,000 and 2,500. For inference in the ablation experiments,\nD. Experimental Results\ntuned on our curated GigaEdit dataset with only 250 hours\nopen-source all code and datasets to support future research\n\n\n=== RESULTS ===\ndelivers strong performance in overall editing quality, pre-\nresulting shortened speech and transcript serve as the original\nRESULTS FOR SPEECH EDITING ON REALEDIT. * INDICATES RATINGS BASED ON SPEECH INTELLIGIBILITY ONLY.\nWe evaluate speech editing performance on the RealEdit\nseveral traditional cascade systems, reaching performance\nthat Step-Audio-EditX exhibits large performance variations\nwhereas CosyEdit maintains relatively stable performance\nThe results of the ablation study are shown in Table IV.\nresulting in lower MCD but higher WER. Adopting one-\n\n\n=== CONCLUSION ===\nV. CONCLUSIONS\non watermarking and speech forgery detection. Future work",
  "2601.07064": "=== METHOD ===\ning. This requires methods that move beyond\nmodels (SFMs) with graph-based modeling and\nvoice conversion (VC) models, synthetic speech\ndriven by diffusion-based models, expressive TTS\nniques. State-of-the-art models such as VALL-E\nexpressiveness. These models, often built upon\nformers (Li et al., 2019), state-space models (Gu\nwhere the generation model is unknown or unseen\nspeech detection, most existing methods frame\nmost existing methods fail to generalize when faced\nmodels, exhibiting significant performance degra-\nsource TTS model and detect speech from unseen\nrange of speech foundation models (SFMs), and\nhypothesize that combining relational modeling via\nand unseen generator detection. This approach is\ntwo components: GNNs model class-level inter-\nhybrid GNN–KNN approach for source tracing and\n(GNNs) for relational modeling with k-\nModels\nmodels (SFMs) used to extract utterance-level em-\nspeaker recognition models. Both are based on\ndard x-vector architecture with Res2Net modules\nFigure 1: Proposed framework: SIGNAL. The model extracts representations, followed by parallel reasoning via a\nwith WavLM trained using masked speech model-\nthree models have similar parameter sizes: WavLM\nford et al., 2023), a multilingual ASR model trained\ntransformer-decoder architecture and performs ro-\nand Tan, 2024), a state space model trained on Au-\nthe final hidden state of each frozen model. The\nModeling Pipeline\nIn this section, we detail the modeling pipeline\nworks (CNN) as downstream models for individual\nrepresentation-based modeling. Further, we pro-\nFigure-1 illustrates the overall architecture.\nIndividual Representation Modeling\noutput layer. The FCN model uses the same dense\nthetic speech. The architecture of SIGNAL is shown\nModel (SFM), yielding a fixed-length utterance\nnode, allowing the model to assess similar-\nTo estimate the model’s uncertainty in attribution,\nsuggests the model is uncertain and attention was\nprimary model selection.\ndesigned to evaluate models in both source trac-\ntrain/dev/test splits provided by DiffSSD. Models\ntion, ensuring methodological consistency across\nTable 2: Performance comparison across different Pretrained Models (PTMs) using GNN, KNN, and their combina-\negories of methods:\n(e.g., KNN-only); and (iii) Unified models, which\nhances attribution performance by modeling class-\nEER: 14.78%. Even smaller models like Mamba-\nTable 3: Performance of various Pretrained Models\nclassifiers. Models are trained on DiffSSD and directly\nspeech detection depends on more than just model\nsize or architecture—it requires the right pairing\ning the GNN’s effectiveness in modeling inter-class\nincluding diverse generative models and musi-\nsetting—models trained on DiffSSD are directly\nshows that our hybrid SIGNAL architecture signif-\nGNN+KNN hybrid architecture, we perform an ab-\nMamba-B. As described earlier, our model applies\nmodeling. Building on this insight, we introduce\nbased relational modeling with instance-level\noverlooked potential of graph-enhanced modeling\nmodeling of unseen generators is a promising di-\nproposing methods to detect and attribute artifi-\nsequence modeling with selective state spaces. In\nFirst Conference on Language Modeling.\ncodec and diffusion models. In Proceedings of the\nfusion model for audio synthesis. In International\nbilistic approach for characterizing speech synthe-\nModels. In Interspeech 2025, pages 1673–1677.\nclassification token aggregation methods. In ICASSP\n\n\n=== EXPERIMENT ===\nate SIGNAL using the DiffSSD dataset, which\nhypothesis, we perform extensive evaluations us-\nVoxCeleb1+2 datasets. ECAPA extends the stan-\nExperiments\nBenchmark Dataset\nthetic Speech Dataset (DiffSSD) (Bhagtani et al.,\nreal speech samples. This dataset is specifically\nfiltered to avoid repetition. The dataset is divided\nports two types of evaluation—closed-set, where\nEvaluation Metrics\nevaluation framework.\nExperimental Results\nCNN setup, it achieves ACC: 83.27%, F1: 82.63%,\nT show noticeable gains under this hybrid setup,\n(PTMs) on the SingFake dataset using FCN and CNN\nAdditional Experiments: To evaluate the gener-\nthetic speech, we experiment on SingFake (Zang\net al., 2024), a benchmark dataset for singing voice\ning EER to 10.38% with the hybrid setup. These\ndatasets using the best-performing representation,\nTable 4: Performance of different PTMs on the SingFake dataset using KNN, GNN, and the proposed GNN+KNN\non four evaluation splits: DiffSSD ID, DiffSSD\nFirst, our evaluation is conducted on two publicly\navailable datasets (DiffSSD and SingFake), and no\ndiffusion-based dataset for speech forensics.\n\n\n=== RESULTS ===\nbenchmark. Our results show that SIGNAL con-\nsistently improves performance across both\nering especially strong results. To the best of\nperformance on both DiffSSD and SingFake\nand SE blocks, yielding improved performance\nresulting embedding dimensions are: 512 for Whis-\nTable 1: Performance metrics (ACC, F1, EER) across different PTMs under FCN and CNN backbones. Top-\napply early stopping based on dev performance.\nTo evaluate the performance of our proposed frame-\nbalanced view of performance in class-imbalanced\ntion. The Blue gradient indicates performance from highest to lowest. Table 4 uses the same scheme.\nTable 1 presents performance using FCN and CNN\nwav2vec 2.0 show competitive performance in seen\nrelatively lower results, reflecting the limitations\nfindings indicate that while CNN-based classifiers\nframework—achieves the best performance across\nset attribution performance.\nachieve moderate performance, with Mamba-B at-\ntaining the best results among them (F1: 82.63%,\nicantly enhances performance across all PTMs.\nfindings affirm the robustness and transferability\nThe results clearly indicate that τ = 0.5 offers\nthe framework achieves strong performance, its\n\n\n=== CONCLUSION ===\nConclusion\nLimitations and Future Work\nrection for future work.\ntions—is an important direction for future work.\nimportant direction for future work.",
  "2601.07367": "=== METHOD ===\nModels (ALMs), development and integration of multi-modal\nIndex Terms—Multi-modal Agents, Voice-to-voice model, Text\nmodels [3], [4]. However, these function in a broad scope in\nnot assessed in these approaches. The VoiceAssistant-Eval\nII. METHODOLOGY\nThe automated evaluation scheme models a conversation\n• The query/motive of the user to model the conversation\nB. Architecture\nconversation if supported by the agent’s architecture 1a. This\narchitecture experimented in this work.\ndifferent architectures. Hence, it becomes important to evaluate\nInteraction Models, 2025.\n[10] i-LAVA: Insights on Low Latency Voice-2-Voice Architecture for\npayment details or try a different payment method.\n\n\n=== EXPERIMENT ===\nthe particular attention needed towards evaluation of agents in\nthe scope of evaluation. However, the benchmark moves in\nalongside automated evaluation for the other metrics. A tester\nC. Evaluation\nGround-Truth for evaluations. The speech transcriptions by\nE. Demo Setup\nTo provide an interactive experience of the agent evaluation\nthe users may interact with the agent and view the evaluations\nin real-time. The Demo setup would comprise of two monitors\nDifferent evaluation metrics viz. Mean Opinion Score\npresents a framework to handle the multi-faceted evaluations\nTABLE I: Evaluation of a cascading RAG based Shopping Agent under different modalities, (see Appendix V)\nEvaluation Summary: The agent followed the general structure of the conversation but made\nEvaluation Summary: The agent was polite and prompt, asking for the necessary information and\nEvaluation Summary: The agent handled the query efficiently and provided the necessary\nEvaluation Summary: The agent handled the conversation efficiently and provided all necessary\nEvaluation Summary: The agent provided accurate and helpful information, maintained a polite\nEvaluation Summary: The agent provided accurate and complete information, maintained a polite\nEvaluation Summary: The Implementation Transcript transcript is highly aligned with the ground-\nEvaluation Summary: The Implementation Transcript conversation is concise, polite, and provides\nEvaluation Summary: The Implementation Transcript conversation is strong in terms of accuracy,\nEvaluation Summary: The agent provided accurate and complete information regarding the return\nEvaluation Summary: The Implementation Transcript conversation is well-aligned with the ground-\nEvaluation Summary: The agent handled the conversation well, providing accurate and complete\n\n\n=== RESULTS ===\nevaluating performance of ALMs and cascading voice-to-voice\nsoning and semantic to evaluate the performance of multi-\n• Accuracy: Evaluates ASR and TTS performance\n• Vocal Quality: Evaluates the performance of agent’s TTS\nIII. RESULTS\nthe performance end-to-end while also focusing on the inter-\npincode 27368’, and the location link is missing. These issues result in incomplete and\n\n\n=== CONCLUSION ===\nIV. CONCLUSION\nFuture Work",
  "2601.08450": "=== METHOD ===\nyet generation order is a modelling choice. We investigate decoding\ndecoding, including the dominating left-to-right approach, is subop-\nIndex Terms— speech synthesis, discrete diffusion model,\nrecent approaches that model discretised acoustic features with a lan-\nguage model (e.g., [3, 4, 5]). In these systems, speech is generated\nFrom a modelling perspective, however, left-to-right generation\nboth past and future phones. Even when the model is conditioned\nViewing decoding order as a modelling choice, consider an au-\ncally. In principle, for fixed orders, separate autoregressive models\nThe masked diffusion model (MDM) [6, 7, 8] is an interesting,\nders σ ∈ST . During training, the model predicts randomly masked\nas a critical modelling choice.\n2. METHOD\nmany models impose a fixed schedule (e.g., left-to-right, one by\none), the Masked Diffusion Model (MDM) predicts probabilities for\nconstrain the model to generate one position at a time (k = 1 for\nconventional autoregressive models, as in [7].\nm⊙y, the model outputs the parameters of a categorical distribution\nmodel learns to reconstruct masked frames yσ(≥t) from the ob-\nwhere Q denotes the number of quantisation bins. Since the model\ntion 4.3), we observe that the model tends to decode contiguous\ndence score (see Eq. (5)), approximated here by the model’s pre-\nGrad-TTS [11]. The model architecture, based on Grad-TTS[11],\nimate l2r, while larger values approach the default strategy.\nWe re-train Grad-TTS on full utterances, since the original model is\nachieve the highest UTMOS among all models but suffer from\n5.1. Combining diffusion and autoregressive models\nto make diffusion models semi-autoregressive. For example, [14]\nuses a block-based approach with left-to-right autoregression across\nblocks and parallel prediction within blocks. Similar approaches\nhave been explored in speech synthesis, notably in models such as\ngrate the iterative refinement of diffusion models with autoregressive\nfor downstream modelling. In our setting, such per-frame tokenisa-\nquency correlations are not modelled. Future work could reduce the\nmodels [3, 4, 5]. Quantisation experiments suggest vocoders need\native model for raw audio,” CoRR, vol. abs/1609.03499, 2016.\nlanguage models are zero-shot text to speech synthesizers,”\nmasked diffusion language models,” in The Thirty-eighth An-\ntoregressive diffusion models,” 2022.\nabilistic model for text-to-speech,” in Proceedings of the 38th\nautoregressive and diffusion language models,” in The Thir-\ning diffusion autoregressive model for raw speech waveform\ntoregressive modeling for speech generation,” in Forty-second\nbased on hidden markov models,” Proceedings of the IEEE,\n\n\n=== EXPERIMENT ===\nof the generated result. Recent experiments on reasoning and vision\n3. EXPERIMENTS\n3.1. Experimental setup\nWe use LJSpeech dataset [10], a public-domain corpus containing\nmale native English speaker. We adopt the same dataset split as\n3.2. Evaluation metrics\nWe conducted preliminary experiments using a public HiFi-GAN\nFig. 2. Evaluation on quantisation levels\nFig. 3. Evaluation on orders with controlled randomness\nThe automatic evaluation metrics are reported in Figure 4. Over-\nFig. 4. Evaluation results for single-frame decoding strategies\nbut rank lower in subjective evaluations, which may be related to\nlike previous experiments, here K frames are updated simultane-\nFig. 6. Evaluation results for TopK decoding\n[10] Keith Ito and Linda Johnson, “The lj speech dataset,” https:\n//keithito.com/LJ-Speech-Dataset/, 2017.\n\n\n=== RESULTS ===\ntimal, while adaptive decoding yields better performance. Finally,\ndifferent orders expose different contexts, the resulting factorisation\n4. RESULTS\nfindings, we adopt 100-class quantisation, which simplifies training\nResults are shown in Figure 3. As randomness increases, MCD im-\nwith partial ordering. Overall, these results highlight that different\nMCD, but top1* is clearly better in log F0. MOS results shows\nously at each step. Results show that increasing K improves MCD\nscalar quantisation but maps the resulting vector to a single token ID\ntimal in speech synthesis, despite its universal adoption. Our results\nshow that adaptive orders generally yield better performance, though\n\n\n=== CONCLUSION ===\n6. CONCLUSION",
  "2601.05554": "=== METHOD ===\nand faithfulness. Inspired by the CLAP, our approach factor-\nResearchers have adopted various methods to assess how\ncorrespond to the perceptual distance. Thus, the method can-\nnot be used for comparison between TTS models.\n[6]. They let a large multimodal model assess the prompt\nmethod cannot ensure whether judgment is truly grounded\npropose an approach inspired by CLAP [9]. Specifically, we\ning its potential, RA-CLAP [10] adopted CLAP-style models\nCLAP methods. First, factorization over acoustic attributes\nexplicitly, it is questionable whether models actually consider\nFig. 1. Architecture of SPAM\nprompt into prompt embedding b using a language model.\nthe language model used. We adopt a Llama-3.1 8B [16] with\na bootstrapping method, we calculated AR as average proba-\nwith RA-CLAP teacher model [10]. Two variants of SPAM\nand automatic metrics, for each model and dataset\n(0.520 and 0.429). Furthermore, when we analyze per-model\nmodels (e.g., ParlerTTS). Such consistency of SPAM reveals\ntilingual codec language modelling,” in Proceedings of\nrobust and adaptive speech large language model,” in\net al., “The llama 3 herd of models,” 2024.\ntext-to-speech models,”\n\n\n=== EXPERIMENT ===\nence. That is, they cannot ensure whether the evaluation is\nbetween different semantics. We conducted two experiments\non two perspectives. The plausibility experiment showed that\nscore (MOS). Also, the faithfulness experiment demonstrated\nIndex Terms— Evaluation Metric, Prompt Adherence,\ncations Technology Planning & Evaluation (IITP) grant funded by the Ko-\nwhether their evaluation appropriately mirrors human percep-\nin line with [8], we define an evaluation as faithful when se-\nsuch attributes during evaluation. Second, multi-positive ex-\nenable quantitative automatic evaluation, we adopt a CLAP-\nFig. 2. Plausibility and Faithfulness Experiment\nprompt-based TTS datasets provide style keys [18, 19, 20],\n3. EXPERIMENTS\nexperiments (see Figure 2). First, plausibility experiment ex-\nence as human does. Second, faithfulness experiment exam-\n3.1. Plausibility experiment\nbility of metric evaluation. When a correlation between MOS\nric mirrors human evaluation well. We used three measures,\n3.2. Faithfulness experiment\n3.3. Datasets and Baselines\nFor the experiment, we used two test sets: TextrolSpeech\npairs from each dataset.\nUsing the dataset, we compare two versions of SPAM\nOn TextrolSpeech dataset\nOn LibriTTS-P dataset\nTable 1. Result of plausibility and faithfulness experiment, on the entire test set\nOn TextrolSpeech dataset\nOn LibriTTS-P dataset\nPlausibility. The result of the plausibility experiment is\nground truth audio for each dataset. Meanwhile, RA-CLAP\nshowed a large gap across datasets: 0.726 and 0.545. This\nFaithfulness. The result of the faithfulness experiment is\nThus, SPAM demonstrated faithful evaluation.\nexperiments, we evaluated whether SPAM produces similar\nplausible and faithful evaluation for prompt adherence.\npressive speech dataset with natural language descrip-\n\n\n=== RESULTS ===\nsmall perturbations [7], the result might not be faithful. Here,\nshowed reliable performance in many speech processing sys-\ntors. As a result, we obtain ˆat for each frame.\nresulting speech, the prompt encoder should be large enough\neraged the added result across frames. The two embeddings\n4. RESULT AND DISCUSSION\ntive prompts. Results from a paired t-test provide a more rig-\nprompts. As a result, we demonstrated that SPAM provides a\nFindings of the Association for Computational Linguis-\n\n\n=== CONCLUSION ===\n5. CONCLUSION",
  "2601.12254": "=== METHOD ===\nGenerative speech enhancement (GSE) models show great promise\nhigh-quality ones. However, GSE models are prone to hallucination\nnon-intrusive method for filtering hallucination errors from discrete\ntoken-based GSE models. Our method leverages the log-probabilities\nwith a suite of intrusive SE metrics, and that our method effectively\nods. Furthermore, we demonstrate the practical utility of our method:\ning improves the performance of subsequently trained TTS models.\nditional approaches have relied on deep neural networks (DNNs) to\ncleaning noisy source speech with an SE model, and (2) subsequently\nRecently, generative SE (GSE) models have emerged [7, 8, 9],\nDespite this promise, GSE models often introduce characteris-\nmodels, making the filtering of such artifacts a critical step in the\nfiltering methods based on non-intrusive speech quality metrics often\nIn this work, we propose a non-intrusive method for filtering\nhallucination errors from discrete token-based GSE models, as il-\nlustrated in Figure 1. Our method leverages the log-probabilities of\nmetrics. Furthermore, we show the practical utility of our method\nmodels. Our main contributions are summarized as follows:\n• We propose a non-intrusive filtering method that leverages confi-\ndence scores derived from discrete token-based GSE models for\n• We show that our filtering method effectively detects GSE-specific\nhallucination errors missed by conventional filtering methods.\nsubsequently trained TTS models.\n2. METHODS\nThis section details our method for filtering errors from discrete\ntoken-based GSE, as illustrated in Figure 2. Our method leverages\nA GSE model in a discrete latent space aims to generate a sequence\nGSE model\nProposed method\ntoken-based GSE model outputs enhanced speech wenhanced, along\nmodel is to generate X conditioned on c. To this end, the model\nwhere θ denotes the model parameters. During inference, a token\nGenhancer as a backbone model: In this work, we employ Gen-\nhancer [7] as a backbone SE model. Genhancer utilizes Descript\ncontain errors. Our method begins by defining a token-level con-\nthe enhanced speech. A high score indicates that the model generated\ning noise or acoustic distortions that impede the model’s processing,\nmodel and (2) confidence-based filtering.\nFirst, we apply the pre-trained Genhancer model to each utterance\neasy). Their curation process relied on a discriminative SE model,\nmodel. While this work shows the potential of GSE for TTS dataset\nout SE. One approach employs DNN-based quality predictors, such\nquality scores from speech samples [4]. Another approach leverages\ndetecting the hallucination errors introduced by GSE models.\ngenerated tokens from discrete token-based generative models are\ninterpreted as a measure of the model’s confidence, which is utilized\nguage model (LM)—derived from negative log-probabilities—is a\nfidence scores from ASR models such as Whisper [23] are used to\npre-trained speech LM. While our method adopts a similar conceptual\napproach, it differs in two key aspects. First, the SE model performs\nconventional methods evaluate the output in isolation.\nnation errors. We also evaluated TTS models trained on in-the-wild\nEvaluation of filtering methods: We used EARS-WHAM dataset\nEvaluation of subsequently trained TTS models: We used TITW-\n4.1.2. Models and training\nDiscrete token-based GSE model:\nthe GSE model. We followed the official configuration3 for the\nmodel architecture and training setup. We used the pre-trained DAC\nmodel [15]4 for the audio tokenizer and the layer-wise weighted sum\nof the pre-trained WavLM model [35]5 to extract conditional features\nfrom noisy input. The model was trained for 400k steps with a batch\nTTS model: We used Matcha-TTS [10] with the pre-trained HiFi-\nGAN vocoder (UNIVERSAL_V1) [11]6 for the TTS model. We\nfollowed the official implementation of Matcha-TTS7 for the model\narchitecture and training setup. The model was initialized with pub-\nfrom this pre-trained model, we trained it for 500k steps with a batch\nMOS model [37] to predict a mean opinion score (MOS).\nDNSMOS model [6] to predict an MOS.\nWhisper Large v3 model [23, 24].\nmodel and calculated the metric scores described in Section 4.1.3 for\neach filtering method with varying filtering thresholds.\nTo demonstrate the ability of our method to detect hallucination\nwe evaluated filtering methods with varying thresholds on EARS-\nTable 1. We also evaluated methods combining two metrics, which are\ndenoted by “&” in Figure 4. For these combined methods, a sample\nperforms the other methods (including combined-metric approaches)\nout filtering is competitive with our method on WAcc, its impact on\nTable 2: Evaluation results of TTS models trained on dataset curated\nusing our proposed method. “top N%” indicates the percentage of\n4.4. Evaluation of subsequently trained TTS models\nTo demonstrate the practical utility of our method, we evaluated\nTTS models trained on in-the-wild datasets curated with and without\nTITW-hard using the trained Genhancer model. Subsequently, we\nutterances. We then trained a separate Matcha-TTS model on each of\nmodels: one on the noisy source data and another on the enhanced but\nunfiltered data. For TTS evaluation, we followed the methodology of\nmodel to synthesize speech from 200 randomly selected texts from\n8,000 (= 40 × 200) synthetic utterances per model. The synthetic\nTable 2 shows the evaluation results for TTS models trained on\nthresholds. The results show that the model trained on the enhanced\nmodel trained on the unfiltered enhanced dataset (“Enhanced (unfil-\nmance of the subsequently trained TTS model, and confidence-based\nsuperior TTS models. They also suggest a trade-off between data\nIn this work, we propose a non-intrusive method for filtering errors\nfrom discrete token-based GSE models. Experimental results show\nthat our method effectively detects GSE-specific hallucination errors,\nwhich are frequently overlooked by conventional filtering methods.\nFurthermore, we demonstrate the practical utility of our method: cu-\nimproves the performance of subsequently trained TTS models. How-\never, our current approach is limited to discrete token-based GSE\nmodels. To address this issue, future work will explore leveraging the\nsure for GSE models operating in a continuous latent space [43, 44].\n[2] Y. Xu, J. Du, L.-R. Dai et al., “A regression approach to speech enhance-\nhancement via generative modeling on discrete codec tokens,” in Proc.\ntion model integrating self-supervised speech and text representations,”\n[10] S. Mehta, R. Tu, J. Beskow et al., “Matcha-TTS: A fast tts architecture\ntive speech enhancement methods: Issues and perspectives,” in Speech\nquality (pesq)-a new method for speech quality assessment of telephone\nmodel,” in Proc. Interspeech, 2024, pp. 682–686.\nlanguage models with asr systems using confidence measures and\ngeneration using speech language model,” in Proc. ICASSP, 2023, pp.\nhigh-quality versatile speech generation models,” arXiv preprint\nabilistic model for speech enhancement,” in Proc. ICASSP, 2022, pp.\n\n\n=== EXPERIMENT ===\nCONFIDENCE-BASED FILTERING FOR SPEECH DATASET CURATION WITH GENERATIVE\napplications such as curating noisy text-to-speech (TTS) datasets into\ncurating an in-the-wild TTS dataset with our confidence-based filter-\nIndex Terms— speech enhancement, discrete token, dataset\ncuration, text-to-speech synthesis, in-the-wild dataset\nspeech datasets, which offer a promising source of diverse speech\ndata [4]. While in-the-wild datasets are invaluable for developing\ndataset curation [4] follows a two-stage pipeline consisting of: (1)\nthe-wild datasets into high-quality datasets suitable for TTS training.\nSpeech dataset\nspeech dataset\nspeech dataset\nFig. 1: Overview of the speech dataset curation process with our\ndataset curation where such references are unavailable.\nmetric for filtering potential errors. Experimental results show that\nby showing that curating an in-the-wild dataset",
  "2601.14472": "=== METHOD ===\nmodeling and inaccurate phase reconstruction. We propose a \ninverse STFT. Unlike mel-spectrogram–based approaches, \nour design jointly models magnitude and phase, ensuring \nwith direct complex-spectrum modeling yields more natural, \nIndex Terms— Neural vocoder, prosody modeling, \nmodeling, particularly with generative adversarial networks \npersistent challenges remain: limited prosody modeling \napproaches like WaveNet [7] achieve high quality but at the \nbut without explicit phase modeling. Vocos [11] predicts \nprogress in acoustic modeling has highlighted the \nbut rarely within a unified vocoder architecture. Most \nphase through indirect or post-processing methods. This \nlack of explicit and simultaneous modeling of prosody and \nour approach consistently outperforms HiFi-GAN [2] and \nthese contributions establish a unified vocoder architecture \nmodeling. \n2. METHODOLOGY \nreconstruct waveforms indirectly, the proposed approach \nmodels such as HiFi-GAN and representation-driven \nconditions spectral modeling on the F0. While prior neural \nIn our approach, F0 is extracted from the reference \nUnlike pitch-aware conditioning in models such as \nFig. 1. Schematic diagram of the proposed architecture. \nthereby modeling both magnitude and phase within a unified \nCompared with models such as Vocos [11], which also \nensuring that both magnitude and phase modeling benefit \nguide the model toward phase-coherent, prosodically \nmodels were trained with identical preprocessing for \nquantifies the reliability of prosodic modeling across \nmodern neural approaches. \nexplicitly modeling both prosody and phase within a unified \nsignals with the original. The proposed model maintains the lowest \nimaginary spectral components, the model effectively \nbridges the long-standing gap between prosody modeling \nsupports the model’s ability to closely follow reference \nbroaden the applicability of the proposed architecture to \nfast waveform generation model based on generative \nacoustic modeling with waveform generator in text-to-speech \ngenerative model for raw audio”, in Proceedings of 9th ISCA \nVocoder Using Sinusoidal Model for Statistical Parametric \n\n\n=== EXPERIMENT ===\nphase-aware losses. Experiments on benchmark datasets \nExtensive experiments on benchmark datasets show that \na lightweight adversarial setup inspired by HiFi-GAN, \n3. EXPERIMENTS \n3.1. Datasets and Setup \nExperiments were conducted on two standard corpora. The \nLJSpeech 1.1 dataset [21] contains 13,100 utterances \n(𝛽1 = 0.8, 𝛽2 = 0.99). Experiments were run on a single \n3.2. Evaluation \nand perceptual quality. For objective evaluation, we \nSubjective evaluation was carried out through formal \nsamples. These evaluations together provide a balanced \nTable I summarizes the evaluation results. The proposed \nTABLE I. Objective and subjective evaluation results. \nfurther improves naturalness and robustness. Experiments \n[18] M. Verwoert, M.C. Ottenhoff, S. Goulis, et al., “Dataset of \n[21] K. Ito and L. Johnson, The LJ Speech Dataset, 2017. [Online]. \nAvailable: https://keithito.com/LJ-Speech-Dataset \n\n\n=== RESULTS ===\nThese results show that prosody-guided attention combined \npitch-aware conditioning, achieving strong results at scale \nstage. As a result, even expressive TTS pipelines remain \nresults discussed in the next section. \n4. RESULTS AND DISCUSSIONS \nlower levels. This pattern reinforces the objective results: \nTogether, these results highlight the advantages of \n[20] M. Morise, “Harvest: A high-performance fundamental \n\n\n=== CONCLUSION ===\n5. CONCLUSION",
  "2601.17761": "=== METHOD ===\nAR-Omni: A Unified Autoregressive Model for\nand inference. Autoregressive (AR) modeling, with a single token stream, a single next-token objective,\npresent AR-Omni, a unified any-to-any model in the autoregressive paradigm without any expert decoders.\nunder a single Transformer decoder. We further address three practical issues in unified AR modeling:\nLarge Language Models (LLMs) have achieved strong\nLanguage Models (MLLMs), which extend LLMs to\nsive modeling paired with expert diffusion decoders,\nAutoregressive (AR) modeling has proven to be an\nAR-Omni: A Unified Autoregressive Model for Any-to-Any Generation\nMethod\nmethods that do not rely on an external diffusion decoder for image synthesis. Streaming indicates early emission of\nnot applicable. AR-Omni is the only model that achieves both Unified I/O and Real-time Streaming without external\ndiffusion models.\nto explore whether an omni model can be built with\nthe same AR purity: a single autoregressive model\nWe present AR-Omni, a unified any-to-any model\nfied AR modeling: 1) Modality imbalance. Unified\nmodel that supports understanding and generation\n2.1 Multimodal Large Language Models\nModels\nadapters to Large Language Models (LLMs), en-\nAR-Omni: A Unified Autoregressive Model for Any-to-Any Generation\nSuch approaches have witnessed\nmodeling responsibility is still mainly taken by\nexternal expert models.\nThe foundation of autoregressive multimodal model-\n3.1 Unified Autoregressive Modeling\nAR-Omni: A Unified Autoregressive Model for Any-to-Any Generation\nSpeech. Unlike prior methods that rely on dual-\nier modeling, we adopt a purely acoustic-based dis-\nditional semantic-to-acoustic modeling, thus bypass-\nInterleaved modeling. To integrate these hetero-\nmodel’s response for the current turn, and <eos> to\ncausal probabilistic modeling task:\nspace of target codes. This guides the model to pro-\nAR-Omni: A Unified Autoregressive Model for Any-to-Any Generation\nMethod\nmodel relies on an external diffusion decoder for image generation; N/S means the model does not support image\nAR-Omni: A Unified Autoregressive Model for Any-to-Any Generation\nMethod\nDecoder Params count only the modality-specific image generation module; diffusion-based methods use a latent\nMethod\ndesigned for interleaved image–text modeling. Stage\nAR-Omni: A Unified Autoregressive Model for Any-to-Any Generation\nMethod\ndual-codebook methods, a(+b) denotes semantic tokens at a tok/s plus acoustic tokens at b tok/s. Codebook uses Dual\nthe transcriber model. First Token Latency (FTL) is a\nAR-Omni: A Unified Autoregressive Model for Any-to-Any Generation\nAR-Omni maintains a diffusion-free, single-model\ninto a unified AR model, without requiring separate\nMethod\nAR-Omni: A Unified Autoregressive Model for Any-to-Any Generation\nstages, indicating a tendency toward model collapse.\nWe presented AR-Omni, a unified any-to-any model\neration. To make unified AR modeling practical, we\nAR-Omni: A Unified Autoregressive Model for Any-to-Any Generation\nmodels\nearly-fusion foundation models.\nquence modeling. In Proceedings of ACL, 2024. URL\nWenhao Huang. MIO: A foundation model on mul-\nwith language models. Advances in Neural Informa-\na visual language model for few-shot learning. ArXiv\ntokenizer for speech large language models. arXiv\nguage models.\nopen-source multimodal models.\nAR-Omni: A Unified Autoregressive Model for Any-to-Any Generation\nlanguage model. arXiv preprint arXiv:2307.08041,\n[24] Zalan Borsos et al. Audiolm: A language model-\ning approach to audio generation. arXiv preprint\nguage modeling. arXiv preprint arXiv:2408.16532,\nmodal models for interleaved image-text generation.\nmodels. Advances in neural information processing\nmodels. ArXiv preprint, abs/2305.17216, 2023. URL\nAR-Omni: A Unified Autoregressive Model for Any-to-Any Generation\nmodels can hear, talk while thinking in streaming.\nmodels. arXiv preprint arXiv:2412.10117, 2024.\nand Bowen Zhou. Enhancing chat language models\nAR-Omni: A Unified Autoregressive Model for Any-to-Any Generation\nAR-Omni: A Unified Autoregressive Model for Any-to-Any Generation\nAR-Omni: A Unified Autoregressive Model for Any-to-Any Generation\nAR-Omni: A Unified Autoregressive Model for Any-to-Any Generation\nAR-Omni: A Unified Autoregressive Model for Any-to-Any Generation\nAR-Omni: A Unified Autoregressive Model for Any-to-Any Generation\nAR-Omni: A Unified Autoregressive Model for Any-to-Any Generation\nAR-Omni: A Unified Autoregressive Model for Any-to-Any Generation\nAR-Omni: A Unified Autoregressive Model for Any-to-Any Generation\n\n\n=== EXPERIMENT ===\nplayable audio chunks. Real-time indicates faster-than-real-time synthesis (RTF< 1) under our setup. “–” denotes\nquences to dominate optimization. Experimental\nDataset\nnoise dataset [40].\n4 Experimental Setup\n4.2 Image Evaluation\n4.3 Speech Evaluation\non the VCTK [47] dataset by conditioning AR-Omni\nuation setup, AR-Omni matches the best-performing\nnal under the unified setup. In contrast, the per-\ndecoding machine. Experiments show competitive\nscale dataset for training next generation image-text\nresources and evaluation conference, pages 4218–\nlarge-scale multilingual dataset for speech research.\nB Dataset Details\ntext-only data. For each dataset, we report modality\nuse publicly available datasets and follow standard\nDataset\nNotes: The dataset also provides additional annotations (for example, captions and visual\nquestion answering style annotations) used for evaluation.\n\n\n=== RESULTS ===\nperformance in understanding and generating natu-\nresults and qualitative examples demonstrating these\nresults in Section 6.2 demonstrate the effectiveness\nTable 5: ASR performance on LibriSpeech test-clean. We report WER. Speech-in tok/s is the number of discrete tokens\nTable 6: Zero-shot TTS results on VCTK. We report WER, first-token latency (FTL), and real-time factor (RTF); N/S\n5 Main Results\n5.1 Image Results\nimage captioning results. Within the diffusion-free\ngeneration results. Compared with the Anole ini-\nsynthesis. These results highlight a clear trade-off:\n5.2 Speech Results\nTable 5 presents the ASR results along with the\nTTS. Table 6 reports zero-shot TTS results, includ-\nTable 7: Ablation results on four omni-pretraining tasks at\noptimized. The results are shown in Figure 2. The\nof individual components to performance, and 2)\nwith results summarized in Table 7. Removing the\nand T2I performance also drops, further demonstrat-\nresults confirm that our proposed strategies not only\nIn Findings of EMNLP, 2023.\nfindings-emnlp.1055/.\nFigure 7: Qualitative image generation results with AR-Omni across diverse prompts and styles.\nFigure 8: Qualitative image generation results with AR-Omni across diverse prompts and styles.\nFigure 9: Qualitative image generation results with AR-Omni across diverse prompts and styles.\nFigure 10: Qualitative image generation results with AR-Omni across diverse prompts and styles.\n\n\n=== CONCLUSION ===\n7 Conclusion and Future Work\nFuture work will focus on enhancing the quality of",
  "2601.20230": "=== METHOD ===\nmultimodal large language model, supported by auxiliary mod-\nis implemented through two paradigms, end-to-end models[4]\nend models process user and agent speech jointly and preserve\nlarge language model (LLM), and speaker verification(SV)\nmultimodal large language model (MLLM) allow the models\nand backchannel, making the model’s predictions more inter-\ndialogue models to improve fine-grained speech handling.\n2. METHOD\nverification model CAM++2 to identify and filter target\n2https://modelscope.cn/models/damo/speech_campp\ntemporal dynamics in spoken language models,” arXiv\nLarge multimodal models for machine listening and\nlanguage models,” arXiv preprint arXiv:2509.14515,\nAn automated and standardized multi-modal method for\nlogue scheme based on large language model,” Advances\nin full-duplex spoken dialogue language models,” arXiv\ndling for full-duplex speech models,” arXiv preprint\n\n\n=== EXPERIMENT ===\nfree, plug-and-play manner. Experiments on the HumDial\ndataset demonstrate the effectiveness of our framework, which\ntation, thereby realizing a semi-cascaded design. Experimental\nresponse latency on the HumDial dataset.\n3. EXPERIMENTS\nExperiments are conducted on two NVIDIA A100 GPUs\nusing the dataset provided by the challenge organizers, with\nevaluation largely following Full-Duplex-Bench v1.5[12] and\nTable 1. Experimental Results on the Dev and Test Set\nexperiments show that a MLLM can replace the traditional\n\n\n=== RESULTS ===\n(TTS) synthesis. The resulting system operates in a train-\nresults show that the proposed framework achieves state-of-\nprovides ASR results as auxiliary semantic context.\n\n\n=== CONCLUSION ===\n4. CONCLUSION",
  "2602.01170": "=== METHOD ===\nHuang et al. (2021) showed how models for emotion detection can be fine-tuned through the transfer learning process towards achieving better accuracy\nby retraining existing models [7].\nrepresentations than what has been previously achieved using end-to-end architectures [1].\nMachine Translation (MT) has made one of the most rapid advancements as well with special emphasis on specific low-overhead language models.\nBuilding on these ideas and undertaking the relevant research work, this project seeks integration of ASR, MT, SER, and TTS methods into a single,\nduring the language conversion pipeline through the application of transfer-learning methods and pretrained models.\nProposed Method\nTo prepare the translation datasets for fine-tuning the Marian MT model, preprocessing steps were applied to make the data compatible with the model:\nMethod\nWe built an end-to-end system that consists of four components as shown in Figure 2, CNN model for extracting the emotion. (LSTM and ResNet50\nmodels were implemented and tested instead of CNN, more details can be found in A). Followed by Whisper [13] pre-trained transformer model to\ntransformer model. Finally, the text will be converted to speech using a MMS-TTS-Ara [11] pre-trained model. Their respective roles in the processing\nCNN Model for Speech Emotion Recognition:\nThis component employs an artificial neural network model that is created in TensorFlow/Keras to recognize emotion in audio data. The following was\nFigure 2: System Architecture\nreduction step for keeping only the relevant information of the data. Several feature extraction approaches were used:\nAfter the feature extraction, the data will be ready to be fed into the CNN model. The construction of CNN is as the following:\nThe CNN model was designed with the following layers:\nThe model is compiled using the Adam optimizer, which is known for its efficiency in training deep neural networks. Categorical cross-entropy is used\nWhisper is an advanced automatic speech recognition (ASR) system developed by OpenAI [13]. Its architecture is based on a transformer-based model.\nEncoder-Decoder Architecture: Whisper adopts an encoder-decoder approach where in this case the encoder is responsible for the raw audio while\nagainst variations like accents, noise, and background disturbances which makes it a reliable ASR model.\nto provide translation through multilingual models were made. Specific steps that included evaluation of pre-training, preparation of training dataset,\nfine-tuning and, evaluation metrics incorporating various metrics were utilized. Domain-specific data and advanced transformer-based architecture has\nPreliminary Model and Challenges\nThe objective of this work was to develop a high-performing translation model. Due to the limited dataset size, training a custom-built Transformer\nmodel posed significant challenges. Initial efforts involved training a Transformer model on 24,000 English–Arabic sentence pairs. This approach\ncomplex language pairs like English and Arabic. Building a high-performing Transformer model typically requires millions of sentence pairs to achieve\nthe desired results. To address these constraints, we fine-tuned a pre-trained model, MarianMT, to optimize translation performance on the available\nMarian MT, a pre-trained translation model by Helsinki-NLP [6], is a transfomer-sequence-model highly efficient based. Another way to consider it is\nmodeling the dependencies in a target sequence. This has emphasized the importance of pre-trained models and extra data this made MariaMT as a\nIn order to undertake training for the Marian MT model optimization, experiments with some key hyperparameters were carried out, such as learning\nThe model minimizes the cross-entropy loss L between the predicted tokens ˆy and the reference tokens y, defined as:\nMarianMT efficiency and being pre-trained on different language datasets provided a strong baseline. In addition, the ability of transformer architecture\nto model long range dependencies is essential in performing complex translations such as English-Arabic.\nThe MMS-TTS-Ara pre trained model is utilized in the final stage of the pipeline as shown in Figure 2, to convert translated Arabic text into natural,\n• Neural Vocoder: Transforms mel-spectrograms into waveform audio using advanced models like HiFi-GAN, producing human-like speech\nIn order to integrate the Marian MT model within the banking domain, a methodological fine-tuning process was implemented, aiming to augment the\nsuch as “the,” “of,” and “to” in English, as well as similar words in Arabic, as shown in Figure 3. Such overused words are essential to help the model\nEnglish-to-Arabic dataset had longer averages of 15 in Arabic and 17 in English. This range in sentence length likely helped the model’s flexibility,\nIn particular, RAVDESS [9] dataset is shown in the table 1, will be used for the task of training our Speech Emotion Recognition (SER) model. each\nFor the translation part we used two datasets in order to effectively finetune the MarianMT model, the first one is English to arabic Translation sentences\nrepository, and this dataset originally was in english language so we used Google Translator and MyMemoryTranslator models from the deep translator\nDifferent hyper-parameters were tested for training the CNN model. Moreover, different hyper-parameters configurations were tested as well for the\ntranslation model to identify the optimal setup for fine-tuning. Below are the key approaches used:\nAfter testing these configurations, the optimal hyperparameters for fine-tuning are detailed in Table 2 under the section ”Fine-tuned MarianMT Model\nTo evaluate the translation quality a combination of BLEU score(Translation quality by measuring n-gram between model outputs and reference)\nBaseline models were created to assess the performance improvements achieved through fine-tuning and additional processing. These baselines provide\n• CNN Model: A baseline CNN model was created without fine-tuning, data augmentation, or feature extraction.\nCNN Model Hyperparameters\nFine-tuned MarianMT Model Hyperparameters\nBest model saved at the end of each epoch, limited to 2 checkpoints.\nTable 2: Hyperparameters for CNN and fine-tuned MarianMT model.\n• MarianMT Model: For translation tasks, the MarianMT model in its pre-trained state was used as a baseline. Its performance was compared\nThese baselines establish a foundation for assessing the impact of enhancements introduced during model development.\nCNN Model\nThe table presented above 4 shows the performance of the implemented CNN model and the Baseline CNN in F1-score. Results show that the\nimplemented CNN model performed better than the baseline across all the classes of emotions. Calm, fearful and sad emotion classes achieved the\nlowest F1 score in both models, this indicates that both models are somehow facing difficulties classifying these emotions, possible reason might be due\nto their overlapping acoustic features. On the other hand, angry emotion class scored the highest F1 score in both models among all different emotion\nTable 4: F1-scores of baseline and implemented CNN models across emotion classes\ntraining techniques while developing state-of-the-art models. Different models were tested instead of CNN, more details can be found in A.\nTraining a Transformer model from scratch yields a BLEU score of 23.00, highlighting challenges that we might face with a small dataset; using a pre-\ntrained Marian MT model improves this score slightly to 25.48. Fine-tuning on both domain-specific and general datasets leads to significant gains—the\nbest model reaches a BLEU score of 56 and a BERTScore F1 of 88.7% as shown in the Table 5. Despite these improvements, it might encounter issues\nwith very long sentences: the model might truncate translations or misinterpret context due to the training dataset, since it does not contain sentences of\ndomain-specific and general datasets allows th",
  "2601.03632": "=== METHOD ===\nZero-shot text-to-speech models can clone a\nlable TTS methods attempt to address this issue,\nfirst reducing the model’s implicit dependence\nreference, these models can preserve the speaker’s\nthese approaches have demonstrated promising re-\nModel\nTable 1: Comparison of controllable zero-shot TTS methods.\nrepresentative controllable zero-shot TTS methods\nmental trade-off. If the model remains strongly\ntions) on top of the base model. Orthogonal LoRA\nalso evaluate our method on several challenging\nhappy references, a case that previous approaches\nautoregressive (AR), and hybrid architectures. In\nthe need for duration models. In AR-based ap-\nproaches, models such as AudioLM (Borsos et al.,\nTTS (Wang et al., 2025) model discrete audio se-\nlanguage modeling techniques for speech gener-\nation. Hybrid architectures like CosyVoice (Du\ngressively model semantic tokens and then employ\nken modeling, continuous token modeling has been\nAll these models perform well in zero-shot TTS,\nlable TTS models, such as FastSpeech2 (Ren\nify the desired speaking style. Notable models\n2023). However, these models are either speaker-\nMore recent approaches attempt to bridge the\nHowever, all these methods cannot support con-\ntice to train LoRA models to modify the style of\net al., 2024) and to combine multiple LoRA models\nerence audio. The model is trained to replicate\nbased style control methods from image generation\nMethod\nHowever, this approach doesn’t directly apply to\nence audio during inference. The model learns to\n(DCFG) to reduce the model’s dependency on the\nModel\nFigure 1: The overall framework of ReStyle-TTS. The proposed method consists of three logically coordinated\ncomponents: (1) Decoupled Classifier-Free Guidance (DCFG) reduces the model’s dependency on the reference\ncontrols how strongly the model follows the text,\nthe model’s reliance on the reference style without\nWith DCFG reducing the model’s dependency on\ntion in the model parameter space.\nHere, Θ denotes the base model parameters, λt\nIn standard flow-matching training, the model pa-\nby the current model and evaluate its speaker sim-\nmodel from scratch, we fine-tune the well-known\nration. In order to reduce the model’s dependence\ntext-prompt-based methods, where emotion is typ-\njust continuously, our method yields a smooth in-\nmodel behaves in the resulting three-dimensional\nmethod can consistently move pitch and energy in\nmodels. arXiv preprint arXiv:2406.02430.\na language modeling approach to audio generation.\nlanguage models. arXiv preprint arXiv:2412.10117.\nsion models. In European Conference on Computer\nDiffusion transformer autoregressive modeling for\nmatching for generative modeling. arXiv preprint\ncodec language models are zero-shot text to speech\ntts: An efficient llm-based text-to-speech model\nWeng, and Helen Meng. 2024. Instructtts: Modelling\nspeech model with freestyle text prompting. In Pro-\nthe official Emotion2Vec model (Ma et al., 2023)\nGeneration. Our approach diverges from the orig-\nEmotion2Vec model. Regarding the specific con-\nmodel’s reliance on the text and the reference audio,\n\n\n=== EXPERIMENT ===\nby weakened reference guidance. Experiments\nspecific datasets (e.g., anime or Van Gogh paint-\ndatasets annotated with particular attributes such as\nExperiments\nExperimental Setup\nDataset. We trained separate LoRA modules on\ndifferent subsets of the VccmDataset (Ji et al.,\nThe VccmDataset is composed of Lib-\nfocused audio datasets (Christophe et al., 2016;\nFor evaluation, we conducted controllable zero-\nshot speech synthesis experiments on the Seed-TTS\nused the VccmDataset (Ji et al., 2024) test set for\nEvaluation. Following ControlSpeech (Ji et al.,\njective evaluations, we conduct MOS-SA (Mean\nOpinion Score–Style Accuracy) evaluations to mea-\nThe evaluation details can be found in Appendix B.\ncontradictory-style setting on the VccmDataset\nsubjective evaluation of MOS-SA in Appendix D.\nThese experiments confirm that ReStyle-TTS can\ntion. Experiments demonstrate that ReStyle-TTS\ning dataset and performing additional LoRA fine-\ndataset. In ICASSP 2021-2021 IEEE International\nEvaluation Details\nFor objective evaluations, following Control-\ntion accuracy. For subjective evaluations, we con-\ncuracy) evaluations to measure the accuracy of the\nsubjective evaluation, and each audio sample is lis-\nof the experimental setup for Contradictory-Style\ninal usage of the VccmDataset in ControlSpeech.\nEach sample in the VccmDataset consists of an\nevaluation did not address scenarios where the style\nples from the VccmDataset as reference audio. For\nevaluation of MOS-SA in Table 5. The results are\nconsistent with the objective evaluation, and our\nWe further performed ablation experiments on\n\n\n=== RESULTS ===\npresent in the reference. As a result, synthesiz-\nTTS. As a result, synthesizing speech with a de-\naudio. As a result, the direct application of LoRA-\nare entangled within fa,t. As a result, adjusting\net al., 2025), resulting in entangled or unstable\ntire subspace spanned by the remaining ones, result-\ntraining. As a result, TCO effectively reinforces\nrange of values. Figure 2 summarizes the results\ntensity knob for each emotion. These results con-\nFigure 3: Two-attribute joint control results.\nFigure 4: Three-attribute joint control results.\na result, the relative ordering among reference sam-\nTable 2 reports the results for emotion transfer\nTable 3: Contradictory-style generation results for pitch\nand energy in Table 3. The results show that our\nthe results for angry in Figure 8 as an example of\nReStyle-TTS achieves the best performance across\ncontrollability. The results are shown in Figure 10.\n\n\n=== CONCLUSION ===\nConclusion",
  "2601.03170": "=== METHOD ===\ntraining-free method not only achieves state-\nlying TTS model. Audio samples are avail-\nlability in most existing methods remains confined\nthis limitation, some methods (Luo et al., 2021;\nmodel to render multiple emotions within an utter-\ntheir cross-model transferability and real-world de-\nwithout retraining the model?\ntic model, our approach focuses on restructuring\nmethod achieves state-of-the-art performance in\nmodel without any additional training. Our contri-\ntraining-free method not only achieves state-\nthe underlying TTS model.\nEmotion-controllable TTS methods can be broadly\nSpeech-prompt-based methods condition synthe-\nibility. In contrast, Text-prompt-based methods\noffer more flexible control, where early approaches\n2025). However, these methods typically operate\ncontrol methods predict fine-grained affective at-\nder their scalability and cross-model transferability,\ngressive approaches. Non-autoregressive meth-\nmodels (Du et al., 2025), but these predictors are\nautoregressive methods lack inherent duration\nsive methods. However, existing approaches still\nSeveral approaches have explored inference-time\ndiffusion-based TTS models, enabling training-free\nglobal embedding methods. However, these meth-\nMethod\nlarge language model, enabling automatic construc-\nOur TTS architecture follows the same config-\ntainty and model stochasticity. To correct such\nsion as the target budget is approached. Detailed\nDatasets and Comparison Models. We use the\nWe compare our method with representative\ncontrollable TTS methods spanning both non-\nThe non-autoregressive models include MaskGCT\nThe autoregressive models include\nComparison with Reference Models\nObjective Evaluation. Since comparative methods\n1.25). As shown in Tab. 1, our method achieves the\nModel\ntion control, as shown in Tab. 2, our method attains\nneutral emotion. In contrast, our method performs\nperforming the baseline and comparative methods\nTab. 1 and 2. Unlike comparative methods that syn-\nthesize segments independently, our approach per-\nline model, our framework achieves state-of-the-\nModel\nMethod\nMethod\nshown in Tab. 4, our method consistently achieves\nour approach further yields a 5.07% average error\ntime perspective, our method achieves smooth emo-\nstrate that our method not only delivers state-of-the-\nmodel.\nframework does not explicitly model gradual emo-\ntion learned in the pretrained baseline TTS model.\nSince our approach operates without parameter\nbetter model continuous emotion evolution and du-\nfine-tuning and model evaluation, and therefore\ntual precision or stylistic expression. All models\noutput traceability, or dedicated detection models,\nference on Empirical Methods in Natural Language\nmodels. arXiv preprint arXiv:2412.10117.\nand style-based models. In 24th Annual Conference\nattention model for multidimensional speech quality\ntion modeling using cross-domain emotion diariza-\nefficient llm-based text-to-speech model with single-\nWeng, and Helen Meng. 2024. Instructtts: Modelling\nspeech model with freestyle text prompting. In Pro-\nmodelling. In Proceedings of the 32nd ACM Interna-\non multi-emotion utterances to support modeling\nguage model via supervised instruction tuning with\noverall model to read text/semantic context\nencouraged as the ratio approaches 1.2, with the\nmodel.\nAblation Models Implementation\nTab. 3, we compare our method with the following\nIn Fig. 4, we compare our MSA method with the\nthe previous method by selecting the top-k at-\nBaseline and Comparative models\nautoregressive zero-shot TTS model that supports\ntent representations, the model further improves\nour comparative methods:\nautoregressive TTS model that a masked gen-\nintermediate models like flow matching. It\ngressive TTS model that combines a language\nmodel for semantic and prosodic modeling\nOur baseline and comparative models adopt a\ngenerated individually among these models and se-\native models are implemented using their official\nmain/models/tts/maskgct\n(ASR) model through comparison with ground-\n2023) ASR model to calculate Word Error Rate\nParaformer (Gao et al., 2022) ASR model to cal-\nmethod is compared with F5TTS(Chen et al., 2025)\ning a pre-trained speech emotion recognition model\nOur method demonstrates robust emotional fidelity,\napproach those of comparative methods. It’s worth\nemphasizing that while other methods could only\nis constant, our method generates continuous and\ntions. Despite the difficulty of modeling such dy-\nnamic emotional control, our model still maintains\nMethod\nMethod\nmethod demonstrates a consistent advantage in\nby the DNSM metric, where our method consis-\ntently outperforms all methods across different text\nNevertheless, we observe that our method still\nscenarios. In the text prompt setting, our method\nIn this scenario, our method and\nposition, MAS prevents the model from drifting\n\n\n=== EXPERIMENT ===\npublic datasets or complex multi-stage training.\nand duration-annotated text dataset to enable\nExtensive experiments demonstrate that our\naligned annotated speech datasets or involve multi-\ndataset with 30,000 samples and fine-tune Qwen3-\ning. Extensive experiments demonstrate that our\nduration-annotated text dataset for LLM-\n• Extensive experiments demonstrate that our\ndatasets or multi-stage training pipelines that hin-\ndataset (MED-TTS) with 30,000 samples, which is\nand Step 2. Based on this dataset, we perform su-\ngies, step-wise checklists, dataset statistics, and\nExperiments\nMED-TTS dataset for content text, text-based emo-\ndomly held out for evaluation, while the remaining\nthe Emotional Speech Dataset (ESD) (Zhou et al.,\nEvaluation Metrics. We adopt both objective and\nclassifier for text prompts. Subjective evaluation\nResults and Evaluation\nfor evaluation. Under this setting, we conduct ob-\njective evaluations for both emotion and duration\nTable 1: Objective and subjective evaluation across different emotion prompt settings. ↓indicates that lower values\nSubjective Evaluation. We report subjective re-\nevaluations.\nEmotion and Duration Control Evaluation. We\nMonotonic Stream Alignment Evaluation. To\nTable 2: Objective and subjective evaluation on different duration scaling settings. ↓indicates that lower values are\nDuration-Specified Evaluation.\na single utterance. Extensive experiments demon-\nels to generate a synthetic text dataset for Qwen3\nand datasets used are publicly available and em-\nprediction with crowdsourced datasets. In 22nd An-\nTo ensure the reliability of the constructed dataset,\nDataset Statistics and Distribution\nthe MED-TTS dataset across languages, text cat-\nFigure 5: Statistics of the MED-TTS dataset. (a) Dis-\nFig. 5a, the dataset is well balanced across lan-\nresentative examples from the MED-TTS dataset\nTable 5: Dataset statistics and representative examples across languages and text categories.\nhighlighting the dataset’s coverage of diverse con-\ncally and remain constant across all experiments,\nMonotonic Stream Alignment Evaluation\nEvaluation Protocol\nutterance for evaluation. All baseline and compar-\nSubjective Evaluation\n(MOS) evaluation focusing on four key dimensions:\nTab.2. The evaluation involved 15 graduate stu-\nto the evaluation, participants were provided with\nThe user interface for MOS evaluation is illustrated\nObjective Evaluation\nOur objective evaluation encompasses several met-\ntruth transcriptions. For English audio evaluation,\nFor speech naturalness evaluation, we utilize\nExperimental Result Supplements\nEmotion-Specific Control Evaluation. We pro-\nvide detailed experimental results for the emo-\nTable 6: Objective evaluation results on English Speech and Text inputs across different text categories. ↓indicates\nCategory-Specific Emotion Control Evaluation.\nTab. 6 extends our evaluation to three distinct syn-\nthesis scenarios in our dataset: Emotional Dialogue,\nYour task is to generate high-quality text utterances for text-to-speech synthesis evaluation.\nYou are an expert linguistic annotator specialized in emotional proso",
  "2601.21886": "=== METHOD ===\nscores can provide better interpretability, but models predicting them\nAutomatic speech quality assessment (SQA) models summarize the\nthese approaches do provide a picture of overall quality, they cannot\nlarge-scale datasets for this task to train a model in a supervised fash-\nNet [7] first looked at how to train SQA models to predict frame-\nlevel scores. Kuhlmann et al. [8] extended this idea to SQA models\nwe show that frame-level predictions of SQA models can be effec-\ning test to 1) check whether this detection method does not produce\nartefacts the SQA model is sensitive to. Compared to randomly ex-\ntracted control samples, the segments detected by our method were\nSQA models provide a prediction of speech quality for the full ut-\nencoder-decoder SQA models [9] from which we extract frame-level\ncause the model could no longer effectively utilize the full context.\nIn the training of neural audio codec models, Liu et al. [10] no-\nSQA model (e.g., a BLSTM as in UTMOS [18]), constraining the\nTable 1. Tested model configurations for PartialSpoof. All models\nthe edited regions, we train SQA models in the conventional way us-\nFor training our models, we loosely follow the setup in SHEET [9]\nwith some modifications1: 1) We always use a WavLM [20] model\nprojection layer. Table 1 shows an overview of the model configura-\ntions with are tested on PartialSpoof [11]. We train all models on the\nmodels first perform a loudness equalization of the input signal to\nization. We train all models for 100 epochs with an initial learning\nWe apply the trained models to the PartialSpoof [11] database with-\nout any fine-tuning. The localization performance for models trained\ntections, but allow the model to only capture a part of a ground\nthe frame-level resolution of our SQA models. We tune3 the de-\ntection threshold for all models on the development split and report\nThe results are shown in Table 3. The high recall of models\nindicated by the lowest precisions and F-scores among all models.\nto the binary targets the model was trained with: A frame is either\nthesized speech samples. As frame quality predictor, we use model\nwhich are shorter than 100 ms. This approach results in 857 de-\nthat our detection approach indeed detected the segments with the\nloss in the training of SQA models improves the localization of seg-\nWe noticed that the types of artefacts the SQA model reacted\nwe found that the model assigned low scores to non-verbal vocaliza-\ninto the alignment of the SQA model with the target application.\nM¨oller, “NISQA: A Deep CNN-Self-Attention Model for Mul-\nAssessment Model Based on BLSTM,” in Interspeech, 2018,\nguage Models,” arXiv preprint arXiv:2409.19283, 2024.\nwith large speech language models,” Advances in Neural Infor-\nAST: A Transformer-Based Model for Speech Quality Predic-\n\n\n=== EXPERIMENT ===\nspeech synthesis evaluation, automatic error localization\nmance of voice transmission systems [1] or the evaluation of genera-\nloss with margin 0.1 [18] in LSQA; 4) we experiment with a 1-layer\nTable 3. Detection results on the evaluation split of PartialSpoof. We\nror rate and F1-score. An alternative to segment-based evaluation is\nintersection-based evaluation [21] which is more flexible by defining\nprecision, recall and F1-score on the evaluation split.\nconfiguration 8 which achieved the best results on all tested datasets.\nSince the evaluation of TTS systems is very sensitive to the context\nFig. 1. Evaluation instructions for listeners.\nreplikant [26] to setup and run the experiment.\nspeech?”) by evaluation set. All control sets receive a higher per-\nDatasets,” in Interspeech, 2021, pp. 2127–2131.\nEnhancing the evaluation of text-\nA Multi-purpose Open-source Speech Human Evaluation Esti-\nEvaluation of Sound Event Detection,” International Confer-\nspeech synthesis perceptual evaluations,” in Interspeech, 2025,\n\n\n=== RESULTS ===\nprovide any results on the types of errors. Gutierrez et al. [5] used\nof long contexts, the detection performance dropped drastically be-\n3.2. BVCC results\nTable 2 shows results on the BVCC test split. To show the effect of\ntation. Indeed, from the results, we find that the consistency losses\nTable 2. Results on BVCC test main. SRCC: Spearman rank corre-\n3.3. PartialSpoof results\nWe measure detection performance with the intersection-based\nan increase in precision, but causes a decrease in recall performance.\nences from the same for voice cloning, resulting in about 10, 000\n\n\n=== CONCLUSION ===\n5. CONCLUSION & DISCUSSION\nproduce undesired false positives. Future work should therefore look",
  "2601.20094": "=== METHOD ===\narchitecture, introduces significant latency bottlenecks on\nby the TS3-Codec architecture.\ntic feature, representations derived from models such as\nthan traditional mel-spectrogram-based approaches [5]. This\nnew synthesis model now predicts codec features instead of\nfeature streams. This separation allows a TTS model to ex-\ning advanced models like Mimi onto mobile or edge devices.\nfeaturing a Transformer-only decoder architecture.\narchitecture composed of Transformers and linear layers, we\nmodel’s storage footprint and computational requirements,\nacoustic models, along with potential resource contention on\nmethodology is not limited to Mimi codec, and provides a\nA variety of state-of-the-art TTS models demonstrate the\nmodels, such as VALLE [10], VoiceStar [11], which lever-\na range of acoustic modeling techniques, including diffusion\nmodels [16], Tacotron-based architectures [5], and depth-\ntransformer-based methods [17].\nthe specific acoustic models and codec features, every sys-\nhardware, a streaming-capable architecture, and high-fidelity\ndio quality during model compression. The Mimi codec is\narchitecture and high-quality reconstruction, making it an\nefficiency of deep learning models, enabling significant re-\n3. METHOD\nFig. 1: Model architectures for the original Mimi decoder\n3.1. Architecture\nThe model architectures of the original Mimi decoder and\nlayer architecture had better performance than a wider 8-layer\ndecoder model is trained using a composite loss of GAN\nrely on a Multi-Scale STFT Discriminator architecture [7].\naudio quality. In the initial stage, the model is trained with\nAdditionally, we observed that the model tended to gener-\neffective trick forces the model to explicitly learn a robust\ntimal trade-off between model quality, size, and latency was\nto further refine the model’s performance.\nMimi model on the same data (the model is denoted as Mimi-\noptimal architecture for the T-Mimi decoder.\ntion study. The model is optimized using the Adam optimizer\nstruction fidelity, respectively. To evaluate model efficiency,\nmodel size (MB), and the inference latency (ms).\nModel comparison\naudio quality metrics. Run for 50k steps to select models.\nTo further reduce model storage and latency while preserv-\napplying QAT following full-precision model training. The\nferent QAT strategies that assign mixed precision to the model\nlayers. Note, in Table 2, all models are only trained for 50k\nsteps to save computation and select models. In these exper-\nthe 4-bit model exhibits a decline in audio quality, the 8-bit\nmodel maintains strong despite achieving a 75% reduction in\nstorage compared to the 32-bit model. The perceptual quality\nof the 8-bit model can be further improved by only increas-\naudio quality and small storage, we choose model T1–10, 8bit\n– T11–12, 32bit for continue training. The final QAT model af-\nwhich is very close to the non-quantized model’s a PESQ of\nthe baseline model.\nexperience. Notably, the CNN-Mimi model was evaluated\nplying quantization to a model not specifically optimized for\nis particularly acute for generative models.\nIn Table 4, we present the trade-off between model storage\nand performance by varying both the model layers and the di-\nmension of the final linear layers. These models are trained\nfor only 90k steps for model selection. First, expanding the\nmodel from 8 to 12 layers lead to substantial improvements\nfeatures. For the 12-layer model, increasing the linear dimen-\nfor model compression: layers closest to the final waveform\nthe landscape of spoken language models:\nLee, “Towards audio language modeling–an overview,”\ndation model for real-time dialogue,”\nguage models are zero-shot text to speech synthesizers,”\nTagliasacchi, et al., “Audiolm: a language modeling ap-\nefficient llm-based text-to-speech model with single-\nAn audio foundation model toward universal audio gen-\n\n\n=== EXPERIMENT ===\nFrom our intial experiments, we found that a deeper, 12-\n3.3. Quatization aware training setup\n4. EXPERIMENTS\n4.1. Experimental setup\n4.1.1. Training setup\nFor training, we use an in-house speech dataset of 5 million\ncorpus, both at full 32-bit precision. For evaluation, perfor-\ntailed experiments that led to this design choice in the abla-\n4.1.2. Evaluation setup\nOur evaluation is based on a set of objective metrics. For au-\n(STOI), wide-band Perceptual Evaluation of Speech Quality\n4.2. Experimental results\nTable 1: Human CMOS evaluation results between T-Mimi\nHuman evaluation is the golden standard to evaluate the\nMimi-32-bit by human evaluation. Table 1 shows the CMOS\n(Comparative Mean Opinion Score) human evaluation results.\nFT-32-bit, and T-Mimi-32-bit). In our evaluation, 200 pairs of\nare on-par. And in the following experiments, we conduct\nacross all evaluation metrics, demonstrating that 8-layer mod-\nand a linear dimension of 2048 for all experiments through-\n\n\n=== RESULTS ===\nTo mitigate the performance degradation typically associ-\npling, and the resulting waveform segments are concatenated\nThe result shows no significant difference between Mimi-FT-\ning the 32-bit performance, we investigate the efficacy when\n4.2.3. On-phone latency benchmarking results\nit is known to cause a loss in performance, a vulnerability that\nsion to 3072 results in modest improvement, with a storage\nout the paper as it demonstrate good performance considering\nTTS performance on mobile devices. Furthermore, the study\n\n\n=== CONCLUSION ===\n5. CONCLUSION",
  "2601.20510": "=== METHOD ===\nAdvanced Text-to-Speech models\nof three state-of-the-art TTS models—Dia2, Maya1, and MeloTTS— rep-\nresenting streaming, LLM-based, and non-autoregressive architectures. A\nsemantic, structural, and signal-level approaches.\nmodels effective against one TTS architecture\na multi-view detection approach combining complementary analysis lev-\nels demonstrates robust performance across all evaluated models. These\nspeaker variation. Today’s models—particularly those leveraging large language\nmodel (LLM) architectures and flow-matching generative approaches—can gen-\nsures [1]. These include graph-based attention architectures such as AASIST [2],\nself-supervised learning approaches leveraging wav2vec 2.0 and Whisper [3, 4],\nMeanwhile, the threat landscape has shifted dramatically. Models released in\nlate 2024 and early 2025—Dia2, Maya1, and MeloTTS—employ architectures\nsuggest it may falter with newer TTS Models. Hierarchical feature fusion meth-\nthree state-of-the-art TTS models against four distinct detection frameworks,\nwe provide the first empirical characterization of how detection architectures\n(XLS-R-SLS) [6], structural graph modeling (SSL-AASIST) [3], and large-\nscale foundation models (MMS-300M) [9] to provide diverse analytical per-\natic vulnerabilities to LLM-based TTS, while hierarchical fusion models\nsystems. These findings suggest that no single detection approach is robust\nacross the full spectrum of modern TTS architectures.\n4. We show that proprietary audio deepfake detection model from UncovAI\ntion models in detection. Section 3 details our dataset construction, models, and\nevaluation methodology. Section 4 presents our experimental results, organized\nEvolution of TTS Threats: From Vocoders to Foundation Models\nand demonstrated that flow-matching models can achieve remarkably efficient\n[12]: detection models trained exclusively on vocoder artifacts fail catastrophi-\ncodec-trained detection models achieved a 41.4% reduction in average EER com-\nThe final architectural innovation reshaping the threat model is streaming\naudio synthesis. Meta’s research demonstrated that streaming-optimized models\nSelf-Supervised and Foundation Models in Detection\ntures were sufficient. More importantly, it suggested that self-supervised models\nmodel pre-trained on 680,000 hours of multilingual audio[4]. What they discov-\nlines, validating that adapting foundation models is critical for generalizing to\nparts of the foundation model matter most? Wang et al. (2024) conducted a\nobservation—that the ”depth” of feature extraction is as important as the model\nitself—inspired the hierarchical approaches that follow.\ntecture, which treats the foundation model not as a monolithic black box, but as\nlarger models (Whisper-Large vs. Tiny) maintain better robustness to shorter\ninputs, suggesting that foundation model size correlates with performance on\nmodels degrade differently with duration, and optimal capacity remains an open\nart models on Deepfake-Eval-2024, the results were sobering: AUC dropped by\napproximately 50% for audio models compared to previous benchmarks. This\nlatest TTS architectures released in 2024–2025. None of these datasets include\nnewer architectures. This is a gap our work implicitly addresses through our use\nMethod\nTTS Models\nOur choice of models was deliberate and motivated by diversity. We selected\napproach to the core problem of speech synthesis. By spanning from streaming-\noptimized architectures to flow-matching systems, we ensure that our deepfake\nquality and prosody as the model resets its internal state. Dia2 solves this prob-\nlem through its Deep-Inherited Attention (DIA) architecture. Rather than treat-\ning each sentence as an independent problem, the model maintains Key-Value\nan entirely different paradigm: the emerging class of Speech Foundation Models.\na language modeling problem. Rather than predicting raw waveforms or spectral\nsemantic reasoning capabilities of large language models. Before speaking, the\nmodel understands the meaning of the text—respecting punctuation, emphasis,\nAudio Codec (SNAC) [20]. Unlike traditional models that generate a flat se-\nthe model predicts seven distinct tokens operating at three different temporal\nthe model synthesizes laughter, whispering, sighing, and hesitation directly from\nOur third model, MeloTTS (Melo) [21], represents a distinct archi-\ntectural paradigm: the lightweight, end-to-end adversarial approach. Unlike the\nVITS architecture [22], which integrates acoustic modeling and waveform gen-\nentirely. This ”Latent Variable” approach allows for extremely rapid inference\ntering), the VITS architecture tends to introduce subtle periodic artifacts in\nDetection Models\nThe challenge of evaluating deepfake detectors is that no single approach can see\nsentations (where frequency artifacts surface), or semantic models (where lin-\nselected three detection models representing each analytical perspective.\narchitecture, originally designed for detecting video forgeries [23].\ntransfer from video to audio detection might seem unusual, the architecture\nfollowed Kawa et al.’s approach of fine-tuning the entire pipeline. This adapta-\nto Whisper’s semantic approach, we employed the SSL-AASIST architecture\nproposed by Tak et al. [3]. This model pairs wav2vec 2.0 as a front-end with\nAASIST as the back-end, creating a system that explicitly models the relation-\nlingual audio. This unsupervised approach means it learns raw acoustic structure\nthan treating audio as a flat sequence, it models it as a heterogeneous graph\nWhile the SSL-AASIST model\nnot uniformly distributed across the depth of large speech models. Lower layers\nThe XLS-R-SLS architecture capitalizes on this insight by treating the 24\nflaws (typical of Dia2). This hierarchical approach is particularly valuable when\nfacing diverse generation architectures, as it does not commit to any single level\nThis structure forces the TTS models to generate speech with frequent speaker\nsee”), which require the TTS models to infer appropriate intonation patterns\nprovide the semantic grounding necessary for models like Maya1 to trigger their\nof each TTS model. Our evaluation focused on five critical dimensions of syn-\nby transcribing the generated audio with OpenAI’s Whisper-large model [24]\nFor speaker fidelity, we assessed how well the models cloned the target voice\ngenerated samples to their own centroid, this metric reveals whether a model\nproduces a stable, distinct persona (score approaching 1.0) or drifts between\nmodels across our 4,000-sample test set.\ninherent to each generative architecture. It is important to note that Speaker\nSimilarity (SIM) is not reported for Dia2; unlike the other models, Dia2 was\nsimilarity comparisons methodologically invalid.\nModel\narchitecture appears to produce highly stable, ”clean” audio, reflected in its\nindicates that the model imposes a highly consistent acoustic signature or em-\ndard forensic metrics. Each metric captures a different aspect of the model’s\ncomparing models without setting a specific decision threshold.\ncial for understanding the model’s effectiveness in imbalanced scenarios,\nWe evaluated the performance of our three detection architectures—Whisper-\nby detection model.\nTable 3 details the performance of the Whisper-MesoNet model. This architec-\n17.05%, with a corresponding AUC of 0.8750. In contrast, the model exhibited\nThe results for the XLS-R-SLS model, which utilizes multi-layer feature fusion,\nare presented in Table 4. This model demonstrated its strongest detection capa-\nproved the most challenging for this architecture, resulting in an EER of 27.10%\nTable 5 summarizes the performance of the SSL-AASIST model. Similar to the\nXLS-R model, this detector performed best on the Dia2 dataset, recording an\nFinally, we evaluated the audio deepfake detection from the proprietary model\nAs shown in Table 6, this model achieved near-perfect separation across all\nSpeech architectures—Dia2, Maya1, and MeloTT",
  "2601.20481": "=== METHOD ===\nModern zero-shot text-to-speech (TTS) models offer unprecedented\ntities upon request. Existing approaches, reliant on retraining, are\ntroduces a profound risk: such models may generate the voices of\nthe influence of specific training data from learned models. How-\never, most existing methods, including gradient-based [12, 13, 14],\nor text generation. These approaches often involve retraining of the\n(TTS) Model\nmodel, leading to high computational cost, unstable convergence,\na deeper structural gap in current approaches to speaker protection\nin TTS. Existing methods are either reactive (e.g., watermarking),\nmodel’s internal activations to suppress identity-specific informa-\nof modern TTS models. Even though EmoSteer [19] has attempted\n(ID-prototype) with intermediate features of TTS models. We statis-\nTTS Model \nFig. 2: The overall framework of TruS, working with TTS models\nout, our method controls to suppress the identity-related activations\nmethods. Crucially, TruS is the first to generalize to unseen opt-out\nmodels from generating both seen and unseen speaker identi-\n2. METHOD\ntraining the model on a filtered dataset with the target speaker data\nremoved. However, as the generalized capability of TTS models is\nincreased [20, 21], the models impose a well-formed speaker em-\nsteering internal representations in TTS models is an effective and\nerence utterance. Given a TTS model G(x, u) that generates speech\nOur TruS is built upon the recent fabulous TTS model, F5-TTS [5].\nIn the following, we describe our approach on top of this baseline\nfor clarity, though our method is generally applicable to intermedi-\nate blocks of other DiT-based [24] TTS architectures.\nDuring inference of TTS models, our TruS simultaneously\nMethods\nwith the retain set of Emilia. Since our method solely works for the seen opt-out set, † scores follow the original model. Training hours are\ncol enables systematic evaluation of whether the method suppresses\nImplementation details. Our method entirely works at inference\ntime, operating on a pretrained open-source TTS model, F5-TTS [5]\nMethods\nMethods\nfeatures computed by a pretrained speaker verification model [31].\nlearning methods (i.e., SGU [18] and TGU [18]), which require care-\ntions for the retain speakers identical to those of the baseline model.\nthrough large-scale tts models: Comparing zero-shot and fine-tuning\napproaches to personalise tts in assistive communication,”\nsynthesis with large language models,” CoRR, 2024.\nand M. MacDiarmid, “Steering language models with activation engi-\n[24] W. Peebles and S. Xie, “Scalable diffusion models with transformers,”\nlanguage models between code execution and textual reasoning,” in\nOpen and efficient foundation language models,”\n\n\n=== EXPERIMENT ===\n(i.e., opt-out) speaker’s identity. Experimental results demonstrate\n3. EXPERIMENTS\n3.1. Experimental settings\nDatasets. Our baseline is F5-TTS [5] pretrained on Emilia [28], a\ners in Emilia constitute the retain set. For zero-shot evaluation, Lib-\nTable 3: Evaluation for emotion preservation and unlearning perfor-\nspeech. We report all metrics under three evaluation conditions: re-\nOur evaluation focuses on two aspects: (i) the degree to which the\ning the emotional speech dataset [30] to verify whether emotion\nand diverse speech dataset for large-scale speech generation,” in SLT,\ndataset,” IEEE Trans. Affect. Comput., vol. 5, no. 4, pp. 377–390, 2014.\n\n\n=== RESULTS ===\ntal results show that TruS effectively prevents voice generation on\nretraining, achieving comparable unlearning performance to existing\n• TruS demonstrates comparable performance to tremen-\nTable 1: Quantitative results on LibriSpeech (-R) and the seen opt-out set (-SO) on Emilia. ‘F5-TTS-FT’ denotes the finetuned F5-TTS only\nof intervention layers. As a result, multiple layers may be selected\nTable 2: Performance on unseen opt-out set (-UO) in LibriSpeech.\nMetrics. Following prior work [18], we assess the performance with\n3.2. Results\nSeen opt-out speakers. Table 1 reports the results on the seen\nformance is preserved after unlearning on LibriSpeech. The results\nsuffers from degraded unlearning performance.\nble 3 include zero-shot baseline performance of F5-TTS, which aims\nTable 4: Performance over different layer selection strategies.\nTable 5: Performance over different numbers of retain speakers.\nin WER-UO. These results suggest that identity unlearning can gen-\n‘µ + σ’ criterion provides the most balanced performance over all\nweaker suppression for identity removal. These results indicate that\nThe pool size of ID-prototype. Table 5 examines the performance\nperformance on seen data, motivating our choice of N=30.\n\n\n=== CONCLUSION ===\n4. CONCLUSION",
  "2601.19781": "=== METHOD ===\nlanguage models (speechLMs) and as efficient intermediate repre-\nkenizer, a method that fine-tunes phonetic tokens via differentiable\nguage models (speechLMs) [3, 4] and serve as efficient intermediate\nmodels with the goal of reconstructing speech waveforms [10, 11].\nsupervised learning (SSL) models [12], and are regarded as being\nby incorporating a pretrained SSL model into the learning of acous-\n[16, 17, 18], known as hybrid tokens. However, these methods are\nrequires additional, somewhat complex architectures in downstream\nmodels to manage multiple streams effectively. Also, the use of\nSSL model using differentiable k-means in a multi-objective frame-\nThe key strengths of our approach are summarized as follows:\nour method effectively incorporates prosodic information while\ntokens using differentiable k-means [21, 22], our method enables\n• Reduced training data requirement: Since our approach fine-\ntunes a pre-trained large-scale speech foundation model (WavLM-\nModel\nFig. 1. Architecture of the Phonological Tokenizer: multi-objective\n2.1. Hybrid tokens utilizing pretrained SSL models\nbased acoustic tokens by integrating pre-trained SSL models to bet-\ntained from SSL models for specific purposes by introducing differ-\ncomponents of the model: 1) the SSL model θssl, performing fea-\ndiscretize the SSL features; and 3) the ASR model θasr, predicting the\nThis approach not only improved the accuracy of ASR but also mod-\n3.2. Proposed method: multi-objective optimization\nWe weight these two losses using α. Both the ASR model θasr used\nonly the fine-tuned SSL model θssl followed by differentiable k-\nThe model training was conducted using ESPnet [33]. The config-\nmodel, we used the 21st layer of WavLM-large [23], and the clus-\ndecoder (AED) model [34], and for the vocoder, we used HiFi-GAN\nTDNN model [36]3. As described in [21], training was conducted\nin two stages. In the first stage, the SSL model θssl and cluster cen-\n4. In the second stage, the entire module, including the SSL model\ntrained model to perform various discrete token-based speech tasks,\nand compared their performance with that of existing models. As\nmodels for ASR, emotion recognition (ER), and speaker identifi-\ncation (SID). For ASR, we trained joint CTC/AED models using\nLibriSpeech-100h. For ER, we trained ECAPA-TDNN models on\nECAPA-TDNN models on VoxCeleb1 [38].\ning trained only for reconstruction, suggests that using an SSL model\nLJSpeech reconstruction (ID): Across all the metrics, our model\nExpresso VC (OOD): Our model outperformed all the baselines in\n[46] as training data and trained each model for 4 epochs. As evalu-\ntages of our method. Future work includes scaling up the training\n[3] K. Lakhotia et al., “On generative spoken language modeling\n[4] S. Arora et al., “On the landscape of spoken language models:\nfrom self-supervised models?,” in Interspeech, 2024.\nspeech representation for spoken language modeling,”\nfor speech language models,” in ICLR, 2024.\n[17] A. D´efossez et al., “Moshi: a speech-text foundation model for\ncoming of codec for audio language model,” in AAAI, 2025.\ntokenizer for audio language modeling,” in ICLR, 2025.\n[28] P. K. Rubenstein et al., “Audiopalm: A large language model\nfactorized codec and diffusion models,” in ICML, 2024.\nmodel on one GPU in a day,” in ACL, 2025.\nSpoken language modelling,” in Interspeech, 2021.\nmodel evaluation,” in ICASSP, 2025.\n[49] A. Grattafiori et al., “The llama 3 herd of models,” arXiv,\n\n\n=== EXPERIMENT ===\nExperimental validation on diverse tasks confirms that our tokens\ndatasets such as LibriSpeech (960h) [26] or LibriTTS (585h) [27].\n4. EXPERIMENTS\n4.1. Experimental setup\ning, trained on a 30-hour subset of the LibriSpeech-100h dataset\nthe following experiments, we used the tokens obtained from the\n4.2. Evaluation on discriminative tasks\nRAVDESS [37], a dataset of the same sentences spoken with differ-\n4.3. Evaluation on generative tasks\nthese evaluation metrics tend to favor neutral speech over emotional\n4.4. Evaluation on speechLMs\nExperimental results across diverse downstream\n[39] K. Ito & L. Johnson, “The lj speech dataset,” 2017.\n\n\n=== RESULTS ===\nwork combining ASR and speech resynthesis. The resulting tok-\nenizer demonstrates high performance in both speech understanding\nAs a result,\nstrong performance in tasks where prosody is crucial, such as\ning superior or comparable performance to baseline tokenizers\nwhich results in low efficiency and limited usability. In this study,\nTable 2. Discriminative task performance: ASR on librispeech-100\n(SID) on VoxCeleb. The best result in each column is bolded.\nthe results for α = 0 and α = 1, where the tokens are fine-tuned\nWe first evaluated the performance of Phonological Tokenizer on\nThe results are shown in Table 2.\nperformance than Discrete WavLM, demonstrated clear superiority\n4See Sec.4.5 for the results of the ablation study\nOverall, the results indicate that our Phonological Tokenizer suc-\nWe then evaluated performances on generative tasks. We trained unit\nThe results are shown in Table 3.\nOverall, the results showed that the Phonological Tokenizer is\nTable 3. Generative task performance: reconstruction on in-domain LJSpeech & voice conversion on out-of-domain neutral read speech\n(TIMIT) and expressive speech (Expresso). The best result in each column is bolded, and those that outperform all baselines are underlined.\nTable 4. SpeechLM performance: sWUGGY and sBLIMP for lexi-\ntion, which is consistent with the ER and SID results in Sec. 4.2.\nuated their performance. We used the slam recipe [44] based on\nThe results are shown in Table 4. The results of ZeroSpeech,\nspeaker consistency results from SALMon, largely align with the\nASR, ER, and SID results reported in Sec. 4.2. In ZeroSpeech,\nhigh scores contrary to the results of ER and SID, but the Phonolog-\nical Tokenizer achieved the best performance in both GenPPL and\nUTMOS. These results highlights that the speechLM constructed\nFig. 2. Ablation results for the vocoder loss weight α: (a) Discrimi-\nEq.(2)). Fig. 2 presents the results for the discriminative tasks (ASR,\nresults of the discriminative tasks, increasing α gradually degrades\nASR performance while improving SID. However, ER achieves its\nand speaker timbre. For the results of the generative tasks, increas-\ntasks, including speechLMs, demonstrate that the resulting tokens\nachieve strong performance in capturing linguistic and prosodic\ntokens showed the best results in prosody-sensitive tasks (ER, VC,\ndata for enhanced performance, as well as enabling inference-time\n\n\n=== CONCLUSION ===\n5. CONCLUSIONS",
  "2601.13801": "=== METHOD ===\nUser interface design; • Computing methodologies →Vision\nfeasibility of integrating advanced AI models onboard UAVs for\nboth the potential and limitations of current approaches. Large-\ntations [8]. Projection-based approaches like LightAir and MaskBot\nSystem Architecture\nHardware Architecture\narchitecture enables HoverAI to function as a socially present, em-\nHowever, several technical and methodological limitations con-\nincorporating noise-robust ASR models to maintain performance.\nface analysis, HoverAI demonstrates a new approach to spatially-\ntilingual zero-shot text-to-speech model.\n[4] Google DeepMind. 2024. gemma:7b-instruct: Open model for Ollama. https:\nEmbodied AI Agents: Modeling the World. arXiv:2506.22355 Retrieved from\nCognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cogni-\n\n\n=== EXPERIMENT ===\nexperimental environment, wearing headphones for audio input/output while the drone hovers and displays a projected avatar.\n(XTTS v2). Evaluation demonstrates high accuracy in command\nEvaluation\nducted a benchmark evaluation measuring performance across\nExperimental Setup\nCritically, our evaluation focused on technical performance met-\ncomparative experiments with non-adaptive or non-visual drone\ncal direction: our current evaluation was conducted in controlled\naware human-drone interaction. Evaluation across 12 participants\n\n\n=== RESULTS ===\nResults\nAs shown in Fig. 4, HoverAI achieved strong performance across\nFigure 4: Performance metrics averaged across 12 partici-\nThe system demonstrates robust technical performance, achieving\nshowed robust performance in speech recognition (WER: 0.181),\n\n\n=== CONCLUSION ===\nDiscussion\nenhances social presence. Future work should therefore include\ntions it does not actually have. To address these risks, future work\nConclusion",
  "2601.22661": "=== METHOD ===\nplay instructions. However, existing models strug-\ntal results demonstrate that our method signifi-\nWith the advancements in Large Language Models (LLMs),\nZhou et al., 2025) or emerging voice design methodologies\nInstruction-based TTS approaches (Vyas et al., 2023; Ji\nTTS. Existing Reinforcement Learning (RL) approaches for\n(b) Role-Play TTS Model\nFigure 1. The overall framework of our proposed method. (a) The pretraining stage of the LALM for computing MCLP. (b) The LALM\nLLM-based TTS models (Chen et al., 2025; 2024; Du\ngrained attributes, whereas recent instruction-based methods\ntions to enhance expressiveness. However, these approaches\nlargely model style as a static property tied to individual\nconstrains their ability to support comprehensive model-\nmodeling perspective, end-to-end speech-language archi-\nthese systems jointly model role-play content and speech\nvised SFT, models often sacrifice acoustic expressiveness\nintroduced to align TTS models with specific quality ob-\nreflect stylistic coherence. Existing RL-based approaches\nleverages the intrinsic likelihood modeling of pretrained\nindex t. At the initial turn (t = 1), the model synthesizes\nof G. For subsequent turns (t > 1), the model additionally\n4. Methodology\nment. However, with the scaling of model parameters and\ntoken with 4 audio tokens. The model is optimized on pairs\nmodel the conditional distribution of stylistically coherent\nmaximizing the expected log-likelihood under the model\nFigure 1(b), the model is optimized to minimize the negative\nThrough this objective, the model learns to synthesize\nWe further align the RP-TTS model using GRPO on a cu-\npolicy and the reference model SFT. The advantage ˆAi is\nmatic Speech Recognition (ASR) model (Huang et al.,\ngating mechanism to prevent the model from learning\nThis formulation effectively creates a curriculum: the model\nand diverse multi-turn environment for modeling stylistic\nturns to focus on immediate context modeling; 2) Content\nclassification model to filter out ”Neutral” speech, retain-\nModel Configuration.\nBase (Wu et al., 2025a) as our base model. It is a 7B-\nWe evaluate our method against three competi-\n• GPT-Audio: An advanced closed-source model devel-\nbackbone model, optimized for general speech tasks.\n• SFT Stage: We fine-tune the base model for 1 epoch\nModel\nError Rate (WER) to quantify the model’s adherence to\nAudio model specifically fine-tuned on ASR (Huang et al.,\nposed method against competitive baselines across two dis-\ntinct settings: (1) W. Audio History: The model is provided\naudios. (2) W/O. Audio History: The model is only pro-\nobserved, our proposed method significantly outperforms\npability of different models to strictly adhere to textual\nHowever, a critical challenge in RP-TTS is that models of-\nOur proposed method effectively mitigates this issue by us-\nour approach achieves exceptionally low CER and Pinyin\nment, our method significantly outperforms all baselines,\n(3.646), approaching the naturalness of the ground truth.\nModel\nAudio-2-mini. Notably, while baseline models show stag-\nour method effectively leverages historical acoustic cues\nwhen available. Specifically, our method achieves a higher\ning that our model successfully utilizes multimodal history\nmodel prone to Reward Hacking.\nof utterances. Consequently, the model achieves the high-\nconstraint, the model hacks the reward by generating lin-\nstyle reward results in an extremely safe model with the\nOur proposed method, by synergizing WER and MCLP\nstructed demonstrate that our method achieves surpassing\nguage models are human parity zero-shot text to speech\nlanguage models are zero-shot text to speech synthesiz-\nmodels. arXiv preprint arXiv:2412.10117, 2024.\nlarge language models. arXiv preprint arXiv:2509.23435,\nmodels. arXiv preprint arXiv:2402.03300, 2024.\nguage models are zero-shot learners. In International\nstructtts: Modelling expressive tts in discrete latent space\nLlm-based emotional text-to-speech model with freestyle\nAudio language models are few-shot learners.\nTo generate objective scene descriptions (S), we utilize Qwen-VL-7B. We explicitly instruct the model to ignore dialogue\nWe generate character profiles (P) by feeding the complete dialogue script of an episode into the LLM. The model is tasked\n\n\n=== EXPERIMENT ===\nLog-Probability (MCLP) as both an evaluation\nevaluation, we construct an RP-TTS dataset with\ntency lies in the absence of objective evaluation metrics and\naudio tokens conditioned on the evaluation audio as contex-\nstylistic divergence. Beyond evaluation, we further incor-\n• Dataset & Experiment: We construct a high-quality,\nmulti-turn RP-TTS dataset with detailed annotations.\nExtensive experiments demonstrate consistent and sig-\nObjective evaluation of speech style remains a significant\ndataset sizes, LALMs pretrained on massive speech corpora\nDuring evaluation, as illustrated in Figure 1(c), we aim to\ntasks. We utilize the dataset Drp constructed in Section 5.\nas a concatenation of the global setup and the dialogue\nTable 1. Dataset statistics across the processing pipeline. The table tracks the volume (Hours, Sentences) and richness (Avg.\nScenes/Speakers) of the data at each stage of filtering and annotation. The final dataset focuses on high-quality drama scenes with rich\nthe RP-TTS dataset from the WenetSpeech corpus via a multi-stage\nances per scene in our RP-TTS dataset.\n5. Dataset Construction\nRole-Playing datasets are either synthetic data centered on\nof role-play, we construct a large-scale RP-TTS dataset de-\nand character descriptions. The dataset will be open source.\n5.3. Dataset Details\nDataset Statistics.\npassing both training and evaluation splits, comprises ap-\nand ensure fair evaluation, we implemented a video-level\nsource material between training and evaluation. From these\nRL Dataset Filtering.\n6. Experiments\n6.1. Experimental Setup\nsubjective evaluation. Red indicates the best result, and Blue indicates the second best.\nτ = 0.2. All experiments are conducted on 32 NVIDIA\nEvaluation Metrics.\nevaluation with 32 native Chinese professional annotators\nusing a curated subset of 31 samples. The evaluation set\n6.2. Experimental Results\nour curated dataset, we observe significant improvements in\nExtensive experiments on a new RP-TTS dataset we con-\nGui, T., and Zhang, Q. Speechrole: A large-scale dataset\ndiorole: An audio dataset for character role-playing in\n\n\n=== RESULTS ===\nzero-shot performance, particularly in timbre cloning via\ndependent performance. Unlike timbre, which is largely\net al., 2025a). However, RL performance is fundamentally\ndeeply captivated by the art of performance and the complexities of love, is \nsourced from ”YouTube” with the ”drama” tag, resulting in\nTable 2. Main performance comparison with baselines. MOS is reported with 95% confidence intervals. ’⋄’ indicates that samples were\nTable 2 presents the comprehensive performance of our pro-\nAudio-2-mini exhibits a significant drop in performance.\nTable 3. Ablation studies on training stages and reward components. Red indicates the best result, and Blue indicates the second best.\nnant stylistic performance regardless of history availability,\ndence intervals. The results reveal a clear and interpretable\nindicating performance comparable to random guessing. As\nThe ablation results demon-\nperformance compared to strong LALM baselines.\nPlease start your analysis and output the results.\n\n\n=== CONCLUSION ===\n7. Conclusion",
  "2601.12289": "=== METHOD ===\nUnlike existing methods that rely on single-task models or\nmodel to handle multiple paralinguistic tasks such as emo-\nTo-Speech (TTS) generative models. It supports both speech-\nefficient model suitable for real-world applications.\nUnderstanding and modelling speaking styles from speech is\nmodels for each task (Tripathi et al. 2020; Ravishankar et al.\n2020), but this approach is often computationally expensive\nand difficult to scale. Alternatively, multi-task models aim to\nmodels frequently suffer from inter-task interference, where\nonly require accurate modelling of speaking styles but also\nspeech generation. Methods such as CosyVoice (Du et al.\napproach for learning speech representations. It aligns\nprocess. However, CLAP-based approaches face notable\nrequires large-scale models with high computational and\ning framework, ParaMETA, designed for modelling the\ncent advances in vision-language modelling (Zhang et al.\nETA adopts an efficient projection-based approach inspired\n• We design a new approach for controlling speaking\nMethod\nding. ParaMETA is a model-agnostic representation learn-\ncoder backbone architectures to validate its flexibility and\nTTS Model\nTTS Model\nTTS Model\nTTS model to support both speech and text prompts for style\nmodel, enabling the generated speech to mimic the speaking\nstyle prototypes, allowing the model to generate speech\nonly enables TTS models to replicate the speaking styles\nusing a batch size of 32 for up to 40k steps. Model is opti-\nmodel and a version pretrained on music and speech. Par-\nevaluated using its pretrained model. To generate text em-\nventional baseline. We also train a model that learns to align\nan ablation study comparing models trained with and with-\nsults across four speaking-style recognition tasks. Models\ntasks. For instance, the general CLAP model achieves only\nproves performance relative to pretrained models. However,\nthese methods still exhibit negative transfer. Cross-entropy\ntion compared to ParaMETA). Models trained with CLAP-\nTable 2: Speech Generative Model Evaluation\native to CLAP, but the model must implicitly discover and\ndent from others, the model avoids cross-task suppression\n2025), a multilingual and style-controllable TTS model con-\nTTS models conditioned on ParaMETA embeddings consis-\nthe model to generate consistent and expressive speech from\nments in both prompting methods. For text-based prompts,\nporating such details into the TTS model can hinder perfor-\nassess its controllability in TTS model. Given a speech sam-\nMethod\nETA in terms of model size, memory usage, and inference\nspeed. Table 4 compares ParaMETA against baseline models\na significantly smaller model footprint. GPU memory con-\nParaCLAP–Towards a general language-audio model for\nencoders and large language models. In International con-\nto-audio generation with latent diffusion models.\na multi-layer perceptron model. In 2020 11th International\nlanguage models are zero-shot text to speech synthesizers.\nSpark-tts: An efficient llm-based text-to-speech model with\n2018. Style tokens: Unsupervised style modeling, control\nmini: Efficient image and video large multimodal models\nmodeling for speaking style captioning and stylistic speech\n\n\n=== EXPERIMENT ===\nthe utility of style labels in the dataset. Then, in the task-\n• We conduct comprehensive experiments showing that\ngenerality. Specifically, we experiment with convolutional\nthe update rate. In our experiments, we set m = 0.99 to\nTable 1: Subject-Independent Classification Performance Evaluation. The evaluation is performed with multiple runs.\nExperiments\nDataset and Experimental Setup\nWe construct a multilingual and multi-style speech dataset\nable Genshin Impact dataset. The combined dataset cov-\nspeech sample in our dataset into the pretrained text en-\nout the LMETA loss. All evaluations are conducted under\ntraining and testing sets are disjoint. Each evaluation is re-\npretrained on large-scale multimodal datasets (CLAP and\nTraining encoders directly on our dataset significantly im-\nFor generative evaluation, we investigate how different types\nTable 2 presents the evaluation results. The scores that\nemotional multimodal actors dataset. IEEE transactions on\n//en.data-baker.com/datasets/freeDatasets/. Accessed: 2023-\nIto, K.; and Johnson, L. 2017. The LJ Speech Dataset. https:\n//keithito.com/LJ-Speech-Dataset/.\nnew emotional speech dataset. In ICASSP 2021-2021 IEEE\n\n\n=== RESULTS ===\nstyles overshadow others (Zhang et al. 2022). As a result,\nsubspace. The resulting task-specific embeddings are de-\net al. 2020), and the resulting embedding is then projected\nThe resulting embedding is aligned with the corresponding\nParaCLAP) show consistently poor performance across all\nperformance are generally strong in emotion recognition\nproduces competitive results in several settings, but it re-\nand uniformly strong performance. Out of 16 backbone–task\ncombinations, ParaMETA achieves the best result in 12\nThese results highlight ParaMETA’s strength in learning\nstyle representations directly from speech. The performance\nding space, the resulting representations can still be unsta-\nperformance in 8 of 16 backbone–task settings. For most\ning yields better results than text-based prompting, with N-\nintended speaking style, resulting in more natural and ex-\ntively. Language manipulation, however, results in minimal\nfor ParaCLAP and 1966 MB for CLAP, resulting in a 70%\nreduction in CUDA memory usage. These results demon-\nefficient performance, making it favorable for deployment\n\n\n=== CONCLUSION ===\ndiscussion. Although not universally beneficial, it improves\nfuture work.\nConclusion\nIn conclusion, we present ParaMETA, a unified represen-",
  "2601.07303": "=== METHOD ===\ntext-to-speech, voice conversion, and other generation models,\ngeneration models, either component can now be modified\nmodels. Their outputs are fused and mapped to five-class pre-\ndictions. The separation and anti-spoofing models are jointly\nPERFORMANCE (EER % AND F1-SCORE) OF BASELINE MODEL ON THE\nmethod does not generate the scores, they MUST use a hyphen\n• Publicly available pre-trained models released prior to\n• Participants are encouraged to report model parameters\nText-to-Speech models, voice conversion models, or any\nother audio generation models–is strictly prohibited.\nSpeech Synthesis with Graph-Based Multi-Modal Context Modeling,”\n\n\n=== EXPERIMENT ===\nDeepfake Detection Challenge Evaluation Plan\nposed CompSpoofV2 dataset1 and a separation-enhanced joint\ndataset designed for component-level audio anti-spoofing, which\n1CompSpoofV2: https://xuepingzhang.github.io/CompSpoof-V2-Dataset/\nspoofing audio dataset CompSpoof [1] in 2025. Then, we\nextend the dataset to CompSpoofV2, which contains more\nII. DATASET\nThe CompSpoofV2 is a dataset designed for component-\nupon the CompSpoof dataset [1], CompSpoofV2 significantly\nand class distribution. Similarly, the evaluation and test sets\nFIVE CATEGORIES CONSIDERED IN COMPSPOOFV2 DATASET.\nAUDIO SOURCES FOR THE EVALUATION AND TEST SETS.\nevaluation and test sets include newly generated audio samples\nIII. EVALUATION CRITERIA\nequally to the final evaluation, regardless of sample imbalance.\nVALIDATION, EVALUATION, AND TEST SETS OF COMPSPOOFV2.\nticipants submit evaluation set predictions and the leaderboard\nreflects evaluation set rankings. During the final ranking phase,\nrelease the metadata of the evaluation and test sets to support\nParticipants may use additional public datasets only with\nlist of approved datasets; any dataset not on this list is not\n• The evaluation and test sets must NOT be used for\ndatasets\napproved datasets; any datasets not on this list are not\nTraining, validation and evaluation sets release;\nAdditional dataset approval deadline\n“Compspoof: A dataset and joint learning framework for component-\n“Vggsound: A large-scale audio-visual dataset,” in IEEE International\nevaluation conference, 2020, pp. 4218–4222.\nacoustic scenes 2019 open set, development dataset,” 2019.\nevents 2017, development dataset,” 2017.\nevents 2017, evaluation dataset,” 2017.\n[12] Justin Salamon, Christopher Jacoby, and Juan Pablo Bello, “A dataset\nDetection Dataset,” in Interspeech, 2025, pp. 3908–3912.\nB¨ottinger, “Mlaad: The multi-language audio anti-spoofing dataset,” in\n\n\n=== RESULTS ===\nseparately evaluated for genuineness, resulting in five target classes.\nSystem performance will be evaluated using the Overall\nMacro-F1 score indicates better overall system performance.\ntrained to preserve spoofing-relevant cues. The baseline results\nWe use CodaBench platform for results submission5. Par-\nresults in the final ranking phase, with the best three used for\n5Results Submission: https://www.codabench.org/competitions/12365/",
  "2601.17880": "=== METHOD ===\nciters, this dataset provides a unique resource to advance computational approaches\nto Qur’anic studies, the integration of modern computational approaches, particularly artificial\nfor Tajweed error detection using deep neural models. Recent Kaggle contributions, including the\ntasks, (B) Multimodal approaches integrating speech and text, and (C) Tools and applications that\nwith 30 reciters provide a unique testbed to develop end-to-end ASR models adapted to classical\nerror rate and segmentation accuracy across reciters, while challenges include modeling tajweed-\nhigh-quality timestamps for words and phonemes that benefit pronunciation modeling. It enables\nvoice, facilitating accurate imitation and learning. In addition, style transfer approaches can allow\nstyle transfer, and prosody modeling. Furthermore, it supports the development of multimodal\nscale, providing a unique platform to advance computational modeling and applications in Qur’anic\nthe explainability of tajweed errors or mispronounced words, allowing models to highlight specific\nresearchers. Leveraging recitation at a word level, models could detect rules such as ghunnah,\nmelodic tendencies, and prosody modeling, which quantifies melodic contours and pause structures\n\n\n=== EXPERIMENT ===\nMultimodal Dataset of the Quran\nWe present QURAN-MD, a comprehensive multimodal dataset of the Qur’an that\neach verse (ayah), the dataset provides its original Arabic text, English translation,\nsemantic context. This dataset supports various applications, including natural lan-\nport both research and community applications. The dataset is available at https:\n//huggingface.co/datasets/Buraaq/quran-audio-text-dataset\nTable 1: Feature comparison across prominent Quranic datasets. The proposed QUR’AN-MD is the\nDataset\ndatasets provide the Arabic text alone, others include translations or transliterations, and a few extend\nrecognition, and text-to-speech synthesis. Table 1 summarizes key features of prior Qur’anic datasets,\ncomprehensive multimodal dataset of the Qur’an. Our dataset unifies Arabic text, English translation,\ncomponents. By making this dataset available on Hugging Face, we aim to provide scholars, linguists,\nRecitations Dataset [7], provide verse-level recordings across numerous reciters, while the Quranic\nAudio Dataset [8] and RetaSy [9] focus on non-native reciters with labeled correctness annotations.\nQuran Ayat Speech-to-Text dataset [13], Quran Reciters dataset [14], Quran.com Audio dataset [15],\nQuran Recitations for Audio Classification dataset [16], and the Comprehensive Quranic Dataset v1\nDataset (QUR’AN-MD)\nThe dataset was constructed by aggregating and harmonizing three publicly available sources of\nQur’anic data: (1) a Kaggle dataset of verse-level speech-to-text recordings by 30 reciters [18], (2)\nnames, and verse counts) was populated into the template. Second, from the Kaggle dataset, verse-\nrepresentation of the Qur’an, ready for release as a standardized dataset.\nDataset Format: The dataset is organized in a hierarchical JSON structure, where each surah (chapter)\nrecitation, both at the verse and word level. The audio and verse-level transcripts aligned in the dataset\nArabic with the unique prosody and melodic contours in recitations. Evaluations can measure word\nThe dataset can drive progress in forced alignment and phoneme-level segmentation, producing\naging parallel recordings from 32 reciters, the dataset enables the development of personalized\npronunciation and tajweed. Beyond machine learning applications, the dataset can be integrated into\nWe curated QUR’AN-MD, a comprehensive multimodal dataset of the Qur’an from three different\nat both verse- and word-levels, the dataset enables a wide range of research applications, including\nBeyond ASR, the dataset can enable automatic tajweed detection and error classification, an area\nidgham, or madd, with precision/recall measured against expert annotations. Similarly, the dataset\nExample: Surah 112 (Al-Ikhlas): Figure 1 shows a simplified representation of the dataset format\nFigure 1: Example of format of Surah 112 (Al-Ikhlas) in the Dataset.\nTable 2: Overview of the QUR’AN-MD dataset.\nEvaluation (LREC), 2009.\nanalyzed and syntactically-annotated quran corpus. Language Resources and Evaluation, 2024.\n[5] Taha Khan et al. Tarteel: Crowdsourcing a large dataset of quran recitations for speech\n[7] MohamedRashad. Quran-recitations dataset. Hugging Face Dataset, 2024. Verse-level audio +\naudio dataset: Crowdsourced and labeled recitation from non-arabic speakers. In arXiv preprint,\n[9] Ahmad Salameh and colleagues. Retasy: A crowdsourced quran recitation dataset from non-\narabic speakers. In Proceedings of the 12th Language Resources and Evaluation Conference\n[11] A. Osman and colleagues. Qdat: Quranic dataset for automatic tajweed error detection. In\nEvaluation of the pronunciation of tajweed rules based on dnn as a step towards interactive\nrecitation learning. 2025. Uses QDAT dataset for classifying Tajweed rules: Al-Mad, Ghunnah,\n[13] BigGuyUbuntu. Quran ayat speech-to-text dataset. Kaggle, https://www.kaggle.com/\ndatasets/bigguyubuntu/quran-ayat-speech-to-text, 2023.\nQuran reciters dataset.\nKaggle, https://www.kaggle.com/datasets/\nQuran.com audio dataset.\nKaggle, https://www.kaggle.com/datasets/\nhttps://www.kaggle.com/datasets/mohammedalrajeh/\n[17] QuranicDataset. Comprehensive quranic dataset v1 (cqdv1). Kaggle, https://www.kaggle.\ncom/datasets/quranicdataset/quranic-dataset-v1, 2024.\n[18] Bigguyubuntu. Qur’an ayat speech-to-text dataset. https://www.kaggle.com/datasets/\n\n\n=== RESULTS ===\nmissing or misaligned entries. This pipeline resulted in a complete, consistent, and multimodal\n\n\n=== CONCLUSION ===\nenables and promotes future work in cutting-edge computational applications at the intersection of\nFuture Work\nConclusion",
  "2602.01030": "=== METHOD ===\nModels Across Linguistic, Demographic, and Positional Variations\nresentative models under linguistic (language\nThe rapid progress of large language models\nguage models encode various forms of social and\nof model behavior under diverse audio conditions.\nrecognition errors. However, this approach reduces\nGemini 2.5 Flash Preview TTS model, which\nmodel may introduce recognition errors, using two\nuations, this setting requires models to process an\nmodels must not only comprehend the linguistic\nModels and Implementation\nModels\ncluding closed-weight models such as the Gemini\nfamily and open-source models such as the Gemma\n3n, Voxtral, and Phi 4 families. Model details are\niments, we access the models through APIs pro-\npendix C.2 for brevity. For model inference, we set\nprocessing to the model outputs to correct format-\nreflects genuine model behavior rather than arti-\nbut also the stability and consistency of model be-\n• Entropy: Does the model’s answer distribu-\nmodel’s answer distribution:\nFigure 2: Mean question entropy across models. Higher entropy indicates greater uncertainty in model predictions.\nthe nine evaluated models. For each model, we\nmodel displays a larger interquartile range, sug-\nacross questions. Within each model family, the\nModel\ncomparisons across nine models under CS and CA\nhow different variable levels influence model be-\naccent settings. Results for other models, presented\nlar pattern: most models achieve higher accuracy\nbias in model predictions. The effects of other\nure 4 visualizes robustness patterns across model\nation across model families, suggesting that cross-\nFigure 4: Fleiss’ κ versus APES across model families. Variables such as language, accent, and gender show higher\nModel\nall models, typically appearing in the left quadrants\nall, while models exhibit relatively greater robust-\nImpact of Model Scale\nmodel robustness across scales within three repre-\nLarger models consistently demonstrate higher κ\ntion order, larger models also achieve lower APES,\ncloning TTS model, to generate the full 400 ques-\nFigure 6: Effect of (a) reasoning complexity and (b) architectural paradigm on model robustness. Higher reasoning\nrelative model rankings remain consistent across\nreflected by higher APES values across all models.\nity affects model robustness by comparing stan-\nduced variability and stabilizes model predictions\npipeline design. While end-to-end models process\nwhere the model first transcribes the audio into\nsentative models, Gemini 2.5 Flash and Gemma\npipeline setting, both models exhibit higher Fleiss’\nmodel\nble 8, all models exhibit consistently higher APES\ntivities already present in text-based models rather\nSmall (ElGhazaly et al., 2025), and model depen-\nnese and Korean in Meta’s XLS R model (Babu\nmodels exhibiting systematic disadvantages against\nnine representative models from four model fam-\nEvaluated Models\nsource categories. While these models capture di-\nversity in architecture and reasoning pipelines, they\nSome open-source models were excluded due to\nof cross-model consistency and generalization.\nbles: Multilingual speech recognition models exhibit\n2024 Conference on Empirical Methods in Natural\nAre models biased on\nof highly capable multimodal models.\nDiyi Yang. 2024. Modeling gender and dialect bias\nguage models. In The Thirty-eighth Annual Confer-\nguage models. Preprint, arXiv:2505.21148.\nmultimodal language models via mixture-of-loras.\nbias in large language models. In Proceedings of the\nguage model that can speak and listen. Preprint,\n2024. GSQA: An End-to-End Model for Generative\nlanguage models: A task complexity-based approach.\nabilities for large language models. In The Twelfth\nin large language models using response history. In\nits reasoning in large language models. Preprint,\nlanguage models. In Findings of the Association for\nSpeechGPT: Empowering large language models\nmodel inference via the Gemini API incurred a to-\ntal cost of under $550 USD. For the other models,\n120B model via the\nability. The model is prompted with task-specific\nbined to guide the model toward more natural, ex-\nusing the largest-remainder method. For each ques-\nModels\nmultimodal models across both commercial and\nmodels, including their API endpoints provided by\n\"D\") pre-generateed by the TTS model. Instruction\nconstraints (30s) in models such as Gemma and Phi\nModel\nOpen-source Models\nTable 10: Evaluated models.\nModel Inference and Post-processing\nAll models were queried through their official APIs\nModel responses were post-processed to ex-\narise from the same model answering item i under\nmodel\", two independently drawn ratings match\nures 11 and 12 show that across models and vari-\nand gender for the other eight models: Gemini 2.5\ning a more interpretable picture of model behavior\nFigure 12: Grouped APES–κ robustness plots across model families under cross-variable perturbations. Unboxed\nTable 17: Accuracy comparison of Voxtral models under different option-order settings, grouped by language,\nModel\nModel\nModel Scale\naccent, and gender. For all models, the APES dif-\nwithin a limited range. For each model, the ta-\ndiffering across models. Overall, these tables pro-\n\n\n=== EXPERIMENT ===\ndataset, a speech-augmented benchmark based\nevaluation.\nconstruct and release the BIASINEAR dataset, a\nbalanced male and female speakers. The dataset\nbalanced evaluation across languages and demo-\ngraphic factors. b) Leveraging this dataset, we\nBIASINEAR Dataset\ning. The final dataset comprises 70.8 hours (≈\nDataset Construction\nthe diversity of the dataset by excluding STEM-\ngual evaluation.\nHuman Evaluation of TTS Quality\ndataset quality, we complement automatic evalua-\nfiltering, human evaluation forms a two-stage pro-\ncess ensuring dataset quality and consistency.\nExperimental Setup\nExperimental Variables\namine robustness, we conduct our experiments on\nevaluation not only of absolute accuracy but also\nrobustness under diverse experimental settings, in-\ntive audio segments according to the experimental\nEvaluation Metrics\nMost experiments in this work rely on TTS gen-\ntion English Global MMLU Lite dataset for each\nder). To test this, we construct a pipeline setup\ncents in datasets such as SQuAD SRC (Tang and\na unified evaluation across linguistic and demo-\nbased evaluations to the speech modality, examin-\nAlthough our dataset system-\nthat our current setup fully captures the natural\nour dataset is derived from Global-MMLU-Lite,\ncould broaden the evaluation as standardized inter-\n2022. Evaluation of off-the-shelf speech recognizers\nEvaluation Conference, pages 6001–6008, Marseille,\nsrc: A dataset for multi-accent spoken reading com-\nAll experiments involving TTS generation and\ntemporarily free during the experiment period.\nDataset Construction Details\nExperimental Setup Details\nexperimental variables. This ensured consistent\ninputs across all experimental variables.\nSetup.\nDetailed Experiment Results\n\n\n=== RESULTS ===\nperformance on a wide range of downstream tasks.\nswering tasks. Taken together, these findings show\noften exhibit systematic performance disparities\nThese findings suggest that transitioning QA tasks\nTable 3: Manual annotation results by language with\ntions and then average the results over 400 ques-\ntions per setting. The results reveal that the Gemini\naccuracy (%) for the original and reversed option orders, with ∆denoting the difference between them. Results are\nbustness results under cross-variable perturbations\nperformance of Gemini 2.5 Flash across Chinese,\nunder the original configuration. These findings\nsentative families. Results for Gemini 2.0 family\nparison less meaningful. These results suggest that\nResults in Appendix D.4 (Tables 21–22) show that\nsistent with our main findings.\ngeneration, suggesting that our findings are not\n(0.5×, 1.0×, and 1.5×). Results in Figure 5 in-\ning) modes. Figure 6(a) reports results for Gemini\nCoT prompting alone. These results suggest that\nthe final predictions. Taken together",
  "2601.04656": "=== METHOD ===\nReward models\nReward models\nguage Models (LLMs) and refined post-training techniques. A notable breakthrough is zero-shot\never, existing instruction-driven models often struggle either to faithfully follow the instructions or\nContent Conflict’. In standard supervised training, models tend to over-rely on the strong acoustic\ndition the model, but to actively decouple these factors and enforce instruction adherence against\nidentity. FlexiVoice is built on top of a pre-trained large language model (LLM). The LLM core\nand content; and (3) Instruction GRPO leverages an audio-language model (ALM) reward to gen-\nthe promise of RL-based alignment for targeted aspects of TTS. In contrast, our approach adopts a\nFlexiVoice-Base is a pre-trained model of FlexiVoice. It is pre-trained using Emilia (He et al., 2024)\ncoverage. Together, these resources form the pre-training corpus for our model.\ncurriculum ultimately yields a robust multi-modality instruction TTS model, FlexiVoice.\nDPO directly aligns the model’s emotional output with the instruction and reference speech without\nrequiring an explicit reward model (Rafailov et al., 2023). The preference dataset D consists of\nwhere πθ is the policy model and πref is the reference model, both initialized from FlexiVoice-Base.\ntext contain styles conflicting with instructions. After DPO training, the model follows emotional\nas a style constraint, penalizing the model if it leaks style from the reference or text. Conversely, rsv\ntage, the model is forced to decouple these factors to maximize the total reward. We use the same\nthe first stage, where rewards are available from SER models, assessing the consistency between\nWe instead adopt the open-sourced Kimi-Audio-7B-Instruct (Ding et al., 2025) as the reward model\nTo equip the model with fundamental instruction-following capability in the pre-training phase, it is\ntual metadata is processed with an LLM-based annotator. This approach enables efficient generation\nTo evaluate the model’s multi-modality controllability as well as its capacity to fol-\nModel\n(DSD), and Role-Play (RP)—allowing us to rigorously evaluate the model’s versatility in handling\nGemini as the judge model to assess consistency, and report macro-average accuracy.\nthe above open-source systems and closed-source commercial models (gemini-2.5-flash-preview-tts,\nInstruct, a recent large audio-language model with strong instruction-following capability.\nModel\ninstruction, TO-easy only includes the text condition (where the model uses a random timbre), while\nmodel uses the reference’s timbre). Most baselines cannot support both simultaneously, and even\ntest whether the model can ignore conflicting emotional cues in text. Baselines and FlexiVoice-\nsynthesis. The FlexiVoice-Base model, lacking explicit disentanglement capabilities, tends to clone\nand open-source models on both languages.\nModel\nnomenon is consistent with prior findings (Li et al., 2023), which indicate that standard ASR models\nthereby testing different aspects of instruction adherence. As shown in Table 4, the pre-trained model\nand reducing the gap to Gemini models. Overall, FlexiVoice consistently outperforms all open-\nalignment provided by S1, the model struggles to optimize for the sparse and high-level rewards in\nPre-trained model (Base), the effect of starting with Instruction GRPO (S3 first), Joint Training, and\ncomplex instructions (74.8) compared to our approach. This indicates that applying the fundamen-\n”cold start” foundation, establishing robust multi-modal responsiveness before the model tackles\nsimultaneously, since they both apply GRPO algorithm. We compare our progressive approach\nOptimizing them jointly leads to interference, preventing the model from mastering either task at\nboosts the model’s ability to separate style from content and timbre, raising the Decoupling Avg.\nto 88.5. Finally, the Instruction GRPO (+S3) further enhances the model’s capability on complex,\nall metrics, representing substantial improvements over the base model and validating the progres-\nresearch purposes, and we encourage responsible use of the proposed methods.\nmodel architecture, training objectives, and evaluation protocols are provided in the main text and\nappendix. We will release the instruction–speech dataset, model checkpoints, and all training and in-\nZhang, and Xiren Zhou. Phi-3 technical report: A highly capable language model locally on your\nlanguage models. arXiv preprint arXiv:2412.10117, 2024.\nspeech models. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and\nZhizheng Wu. Nvspeech: An integrated and scalable pipeline for human-like speech modeling\nfor generative modeling. In 11th International Conference on Learning Representations, ICLR\nFinn. Direct preference optimization: Your language model is secretly a reward model. Advances\ngual codec language modelling. In Proceedings of the 32nd ACM International Conference on\nMODEL STRUCTURE\nAs illustrated in Section 3, our model mainly contains two stages: auto-regressive LLM and flow\nmatching. In the first stage, the model receives the inputs of text, instruction, and reference speech.\n2023) in this case). The model structure is shown in Figure 3\nalign, and those where they conflict. This approach helps guide the model to learn the ability to\nability, respectively, as reference speech. This enables the model to learn to decouple timbre and\nTo enhance the model’s performance in complex instruction-following\nWavLM-large-finetuned (Chen et al., 2022) respectively. Given the DPO optimized model, we train\nResults show that models using speaker verification as the reward signal achieve significant improve-\nThis occurs because models computing speaker embeddings (WavLM in this case) do not rely solely\nSpeaker verification signals, as a more loose form of speaker similarity, can accommodate models\nVALIDATION OF REWARD MODELS\nFor the complex instruction following in Stage 3 (S3), we require a robust reward model to evaluate\nas a reward model for GRPO is impractical due to its prohibitive inference costs and high latency dur-\neffectively captures the preference patterns of the larger model, providing valid and efficient signals\nTable 7: Agreement evaluation (Macro-F1) between our surrogate reward model (Kimi-Audio-7B)\nthe effectiveness of using Kimi-Audio as a reward model.\nModel\npitch. A higher correlation indicates that the model more accurately follows the gradient of the\nattributes. Notably, it significantly outperforms the baseline models and the pre-trained FlexiVoice-\nstrong speech and audio expertise. We selected two baseline models with relatively comprehensive\ntask support, our pre-trained model, and FlexiVoice for subjective evaluation. For each task in each\nlanguage, we randomly select five paired samples from each model and the ground-truth. To ensure\none generated by a model) under the condition of a given target emotion. The primary focus is on\ndecoupling ability of the models.\npleting in approximately 2 hours. We train the model for 3 epochs with a learning rate of 1 × 10−5\n\n\n=== EXPERIMENT ===\nstrong capability in decoupling control factors. Human evaluations further con-\nOpen-sourced datasets\nble controllability, we first construct a large-scale and diverse speech dataset with natural language\ninstructions, named FlexiVoice-Instruct. The FlexiVoice-Instruct dataset is annotated with the help\nemotion datasets and the InstructTTSEval (Huang et al., 2025) benchmark. The experimental results\ncontrol evaluation, and demonstrates strong performance on complex instruction tasks. Human\nevaluation results further confirm the naturalness and robustness of the generated speech.\n• We develop a large-scale and diverse speech dataset with natural language instructions,\nInstruction-Speech Dataset\nInstruction-based TTS has driven datasets that couple text descrip-\ndataset targets higher-quality, more natural annotations to strengthen instruction-following.\nla",
  "2601.18438": "=== METHOD ===\nlearning-based assessment models rely primar-\nmodels pairwise quality preferences by directly\nadvancement of speech generation models and the\nation methods. However, the gold standard for\nspeech approaches natural quality, perceptual dif-\nmotivates speech quality assessment models that\nAs a result, models trained solely on MOS super-\ntraining objective by explicitly modeling paired\n• UrgentMOS aligns model training with practi-\narchitecture and probabilistic MOS prediction, en-\nemploys self-attention–based temporal modeling\nproves data efficiency and model compactness by\nsmaller models using pseudo-labels on unlabeled\nwhere model improvements across iterations are\nframeworks that jointly model pair construction\nFigure 1: Overview of the UrgentMOS architecture and training paradigm. UrgentMOS consists of an Absolute\nnaturalness-related metric group to model pairwise quality preferences using cross-attention. LCE denotes cross\npairwise model that directly compares homolo-\nscarcity, some approaches derive pairwise supervi-\nPreference-based modeling has also been explored\nlanguage models with structured textual queries to\nleverages large speech–language models as general-\ncated judgment models to achieve human-level\nNevertheless, these methods remain largely experi-\nComplementing these model-centric approaches,\nMethod\nThe overall architecture and training pipeline of Ur-\nMOS explicitly models relative quality judgments\nNCPM employs cross-attention to model interac-\nModel Components\nUrgentMOS adopts a multi-branch, architecture-\nric prediction and preference modeling modules.\nThis design enables multi-resolution modeling\nwork architectures, such constraints may unneces-\nsarily limit model capacity and flexibility. Urgent-\nMOS instead adopts a data-driven approach that\nfying the model architecture. Concretely, for each\ntention heads, a model dimension of 768, and a\nare set to be equal. The model is trained using\nJudge models perform well on their own bench-\nindicating that existing MOS models have limited\nperformance. Models trained using reference-level\nMoreover, models trained with naturalness-\nModel\nModel\nwith naturalness-conditioned preference modeling,\nand pairwise preference evaluation, aligning model\ncent language-based evaluation approaches. Nev-\nversatile speech generation models. arXiv preprint\nintelligibility and quality assessment model using a\nPrompting audio-language models for audio quality\nlarge audio language models.\nin few-step generative modeling.\nDiTAR: Diffusion transformer autoregressive model-\nattention model for multidimensional speech quality\nment using a crowdsourcing approach: evaluation of\nstandardized methods. Quality and User Experience,\nspeech enhancement methods: Issues and perspec-\ntion of speech quality (PESQ)—a new method for\nlingual speech enhancement testing: Approach and\ning Model Designed for Speech Enhancement Com-\nnecessarily reflect intrinsic model efficiency. By\ntion of the tie threshold δ. Models that directly pre-\noptimal δ varying across models. Even when an\nmodeling preferences rather than deriving them\nModel\nThe Use of Large Language Models\nModel\nFigure 4: Preference accuracy on the SpeechEval test set versus tie threshold δ. Direct preference models are\n\n\n=== EXPERIMENT ===\nheterogeneous datasets. In this work, we pro-\nlarge-scale, multi-source datasets. Beyond ab-\nbenchmarking. Extensive experiments across\na wide range of speech quality datasets, in-\ntive evaluation settings.\nbased evaluation paradigms that directly assess rel-\npoorly aligned across datasets due to differences\nhuman MOS annotations across datasets, Urgent-\ning on a large and diverse collection of datasets\ndemand for comparative evaluation, UrgentMOS\nspeech samples from preference-annotated datasets\nACR datasets, enabling unified support for both ab-\nsolute and comparative speech quality evaluation.\ndatasets with incomplete annotations.\ncal evaluation protocols by directly supporting\ntated speech quality dataset derived from ex-\nisting ACR datasets, providing a benchmark\n• Extensive experiments on diverse speech qual-\nity datasets show that UrgentMOS achieves\nand comparative evaluation settings.\nacross datasets. NISQA-MOS (Mittag et al., 2021)\ndata. However, in practical evaluation scenarios\nsion from existing MOS datasets (Shi et al., 2025b).\nbased evaluation that better reflects human percep-\nQualiSpeech introduces a large-scale dataset with\nthen used by both evaluation modules.\nin practice, different datasets provide heteroge-\nComparative evaluation protocols such as CCR are\npublicly available CCR datasets remain limited in\nscale. In contrast, ACR datasets with MOS anno-\nsamples. To mitigate biases from dataset-specific\nrestricts pairs to samples from the same dataset.\nExperiment Setup\nDatasets\nspeech quality datasets spanning text-to-speech\ndition, preference-annotated datasets, including\nEvaluation\nEvaluation is conducted using a combination of\nassessment scenarios. For datasets that provide\nstandard evaluation splits, we directly adopt the\nofficial test sets. To further align evaluation with\ndatasets used either do not contain such paired annotations or\nis applied only to datasets that contain sufficient\ndatasets, the total number of constructed preference\n{A ≻B, B ≻A, tie}. For absolute evaluation, we\nPreference-based Evaluation\npreference-based datasets (SpeechEval and Speech-\nties. For derived preference datasets, acc0.5 uses\nTable 2: Training datasets used in UrgentMOS. “pairs” refers to preference-labeled sample pairs.\nDataset\nscope matching for evaluation.\nDataset\nto other domains., indicating dataset-specific over-\ndomain datasets, demonstrating better robustness.\nevaluation datasets. This suggests that a threshold\nseveral datasets, particularly when a single shared\nCorrelation-based Evaluation\nTable 5 presents LCC and SRCC on datasets with\ntions across all datasets, demonstrating better cross-\nTable 4: Preference evaluation accuracy (acc0.5 / acc0). Best results are shown in bold and second-best results are\nTable 5: Correlation results (LCC / SRCC) on representative datasets. Best results are shown in bold and second-best\nscale, multi-source datasets with inconsistent super-\nRec. P.862: Perceptual Evaluation of\ndataset for the evaluation of neural text-to-speech\nprediction with crowdsourced datasets. pages 2127–\nFingscheidt. 2023. Evaluation metrics for generative\nComprehensive training and evaluation of neural\nand interpretable speech quality evaluation. arXiv\nspeech quality assessment dataset with natural lan-\nquality metrics on the Urgent2025-SQA dataset.\nAdditional Evaluation Results\nAdditional evaluation results for preference-based,\nTable 9: Preference evaluation accuracy (acc0.5 / acc0) on additional test sets not included in the main paper.\nideas and experiments are original contributions.\nTable 10: Additional correlation results (LCC / SRCC) on evaluation datasets not shown in the main paper. Best\nadditional evaluation datasets.\n\n\n=== RESULTS ===\nart performance in both absolute and compara-\nstate-of-the-art performance in both absolute\nproved ranking performance when absolute MOS\nperformance in evaluating speech naturalness via\nresulting representation is denoted as Hi ∈Rdi×li,\nResults\ntently achieve strong performance across TTS,\ntably, F1C1M15 shows degraded performance on\nabsolute MOS annotations; The correlation results\nunderlined. Additional results are reported in Table 9 in Appendix F.\nresults are underlined. Additional results are reported in Table 10 in Appendix F.\nDespite its strong performance, UrgentMOS does\nresults of URGENT 2025 Challenge. arXiv preprint\nFévotte. 2006. Performance measurement in blind\na larger set of metrics can degrade performance.\nsetting. Baseline results are reported using their re-\nAdditional Metric Correlation Results\nperformance across all thresholds, as they do not\nresults are in bold, second-best are underlined. CHiME-7. refers to the CHiME-7-UDASE-Eval set.\nTa",
  "2601.11027": "=== METHOD ===\nWenetSpeech-Wu: Datasets, Benchmarks, and Models for a Unified\nrelease a suite of strong open-source models\nand models to support future research on di-\nof powerful open-source models (An et al., 2024;\ndata, benchmarks, and models mutually reinforce\nbenchmarks, and accessible models has substan-\nModel\nsubstantial challenges for speech modeling. More-\ndifferent methods. At the model level, even foun-\nand text-to-speech synthesis (TTS) models, are ei-\nsource models trained on WenetSpeech-Wu, includ-\ninstruct TTS models, which substantially outper-\nthereby establishing strong models and empirically\n• We open-source strong models trained on\nWu Dialect Speech Processing: Models\nfoundational ASR and TTS models whose perfor-\nstanding models and instructing TTS systems tai-\nquence, existing studies typically evaluate models\ndatasets, benchmarks, and models.\ntwo pretrained ASR models using 880 hours of\nself-supervised learning (SSL) model, and Step-\nAudio2-FT (Wu et al., 2025). These models are\nASR models, Dolphin (Meng et al., 2025) and\nmodel weights via grid search. The fused results\net al., 2025), a large language model, aiming to\nStep-Audio2-Wu-ASR model. Additionally, sub-\naverage score across all pairs, reflecting the model’s\nmodel’s ability to follow emotion-related instruc-\nStep-Audio2-Wu-Und model. A sample is consid-\nnote open-source baselines, commercial models, ASR\nmodels trained on WenetSpeech-Wu, and annotation\nmodels trained on in-house data. Bold numbers indicate\nModel\nASR Models\nAnnotation Models\nModels & Experiments\ning models, we develop models for the two core\nThese include ASR models and unified speech un-\nderstanding models for understaning, as well as\nTTS and instruct TTS models for generation.\nASR models. To accommodate different appli-\nASR models at three scales: a Conformer-U2pp-\nWu (Wu et al., 2021) model, a Whisper-Medium-\nWu (Radford et al., 2023) model, and a Step-\nAudio2-Wu-ASR model. The Conformer-U2pp-\nWu and Whisper-Medium-Wu models are trained\nModel\nWu-ASR model is a Step-Audio2-mini fine-tuned\net al., 2025). All models are pretrained on the ASR-\nthe-art performance across all model scales, with\nSpeech understanding models. For speech un-\nmodel on the WenetSpeech-Wu corpus using task-\naware, quality-graded data. The model is first pre-\nmodels, including Step-Audio2-mini and Qwen3-\nresults, respectively; light green rows indicate models trained on WenetSpeech-Wu or further fine-tuned on an\nModel\n* Single-speaker finetuned model, and speaker similarity is not evaluated.\nmodels. Comparison with Step-Audio2-mini il-\npared with Qwen3-Omni, our model shows notable\nTTS models. In the continual pre-training (CPT)\nprocessed data, enhances the model’s fundamental\nCosyVoice2-Wu-SS approaches or surpasses the\nInstruct TTS models. The instruction training\nTable 7: Performance of instruct TTS model.\nmodel before instruction fine-tuning, the results\nmarks, and models. We introduce WenetSpeech-\nmodels, and instruct TTS models, enabling commu-\nanced, which may affect model generalization to\nmanual labeling. Finally, our baseline models are\nthrough more specialized modeling and training\ntion models for natural interaction between humans\nto-Speech: A Linguistically Motivated Approach.\nwith large language models. CoRR, abs/2412.10117.\nVox-profile: A speech foundation model benchmark\nlanguage models. CoRR, abs/2106.09685.\nlarge-scale automatic speech recognition model for\nvey on Speech Large Language Models for Under-\nguage Models for Multitask Understanding in Low-\nto-end model for speech recognition.\nmandarin speech recognition models from encoder-\nThe training hyperparameters for different models\nvide a more detailed analysis of model perfor-\ntion across WS-Wu-Und and baseline models. As\nshown in Figure 5, our model achieves more bal-\nTable 9: Optimization and training hyperparameters for proposed ASR and TTS models.\nModel\n\n\n=== EXPERIMENT ===\nthis dataset, we introduce WenetSpeech-Wu-\nproposed dataset. Together, these contributions\nopen-source proposed datasets, benchmarks,\ndiverse datasets (Zhang et al., 2022; Zen et al.,\ndimensional evaluation benchmarks (Sakshi et al.,\ntem. The lack of sufficient datasets, standardized\n✗indicate experimentally verified availability, unverified availability, and absence of dimension, respectively. Ab-\nDataset\nthe only publicly available dataset, MagicData-\nrecognition (ASR), with no open datasets available\nprocessing tasks. From an evaluation perspective,\n3https://magichub.com/datasets/\ndialects. Building upon this dataset, we introduce\nsystematic evaluation of Wu dialect speech pro-\ndataset. Together, these contributions lay the foun-\ndating the effectiveness of proposed dataset.\nSpeech Datasets for Low-Resource\nfor Sichuanese. However, these datasets primarily\nthrough ASR and TTS experiments, without val-\nopen-source Wu dialect dataset, providing merely\nand Evaluation\nstandardized benchmark for evaluation. As a conse-\ndataset with multi-dimensional annotations, as il-\nDatasets\naudio quality measures. The dataset comprises ap-\negy. In the following, we describe the dataset statis-\na unified platform for fair evaluation.\ning strict filtering criteria. For evaluation, speaker\ncludes two evaluation sets for instruct TTS. The\nspeaking rate and pitch. For evaluation, two experi-\naccuracy is reported as the evaluation metric (Gao\n(EMOS) (Cho et al., 2025). The evaluation involves\nset. Evaluation is performed on the ASR test set of\nhouse test sets are strictly out-of-set evaluations for\ninternal high-quality dataset.\nTTS-Mid dataset for ten epochs. In the SFT stage,\nwe use TTS-High dataset trained for three epochs.\na 10-hour internal high-quality dataset.\nAs shown in Table 6, the experimental results\ndata are from the Inst Pro and Inst Emo datasets as\ncessing ecosystem encompassing datasets, bench-\ninstruct TTS. Building on this dataset, we release a\njective and Interpretable Prosody Evaluation in Text-\nDataset For Large-Scale Speech Generation. In Proc.\nKeSpeech: An Open Source Speech Dataset of Man-\nDetails of Datasets\nDetails of Experiments\nSupplementary Experimental Results. To pro-\nPrompt Templates Used in Experiments. For\nunderstanding and instruct TTS experiments.\n\n\n=== RESULTS ===\npetitive performance across multiple tasks and\nSOTA-Performance\nbels. As a result, the usability of such resources for\nrather than a primary research target, resulting in\nto-noise ratio (SNR), resulting in a high-quality\nmulti-speaker scenarios. Performance is evaluated\nsamples, for a total of 1,000 samples. Performance\nTable 4: ASR results (CER%) on various test sets.\nbest results; underlined numbers indicate second-best\nresults.\nSpeech understanding performance on\nTable 6: TTS results on WenetSpeech-Wu-Bench. Bold and underlined values denote the best and second-best\nthe second-best results. For the Wu-to-Mandarin\nsis performance in terms of Wu dialect capabil-\n(SS-SFT) stage achieves the best performance\nand future work may further improve performance\nanced and consistently higher performance across\nFigure 5: Supplementary comparison results on age,\n\n\n=== CONCLUSION ===\nConclusion\nlimitations remain to be addressed in future work.",
  "2601.04029": "=== METHOD ===\nSpeakerSleuth: Evaluating Large Audio-Language Models\nLarge Audio-Language Models (LALMs) as\njudges have emerged as a prominent approach\nnine widely-used LALMs, we find that models\nsame speaker’s turns, some models overpre-\nlenient. Models further struggle to identify the\nformance degrades dramatically as models pri-\nfor a speaker. On the other hand, models per-\nverification methods to assess speaker consistency\nMost approaches (Zhang et al., 2024b; Lee et al.,\nbedding models (Desplanques et al., 2020; Chen\net al., 2022). However, these methods face funda-\nModels\n2025a,c). Unlike embedding-based methods that\nand compare embedding methods and LALMs for\nand embedding-based methods on speaker consis-\nand embedding methods reveals critical insights\ninto their capabilities. We find that models struggle\nsistent decisions where some models are too strict\nIn contrast, embedding methods achieve stronger\nsistent model-specific biases.\nand embedding methods, revealing that mod-\ncated controllable generation approaches. Speaker\ncloning methods (Li et al., 2025; Wang et al., 2023)\nmodels (Guo et al., 2023; Du et al., 2024) al-\nModels exhibit\nchallenges necessitate robust evaluation methods\n(Ribeiro et al., 2011). While neural approaches\nple utterances in a dialogue. Existing approaches\ninclude embedding-based methods (Snyder et al.,\ntion models (Desplanques et al., 2020; Chen et al.,\nment capability, where the model must rely on its\nlem. Given AS, the model predicts:\nSuccess on this task demonstrates that the model\nGiven AS, the model must identify which turn, if\nany, disrupts speaker identity. The model predicts:\nSuccess on this task demonstrates that the model\nmodel is presented with a set of three audio can-\nmodel predicts:\nwhere ˆc is the selected audio that the model believes\nSuccess on this task indicates that the model\nspeaker, we assess whether the model can verify\nFriends, Harry Potter), testing the model’s ability\nmodel’s ability to detect varying degrees of acous-\nModel / Method\nLarge Audio-Language Models\nSpeaker Embedding Methods\nLALMs and embedding-based methods, as the lat-\nModels\n(Ye et al., 2025). These models vary in architecture\nand scale, enabling analysis of how model capacity\nSpeaker Embedding Methods\nWe evaluate three speaker embedding methods us-\nWavLM (Chen et al., 2022). All methods classify\nmance. We evaluate whether models possess sta-\n(S2/S3 at 0.7%/0.0%). In contrast, models such\nmodel (Gemini-2.5-Flash) achieving only 60.1%\nModel\nModel\nand most models scoring below 50%.\nSecond, localization results reveal that models\nindicates that models form impressions based on\nconfirms that models cannot fully exploit even\nSpeaker Embedding Methods.\ntion task, speaker embedding methods achieve\nECAPA-TDNN-based methods achieve high accu-\nwhere models perform relative judgment by com-\n82.6% accuracy and several models exceeding 60%.\nin discrimination, indicating the model can per-\nSpeaker Embedding Methods.\nbedding methods achieve near-perfect discrimina-\ndatasets, particularly for WavLM-based methods\nOur main results evaluate models using only the\nembedding-based methods, LALMs have the poten-\ndramatically (most models >90%) while S2/S3 col-\nlapse (most <10%). Models prioritize textual co-\nmodels essentially default to text-based reasoning.\nfusion. Developing methods to better leverage dia-\nIn our main evaluation, we provide models with\nsent, requiring models to judge consistency solely\ncauses models to default to Consistent judgments:\nanchor, models adopt extremely lenient thresholds\ntext alone, models rely heavily on explicit reference\ntask (Table 2), where models achieved substantially\ntal limitations: models lack stable internal thresh-\ndata generation methodology can be readily ex-\ntion of model robustness across specific acoustic\nFinally, we did not explicitly analyze model bi-\nacoustic features in audio-language models.\nlatest models, with an improved gemini 2.5 flash and\n-latest-models-with-an-improved-gemini-2\nlanguage modeling approach to audio generation.\ntext foundation model for real-time dialogue. arXiv\ndiffusion models. In International Conference on\ngeneration with large language models. In Proceed-\n2025. Styletts: A style-based generative model for\nmultimodal language models via mixture-of-loras.\nmodels. In Forty-second International Conference\nLarge language models sensitivity to the order of op-\nEmpirical Methods in Natural Language Processing\nMichael Seltzer. 2011. Crowdmos: An approach for\n(pesq)-a new method for speech quality assessment of\nlanguage models are zero-shot text to speech synthe-\nlanguage models. arXiv preprint arXiv:2509.18816.\nabling auditory large language models for automatic\nthesis in the era of large language models: A system-\non Empirical Methods in Natural Language Process-\n2025. Omnivinci: Enhancing architecture and data\nmodels as automatic dialogue evaluators. In Proceed-\nus to evaluate model performance on clean speech\nversational context with model limitations, we con-\ncontext while remaining within model context lim-\nGurevych, 2019) text embedding model, while\nSpeaker Embedding Methods Details\nthe model selects the candidate that maxi-\nparison method evaluates each context embedding\nSpeaker embedding methods require threshold τ\nscore across all method-extractor combinations.\nvarying by method, extractor, and metric.\nAudio-Language Models (LALMs) as speaker con-\ntion problem where the model identifies whether\nibration patterns across models.\nquiring models to correctly predict no inconsistent\nThe Discrimination task presents models with three\nwhere ˆaj ∈{A, B, C} is the model’s predicted\nThis task isolates the model’s acoustic discrim-\nModel\nModel\nModel\nLarge Audio-Language Models\nSpeaker Embedding Methods\nModel\nLarge Audio-Language Models\nSpeaker Embedding Methods\nModel\nLarge Audio-Language Models\nSpeaker Embedding Methods\nModel\nModel\nModel\nModel\nModel\nModel\nModel\nModel\nModel\nModel\nModel\n\n\n=== EXPERIMENT ===\nverified evaluation instances across four diverse\ndatasets spanning synthetic and real speech,\nfrom four diverse datasets spanning synthetic and\ncomprising 1,818 evaluation instances from 152\nOur evaluation of nine state-of-the-art LALMs\nfor multi-turn speaker consistency evaluation\nSpeech Quality Evaluation\nSpeech quality evaluation systematically assesses\nSpeaker Consistency Evaluation\nspeaker consistency evaluation measures whether\nLee et al., 2025), and human evaluation (Zhang\ntures, we propose an evaluation framework. Rather\nLLM Evaluation\nfrom four datasets to ensure diversity in conversa-\n(e.g., fillers, backchannels), enabling evaluation on\nDataset\nacross datasets.\n1,818 total evaluation instances. The benchmark\nacross the four source datasets. Table 1 summarizes\nthe dataset statistics, and Figure 3 shows the distri-\nExperimental Setup\nEvaluation Protocol\nOur primary evaluation uses audio-only input with\nfour datasets (Bazinga, AMI, Behavior-SD, DailyTalk). Bold indicates the highest value per column.\naffects consistency evaluation.\nEvaluation Metrics\nacross domains, with dataset averages ranging from\nTDNN reaching 99-100% on most datasets. This\ndiverse datasets. Our evaluation reveals fundamen-\nSecond, while our benchmark includes 4 datasets\nhaviors and motivating more robust evaluation of\nDailytalk: Spoken dialogue dataset for conversational\ndataset for multi-party dialogues structuring. In Pro-\nEvaluation Conference, pages 3434–3441, Marseille,\nstra. 2001. Perceptual evaluation of speech quality\ninterpretable speech quality evaluation.\nspeech quality assessment dataset with natural lan-\nspeech quality evaluation. In ICASSP 2025-2025\nfrom Season 1) from Bazinga dataset in SpeakerSleuth.\nDataset Details\nthe four datasets used to construct SpeakerSleuth,\nparty dialogue dataset collected from TV series and\nmovies. The dataset features scripted but naturalis-\nFor SpeakerSleuth, we utilized the evaluation set\nduced large-scale dataset of synthesized spoken\ninterruptions, a",
  "2601.13910": "=== METHOD ===\nA Review of Deep-Learning-based Singing Voice Synthesis Approaches\nvent of large language models and novel\nand then organizes current architectures into\nend approaches.\ning singing modeling and control techniques.\nmethods such as waveform concatenation (Ken-\nof modern generative models, contemporary SVS\nmulti-speaker models capable of high-quality tim-\nvent of multimodal large language models (Achiam\nadopt an “acoustic-model + vocoder” pipeline (Lu\net al., 2020). In this cascaded approach, an acoustic\nmodel predicts intermediate acoustic features from\nnewer systems as end-to-end approaches. Building\non prevailing SVS architectures, we identify three\ncore technologies: singing-voice modeling, control\nSVS. Section 3 examines the different architectures\nof SVS models. We discuss the method for mod-\nresources of SVS models in Section 5.\nmodeling to enhance naturalness (He et al., 2023),\ntency models (Song et al., 2023), researchers also\nintroduces a DL-based SVS model that controls\nmodel singing techniques. The emergence of LLM\ngeneration models. Examples include FreeStyler\nSVS Model\nSVS Model\nSVS Model\nModel\nsystem with an LM to jointly model prosody and\npotential of end-to-end models in song generation.\nArchitecture\nAcoustic Model\nIn cascaded models, the acous-\ntic model manages the \"music score →spectral\nin the network architecture for SVS. Inspired by\nCascaded SVS Model\nEnd-to-End SVS Model\nModel\nFigure 2: We categorize SVS models into two paradigms, cascaded and end-to-end approaches. A system is end-to-\nTransformer-based TTS models, early DL-based\n2020a) adopts non-autoregressive acoustic models,\n2017) architecture. DeepSinger (Ren et al., 2020b)\nmodel while using a large amount of internet\n2022c) introduces data augmentation methods and\nthermore, diffusion models (Ho et al., 2020), as a\nlable F0 trajectories. These approaches provide\nerative paradigm; flow-matching acoustic models\ndiffusion-based methods (Chen et al., 2020b). At\nsearchers have advanced end-to-end SVS models.\nVITS architecture (Kim et al., 2021b) to SVS, deliv-\nRepresentations and Modeling of SVS\nmonly used representation and modeling choices\nmodel rhythmic uncertainty (Zhang et al., 2022b);\nrecent systems refine alignment modeling, for ex-\nence (Lee et al., 2022a). Among discrete methods,\na learned prior to model frame-level variability\ntent modeling with quantized contrastive learning,\n2023), and this semantic modeling solution may\nponents and Mega-TTS (Jiang et al., 2024) models\n2024) model speaker style and emotion in this way,\nResources in SVS Models\nmethods are outlined in Section 4. Notably, recent\nObjectively, early work often sought to model sub-\nmodel toward a perceptual objective, such as maxi-\nreliable approach remains subjective evaluation, ex-\nas a proxy for the model’s transfer capability.\na model’s ability to precisely manipulate specified\nthe model follows a given instruction, such as al-\ntribute prediction models; for example, an emotion\nrecognition model (Ma et al., 2024) can infer the\nlearning from tasks and architectures to core tech-\nnologies in SVS including singing modeling, con-\nRecent advances in generative models (e.g.\ners (Vaswani et al., 2017), diffusion models (Ho\ntraditional methods such as waveform concatena-\ndeep models yield markedly better voice quality\nwhen applying the reviewed methods. (1) Data\nings. (2) Misuse of generative models. Modern\ntitioners should comply with model developers’\nof high-quality versatile speech generation models.\nwards optimize diffusion generative models. arXiv\non source-filter model. In ICASSP 2024-2024 IEEE\n2020. Jukebox: A generative model for music. arXiv\n2024. Songcomposer: A large language model for\ndecoder acoustic models and wavernn vocoders.\nfusion model from satellite observations. In ICASSP\nnoising diffusion probabilistic models. Advances\nDeep Generative Models and Downstream Applica-\nmodels. Neural Networks.\nsion transformer autoregressive modeling for speech\nSpecmix: A mixed sample data augmentation method\nDong Yu. 2021. Bilateral denoising diffusion models.\nStyler: Style factor modeling with rapidity and ro-\nto-end unified model for text-to-speech and singing\nable approach for high-quality binaural speech syn-\nthesis with flow matching models. arXiv preprint\ning for generative modeling. In The Eleventh Inter-\nation with latent diffusion models. arXiv preprint\nsolver for diffusion probabilistic model sampling in\nand Yike Guo. 2024. Comosvc: Consistency model-\ntts models on complex prosodic, expressiveness, and\nlinguistic challenges using model-as-a-judge. arXiv\nmethod for automatic speech recognition.\nsion models with transformers. In Proceedings of the\nof speech quality (pesq)-a new method for speech\nfacts of high guidance scales in diffusion models. In\nNaturalspeech 2: Latent diffusion models are natural\ngraph architecture for speech emotion recognition.\nDenoising diffusion implicit models. arXiv preprint\nSutskever. 2023. Consistency models.\nvoice synthesis with vibrato modeling and latent en-\ncient foundation language models. arXiv preprint\n2024. Diffusion model alignment using direct prefer-\ninference via fine-tuned vision-language models\nefficient llm-based text-to-speech model with single-\nwith large language model training paradigms. arXiv\nUnsupervised style modeling, control and transfer in\ngrade mandarin speech recognition models from\nemotion captioning with large language model. In\nmodel based on generative adversarial networks with\nsinging voice synthesis via consistency model. In\nopen foundation models for long-form music genera-\non Empirical Methods in Natural Language Process-\nacoustic model and effectively improves robust-\nbustness, and yield more controllable SVS models.\ntraining methods are generally based on the fol-\ntypical schedule trains the acoustic model and du-\nFor consistency-model frameworks, lightweight\nteacher (often a diffusion model) and serves as its\nInference Acceleration Method\nIn addition to the consistency model, rapid progress\nin deep generative modeling (Ho et al., 2020; Liu\net al., 2025b). Beyond step-size reduction, methods\nArchitectures of SVS\ndirection, benefiting from direct modeling. In con-\ntivity to alignment and temporal modeling (Pee-\ntoregressive models, and the progress of MLLMs\napproaches that combine autoregressive and non\nautoregressive modeling, such as DITAR (Jia et al.,\n2023; Kim et al., 2021b). A dual-track approach\nexplore sparse architectures such as Mixture-of-\nforcement learning to align model behavior with\nModels\nboth new methodological insights and practical\nMLLMs may contribute by modeling the pragmatic\nThanks to their strong long-sequence modeling\nlarge models have become foundational in end-to-\nMLLM-like architectures to generate high-quality\nner. These models capture complex dependencies\nfoundation models such as AudioGen-Omni (Wang\nthis approach has been already widely applied in\ninformative model iteration and paves the way to-\n\n\n=== EXPERIMENT ===\nFinally, we review relevant datasets, annota-\ntion tools, and evaluation benchmarks that sup-\nOpen-Source Datasets\nHigh-quality datasets are the foundation of effec-\nture singing’s intrinsic complexity, making dataset\nTable 1: Overview of publicly available singing voice datasets. Lang denotes the number of supported languages.\n2022a). Recent datasets add musical-score anno-\nEvaluations for SVS\n2021). This section details a range of evaluation\net al., 2024, 2025) introduces a dedicated dataset\nquality evaluation practices in SVS were adopted\nused in evaluations like the Singing Voice Conver-\nthe objective side, evaluation typically relies on at-\nlect and demonstrate datasets, annotation tools and\nevaluation benchmarks for SVS systems. Through\ndataset for singing voice research. In International\nlingual singing dataset with style transfer. In ISMIR,\ndio dataset with refined annotations. arXiv preprint\ntensive, multilingual, and diverse speech dataset for\nmonaural recordings using",
  "2601.17086": "=== METHOD ===\nnoEdit, a model editing technique that surgically corrects pronunciation errors\nin pre-trained TTS models without retraining. Correcting such errors tradition-\nwords while provably preserving the rest of the model’s behavior. We first adapt\ning a constrained update that drives the model’s acoustic output toward a desired\nsimilar PEFT methods reduce computational cost but still require task-specific training and lack\nexplicit guarantees about preserving general model behavior. Moreover, prompt engineering for\nWhere does pronunciation live in a large language model(LLM) based text-to-speech(TTS) system, and\nhow can we surgically modify it for specific texts while preserving all other model behavior?\nThis ensures that the correction cannot affect any aspect of model behavior other than the target\n3. Parameter Parsimony and One-Shot Correction: Unlike PEFT methods, SonoEdit does not in-\nnious modifications to LLM-based TTS models that are guaranteed not to introduce catastrophic\n2.1. LLM-based Text-to-Speech Modelling\nRecent advances in neural text-to-speech (TTS) have demonstrated that Large Language Models\nlanguage modeling for audio generation, while VALL-E [4] demonstrated zero-shot voice cloning\nral codecs like EnCodec [9] and SoundStream [10] to discretize audio into tokens.Models such as\ntraditional concatenative or unit-selection methods.\nThese LLM-based TTS approaches operate by conditioning the speech decoder on embeddings de-\non in-distribution data, these models exhibit a critical failure mode: the hallucination of incorrect\nUnderstanding and modifying the internal representations of neural TTS models is a critical re-\ning [14], and attribution patching [15]. Layer-wise analysis [16] in language models revealed that\nIn the context of TTS, recent work has focused on identifying which components of the model are\nto directly intervene in specific layers to correct pronunciation errors. However, such approaches\nthen input into a Large Language Model (LLM), which generates a sequence of New Tokens. Fi-\nthe model’s general acoustic capabilities or speaker identity preservation.\nOur work builds upon these insights by employing a more principled approach: we use Acoustic\nral networks without catastrophic forgetting. Recent methods such as ROME (Rank-One Model\ncorrecting facts in Large Language Models while preserving the model’s general capabilities.\nThe core insight of these methods is that knowledge in neural networks is often localized to specific\nprojection, interventions can be made orthogonal to existing model behavior, thereby minimizing\ncomputing weight changes that preserve the rest of the model’s output distribution.\nWhile these methods were originally designed for factual knowledge correction in language models,\nmodel encodes facts in localized, structured representations, a TTS model encodes pronunciation\nFull model fine-tuning is computationally expensive and prone to catastrophic forgetting, especially\nwhen correcting errors in specific, localized aspects of model behavior. To address this, parameter-\nefficient fine-tuning (PEFT) methods such as LoRA [21], adapters [22], QLoRA [23], and Prefix\nLoRA, one of the most popular PEFT methods, assumes that weight updates lie in a low-rank sub-\nnumber of trainable parameters while often preserving model performance.\nHowever, LoRA and similar PEFT methods have significant limitations in the context of pronunci-\nmethods can introduce subtle artifacts or degradations in acoustic quality when applied to locally-\neral speech, so corrections can still indirectly interfere with other aspects of model behavior.\nIn contrast, our approach (SonoEdit) operates directly in the null-space of general speech charac-\nto the model’s core acoustic capabilities. By combining causal tracing with null-space constrained\n3. Methodology\nguage Model (LLM) serves as an intermediate planner between text and acoustics.\nprocess can be formalized as a sequential prediction task where the LLM models the conditional\nKnowledge editing frames model correction as a targeted modi-\nother behaviors. Given a model with parameters W, the goal is to compute a minimal update ∆W\nsuch that the model produces a desired output for a particular input, without affecting its responses\nflattened SNAC architecture. We perform interventional analysis by corrupting the input represen-\ntervention. For computational efficiency, we estimate these probabilities using the model’s logits:\nically layers L/2 to 3L/4 for a model with L layers). This precise localization allows us to restrict\nweight update. This makes the approach particularly well-suited for deployed TTS systems where\nout model degradation.\nEvaluation Models: We evaluate SonoEdit on Orpheus-TTS [11], which leverages a LLaMA-3B [27]\nthat are (1) systematically mispronounced by the baseline Orpheus model with error rates exceed-\ncomplementary methods: for well-documented words, we use phoneme-injected synthesis by man-\naccuracy through forced alignment. To assess global model preservation, Global-WER measures\nthe Word Error Rate on the preservation set, ensuring that edits do not degrade model performance\ninfluence). Layers 15-21 consistently show the highest scores across all three methods, identify-\nFigure 3: Three complementary analysis methods show convergent evidence that layers 15-21 en-\nBased on the layer-wise analysis in section 4.2, we edit the model weights exclusively within layers\nexisting methods, which SonoEdit resolves through null-space constraints. Full Fine-Tuning (FFT)\nWER, statistically indistinguishable from the original model confirming that null-space projection\nTable 2: Comparison of editing methods on Orpheus-TTS. Target-WER measures success in fixing\nMethod\nOriginal Model\nactual example of how the models behave on employing different techniques is provided in fig. 4.\nthe model preserves intelligibility, WER, and prosodic structure. Drift values are minimal, demon-\nMethod\nof general speech. This approach prevents catastrophic forgetting: experiments on HardNoun-300\n0.99), significantly outperforming unconstrained methods. SonoEdit offers a parsimonious, one-\nYanqing Liu, Huaming Wang, Jinyu Li, et al. Neural codec language models are zero-shot text\nA language modeling approach to audio generation. IEEE/ACM Transactions on Audio, Speech,\nJiang Bian. NaturalSpeech 2: Latent diffusion models are natural and zero-shot speech and\n[13] Yonatan Belinkov and James Glass. Analysis methods in neural language processing: A survey.\nCausal abstraction for faithful model interpretation.\nmodel editing at scale. arXiv preprint arXiv:2110.11309, 2021.\nLu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1\nllama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\n\n\n=== EXPERIMENT ===\nsteps of coarse tokens across a diverse speech dataset (e.g., LibriTTS[26]). We compute the un-\n4. Experiments\nOur experiments aim to address three major research questions fundamental to the SonoEdit ap-\ntions? To investigate these questions comprehensively, we conduct all experiments in a one-shot\nEvaluation Data: To rigorously test pronunciation correction across diverse linguistic contexts, we\nconstruct HardNoun-300, a carefully curated multilingual dataset of 300 proper nouns (50 per lan-\nEvaluation Metrics: For evaluation, we employ both objective and subjective metrics to comprehen-\n\n\n=== RESULTS ===\nprosody, stress patterns, and intonation contours. However, despite their remarkable performance\nof Uk. The resulting update is applied only to the localized layers identified via acoustic causal trac-\nAs a result, SonoEdit enables one-shot pronunciation correction: a single computation permanently\nperformance. In contrast, SonoEdit achieves 2.8% Target-WER while maintaining 3.15% Global-\n\n\n=== CONCLUSION ===\n5. Conclusion",
  "2601.15596": "=== METHOD ===\nMethodologically, we first identify that discrete speech tokens\nrating a Large Language Model (LLM) for content-style encoding\nresponse, zero-shot speech synthesis, large language model\nscale generative models and massive-scale datasets. Recent\narchitectures leveraging neural codecs, latent diffusion, and\nlarge-scale prompt-based models [16], [17], Voice Conversion\n[19]. However, these approaches face two critical limitations.\nsecond is unsatisfactory acoustic authenticity: existing models\nvoice. Our approach leverages a two-stage architecture: a\nLarge Language Model (LLM) based text-to-semantic encoder\nwithin the token space, which allows a two-stage model to\nstage model that leverages the inherent soft factorization in the\nation methodology integrating objective metrics, human listen-\nThe advent of Large Language Models (LLMs) has pro-\ntic models and hand-crafted features [20], [21], LLM-based\napproaches represent a paradigm shift by redefine speech\nDiverse approaches have emerged within this paradigm.\nresearchers. A common approach is to use self-supervised\nsuch as digital signal processing based methods [35] and\napproaches [18] based on GMM and DNN.\nversion [36], [37]. These approaches hold potential in address-\nthe model with a small amount of target speaker’s whisper-\nstyle speech [19]. While this method effectively generates\nSecondly, zero-shot TTS models using in-context learning\nnique where models adapt to new tasks or generate outputs\nrequiring explicit retraining. This allows models to produce\nFurthermore, recent commercial models, such as Eleven-\nproposed method, is to the best of our knowledge the first\nIII. METHODOLOGY\nLLM based Text-to-Semantic Model\npipeline, decoupling semantic modeling from acoustic recon-\nThe first stage is a LLM-based Text-to-Semantic Model.\nWe employ a decoder-only Large Language Model (LLM)\nAutomatic Speech Recognition (ASR) objective. The model is\ntransformer encoder architecture [2], [41] with loss in Equation\nA core premise of our approach is that discrete speech\nThese observations justify our dual-modeling strategy: since\nto model these dominant stylistic variations. Concurrently,\n(3) Virtual Pool Retrieval, our proposed automated approach.\nby our DeepASMR model to serve as stylistic coun-\nthat the LLM focuses on modeling the target style variations\nprevent the model from overfitting to whispered speech and\nby modeling the flow field from a Gaussian prior. Cru-\nscenarios. This topical breadth allows the model to learn not\nthat can degrade recognition accuracy or mislead the model.\nings that could degrade model performance.\ngenre. This approach ensures the dataset captures the intimate,\n0.5B model [40]. The acoustic decoder is implemented as a\nthe model is pre-trained on 200,000 hours of internal data for\nand acoustic model, with 10,000 warm-up steps and a Noam\nthe LLM and acoustic model respectively. For this stage, a\n2.5 Pro model [53] for automated style detection. Through\nprompt engineering, the model scores speech style on a contin-\nmodel’s ability to generate unvoiced speech, we employed\nusing a frame-by-frame approach to extract RMS energy (E)\nunvoiced). A score approaching 100% indicates the successful\nthesis models across two synthesis categories:\ntwo main-stream zero-shot TTS models—CosyVoice2 [16]\nand F5TTS [3]—as baseline models. These models utilize in-\nthese models with our proposed DeepASMR-DB to have a fair\nwork. Standard zero-shot models lack the disentanglement\nyinter using a TTS model conditioned on the auxiliary\n3) Timbre Transfer: We apply a Voice Conversion model\nVC models, CosyVoiceVC [16] and SeedVC [56], creating a\nto style modeling rather than speaker identity or linguistic\nTTS Model\nTTS Model\nVC Model\nTABLE I: Objective and LLM-based Evaluation Results for Different Models under Intra and Cross-Style Synthesis Scenarios\nmodel achieved the lowest error rates across both languages\nhighly competitive. While the fine-tuned CosyVoice2 model\nour DeepASMR model consistently outperformed the zero-\nall models exhibited lower SIM scores compared to intra-\nof the metric than a model failure. The ground truth itself\ntarget ASMR. DeepASMR was the only model to achieve high\ndesign, we benchmarked DeepASMR against baseline models\nauthenticity—performance compared with general models.\nscores, validating the effectiveness of our specialized method\nTTS Model\nTTS Model\nVC Model\nTABLE II: Subjective Evaluation Results for Different Models under Intra and Cross-Style Synthesis Scenarios\npurpose TTS models. This confirms that our model’s special-\napproaching the ground truth.\nfine-tuning phase. Table IV compares the pretrained model,\na model fine-tuned exclusively on DeepASMR-DB against a\nmodel fine-tuned on a mixture of DeepASMR-DB and the\nPretrained Model\nspeech is critical for model stability. We acknowledge that\nN2N WER, restoring the model’s general capability. Further-\nthat maintaining a foundation of modal speech helps the model\nspeech is inherently strong and stable. Once the model re-\nlarger iterative refinement steps serves as an effective method\nTTS Model\nVC Model\nTable VI presents a quantitative evaluation of the models’\nbased approaches struggle to suppress vocal fold vibration,\nIn contrast, the cascade models exhibit inflated unvoiced ratios\nE. Comparison with Commercial Models\nare among the leading commercial large-scale TTS models.\nWhile their specific model architectures and training data\nmodels is predominantly based on vocal cord vibrations.\nModel\nCommercial Models\ncial models currently struggle to replicate this characteristic,\nmodel, where a higher ratio indicates that the speech is more\nunvoiced. We evaluate our proposed method, DeepASMR,\nby detection models designed to discriminate whether an audio\nwith factorized codec and diffusion models,” in Forty-first International\nsynthesis with large language models,” arXiv preprint arXiv:2412.10117,\nlarge language model for speech synthesis: An empirical study,” in\nSeamless speech interaction with large language models,” arXiv preprint\ntext-to-speech model with freestyle text prompting,” arXiv preprint\nshortcoming of codec for audio language model,” in Proceedings of\nlightweight one-shot whisper to normal voice conversion model using\n[35] T. Uchida and M. Morise, “A practical method of generating whisper\nvoice: Development of phantom silhouette method and its improvement,”\nmodel with single-stream decoupled speech tokens,” arXiv preprint\n\n\n=== EXPERIMENT ===\nand introduce a novel evaluation protocol integrating objective\nspeech analysis. Extensive experiments confirm that DeepASMR\n[18], and task-specific fine-tuning on limited ASMR datasets\n3. Large-Scale Dataset: We release DeepASMR-DB, a\npublicly available ASMR dataset.\n4. Comprehensive Evaluation: We introduce a novel evalu-\n5. State-of-the-Art Performance: Extensive experiments\nFig. 4: A comprehensive pipeline for constructing the DeepASMR-DB dataset\nASMR speech dataset) and the Emilia dataset (normal speech\ndataset) [47] for 40 epochs. The mixing strategy is crucial to\nA. Overview of DeepASMR dataset\nbilingual dataset specifically curated to advance research in\ndataset of its kind. The dataset is balanced across two major\nThe dataset is under a CC BY-NC 4.0 license. The copyright\nsamples are provided for review, and the complete dataset will\nthat the dataset remains professionally high-fidelity and pro-\nV. EXPERIMENTAL SETUPS\nconstant learning rate of 1e −5 is used. All experiments are\nB. Evaluation Metrics\nWe adopt a multi-dimensional evaluation protocol compris-\nLLM-based evaluation4.\nD. Testing Dataset\ncomprehensive evaluation benchmark comprising eight distinct\nvariation, we selected datasets that provide paired Normal and\ntesting set and Whisper40 dataset [57] for chinese testing set.\nBy leveraging these paired datasets, we establish a ground\nA. Objective Evaluation\n(CosyVoice2 and F5TTS) explicitly fine-tuned on this dataset.\nB. Subjective Evaluation\nTable II s",
  "2602.02591": "=== METHOD ===\nexisting speech generation models in creating immersive auditory\nbank architecture and a cross-modal hybrid supervision strategy to\noutperforms existing baseline models in terms of audio fidelity, con-\nAlignment, Latent Diffusion Models\nEarly approaches to environment-aware speech generation pri-\nas in DiffRENT [2] and AST-LDM [8]. Although these methods\ndriven approach, attempting to jointly model speech and environ-\ntity but lacking the ability to perceive or model environmental acous-\nchallenges at both the data and model design levels. First, on the\ncally recorded in acoustically “clean” conditions, preventing models\nthe model design level, existing alignment architectures are insuffi-\n(3) We propose an acoustic feature alignment method based on\ndecoupling. The core of this method is our designed Decoupled\n2. METHOD\ntextual prompt to drive both a text-to-image model (FLUX.1) and\na text-to-audio model (Stable Audio Open [13])—to ensure seman-\nreal videos to improve the model’s generalization capability.\ntween vision–language models (VLMs) and large language mod-\ndataset, providing a solid foundation for model training.\n2, the overall architecture consists of three key components: a Con-\nception Pathway, the model receives multimodal scene inputs, in-\nrepresentations, we employ the pretrained CLAP model as the Au-\ndio Encoder and the pretrained MetaCLIP [20] model as the Visual\nModel, which iteratively performs denoising to generate acoustic la-\nMy model is \nFig. 2: The overall architecture of the VividVoice framework.\nto explicitly guide the model in learning decoupling. Specifically,\nsame U-Net structure as the AudioLDM [22] diffusion model and\nthe VividVoice model, we use a set of objective metrics. Word Error\nlated by a pretrained Whisper-Large-v3 model). The overall audio\nTable 1: The experimental results of the benchmark evaluation for different methods. Here, ↑indicates that higher values are better, while ↓\nModel\nSince existing vision-driven models typically address only single-\nmodel significantly outperforms the baseline on the real-world test\nquality. Subjective evaluations reveal that our model scored 3.08\nVoice model with two mainstream fusion methods: (1) w/ Concat-\nstronger Attn-Fusion baseline, our method achieved a 16.0% and\nModel\nconducted an A/B preference test comparing our model with the\nbaseline model (w/ Attn-Fusion). The test included two scenarios:\nwhich model better controlled the varying attribute independently\nsults strongly demonstrate the significant superiority of our model in\nsignificantly outperforms existing models in terms of audio fidelity,\ning audio quality compared to traditional fusion methods.\n[2] Jaekwon Im and Juhan Nam, “Diffrent: A diffusion model for\noldm: Text-to-audio generation with latent diffusion models,”\n\n\n=== EXPERIMENT ===\nconstructed a large-scale, high-quality hybrid multimodal dataset,\ndata level, a strongly aligned dataset that simultaneously includes\ndiovisual scenes but lacks stable speaker identity, while datasets such\ndataset, Vivid-210K. This dataset, built through an innovative pro-\nSynthetic Environment Dataset\ndataset.\nSpeech Synthesis, we constructed a large-scale multimodal dataset,\nspeakers. The dataset consists of a procedurally synthesized pre-\nthe LRS3 dataset. To obtain matching audiovisual environments,\nwe examined existing datasets such as VGGSound, but found that\nTo ensure dataset quality and cross-modal consistency, we de-\nsign an automated evaluation pipeline based on collaboration be-\n98.6%. Furthermore, subjective evaluation conducted by domain\nthe dataset, we introduce Contrastive Disentanglement Supervision\n3. EXPERIMENTS\nAll our experiments are based on the proposed Vivid-210K Dataset.\nSubjective Metrics: For subjective evaluation, we conducted a\n3.3. Evaluation Results\nset across the majority of both objective and subjective evaluation\ndard cross-attention mechanism [26] for fusion. The experimental\nTable 2: Ablation experiment results for the D-MSVA module. w/\n3.5. Evaluation of Decoupling Ability\nsystematically constructed a large-scale multimodal dataset, Vivid-\njective experimental results provide strong evidence that VividVoice\ncontent clarity, and multimodal consistency. Ablation experiments\ndataset for audio events,” in 2017 IEEE International Confer-\nman, “Lrs3-ted: a large-scale dataset for visual speech recog-\n\n\n=== RESULTS ===\nmental results provide strong evidence that VividVoice significantly\nmental acoustics. As a result, they struggle to preserve fine-grained\nThese results strongly demonstrate the reliability of the Vivid-210K\n(Msv). The results are then combined to form the “recalled” scene\nfine-tuning set as the test set to evaluate performance in realistic sce-\nObjective Metrics: To quantitatively evaluate the performance of\nvance filtering in future work. These results strongly demonstrate\nthe superiority of the VividVoice framework in overall performance,\nresults show that our proposed D-MSVA achieves the best perfor-\nFig. 4: A/B Preference Test Results for Decoupling Ability.\n\n\n=== CONCLUSION ===\n4. CONCLUSION",
  "2602.03420": "=== METHOD ===\nbrid TTS architectures, and how such complex\nmodels, introducing a quantitative, controllable\nalso provide a lightweight steering approach for\n2022; Tang et al., 2024). For example, a TTS model con-\npointing news, are flattened in most models. This limitation\nIncreasing label granularity or retraining models with richer\nredefining the conditional distribution of the model, allow-\nadditional conditioning variables or retraining the model,\nases. Crucially, steering vectors do not require the model\ntional expression in TTS models, several fundamental ques-\nWhere to steer? Modern TTS architectures, such as modu-\nlar text-to-speech language model (SLM) followed by flow-\nthese modular systems. Second, we introduce a method for\ntext-independent acoustic steering. This approach allows af-\ncoherence bias of current models.\nModel Overview.\nhybrid TTS systems adopt a two-stage architecture. Let\nthe first stage, a text-to-speech language model fSLM maps\nIn the second stage, the flow-matching acoustic model\nText-Speech Language Model\nModel (SLM) \nText-Speech Language Model\nFigure 1. Overview of our method. Left: Stage-1: The SLM generates speech tokens; steering vectors are injected at selected layers and\na discriminability-driven approach to identify layers and\ninability. While the precise peak varies by model (layers\ntor for each individual emotion at identified model layers.\napproach, from the mean neutral representation to the mean\n• No-steering: Original model outputs.\nmethods (CoCoEmo with Ins/Emo V with α). Adding steer-\nModel\nMethod\nthat combine language models with flow-matching or\nSeveral methods introduce fine-grained interfaces, includ-\nglobal conditional vectors. While prompt-based approaches\nActivation steering modulates model\nmarily focused on LLMs, where advanced methods explore\ning in hybrid TTS models, demonstrating that steering pro-\nfuture TTS systems where model behavior can be perturbed\nmodels: Versatile steering vectors through bi-directional\nstyle spaces with language models: Emotional tts without\nceedings of the 2024 Conference on Empirical Methods\nSchuller, B. Modeling emotional trajectories in writ-\nmodels. arXiv preprint arXiv:2412.10117, 2024b.\nspeech corpus with codec language text-to-speech models.\nspeech language models. Advances in Neural Information\ncontrol in auto-regressive tts models.\nscore (mos) revisited: methods and applications, limita-\ntts: Multi-scale emotion modeling using cross-domain\nmodels. In The Thirty-ninth Annual Conference on Neural\nmodels with activation engineering, 2024. URL https:\nneering: A top-down approach to ai transparency. CoRR,\nWe evaluate our method on two state-of-the-art LM-based TTS systems with different architectural designs. Table 4\nTable 4. Architecture configurations of the TTS backbones used in our experiments.\nLM Architecture\nlanguage model for autoregressive generation. This architectural distinction leads to differences in the structure of internal\nrepresentations and, consequently, in the optimal steering locations and magnitudes for each model.\nto CosyVoice2 (10–17). This architectural difference motivates per-model calibration of steering sites.\ninteraction between representational structure and model-specific generative dynamics also plays a key role.\nusing pyannote/speaker-diarization-3.1 model, resulting in 1,055 samples.\nE.1. Baseline Models\nnative natural-language instruction interface, without modifying internal model activations. Two instruction variants are\nconditioning interface, without modifying internal model activations. Emotion is specified by directly providing an explicit\nText VA values are predicted using a context-aware valence-arousal regression model (Christ et al., 2024), with outputs\nas summarized in Figure 13 and Table 7. Under low- and mid-mismatch conditions, both models achieve strong E-SIM\ncomparable WER. These results indicate that activation steering remains beneficial even for models with built-in emotion\nModel\nMethod\nModel\nMethod\nTable 8 summarizes quantitative results across datasets and models. Single-emotion steering improves TEP for both\n\n\n=== EXPERIMENT ===\nsteering framework, and multi-rater evaluation\ncontrol. Third, we develop a novel multi-rater evaluation\neffective for steering emotional expression. Experiments\nacross multiple datasets and backbones show that injecting\nFor evaluation, we synthesize waveforms from mSLM\ni=1 be a dataset of N samples,\nGiven the dataset D = {(xi, ai, yi)}N\nfor dataset-specific details). In mismatch scenarios where\n3.3. Mixed-Emotion Evaluation\nderived, providing a robust evaluation of mixed-emotion.\n4. Experiments\n4.1. Experimental Setup\nDatasets.\npreserving content. The combined dataset contains 20,691\nEvaluation\net al., 2024), and audio VA values are taken from dataset\nEvaluation Metrics.\nObjective evaluation include:\nFor subjective evaluation, we conduct Mean Opinion Score\ndatasets (Appendix F).\nexpressions in the IEMOCAP dataset, where overly emotion\nTable 2. Mixed-emotion evaluation on CREMA-D. Objective metrics (E-SIM, TEP, ρ, H-Rate, S-SIM, WER) and subjective metric\nIn-distribution evaluation on CREMA-D\nOut-of-Distribution (OOD) evaluation on IEMOCAP\nTable 3. Evaluation on the high-mismatch set of IEMOCAP. Best\nemotional multimodal actors dataset. IEEE transactions\nemotional speech dataset. In ICASSP 2021-2021 IEEE In-\nmodules for our cross-conditioning experiments.\nwhere P denotes all unordered pairs among the five utterances. Dataset-level results are summarized by the mean and\nD. Datasets\nD.1. Steering Vector Extraction Datasets\nWe use multiple emotional speech datasets with paired recordings (same speaker, same text, different emotions) for steering\nTable 6. Statistics of datasets used for steering vector extraction. Numbers indicate the count of utterances per emotion category.\nDataset\nexcluded samples for mixed-emotion synthesis evaluation. This pairing protocol ensures that the difference-in-means\nfor computing steering vector centroids, the validation set for site selection (linear probe evaluation), and the test set for final\nevaluation. All experiments use English utterances only.\nD.2. Evaluation Datasets\nIn-Distribution Evaluation.\nD, which are the same datasets used for steering vector extraction. For mixed-emotion steering, we use CREMA-D samples\nwhose majority-vote human annotations differ from the dataset-provided labels. We further restrict this set to utterances\nThis yields 772 samples for evaluation.\nOut-of-Distribution (OOD) Evaluation.\nused for both OOD evaluation and text–emotion misalignment analysis.\nFor mixed-emotion experiments, we derive soft emotion distributions from multi-rater annotations by normalizing annotator\nFor text–emotion mismatch evaluation, we select utterances with a single non-neutral emotion label according to multi-rater\neach emotion–mismatch subset (e.g., happy–low), we randomly sample up to 500 utterances. The same dataset is used for\nE. Additional Experimental Details\nsynthesis and text–emotion mismatch experiments, the prompt requests a single target emotion. For mixed-emotion synthesis,\nThe following examples illustrate the prompt formats used in our experiments:\nE.2. Evaluation Metrics\nWe use the following metrics for comprehensive evaluation:\nground-truth emotion provided in the dataset. Higher values indicate better emotion matching.\nFor text-emotion mismatch experiments on IEMOCAP, we quantify the degree of semantic-acoustic conflict using the ℓ2\nFigure 10. Mixed-emotion synthesis results for CosyVoice2 using CREMA-D (in distribution) dataset. Higher values indicate better\nFigure 11. Mixed-emotion synthesis results for IndexTTS2 using CREMA-D (in distribution) dataset. Higher values indicate better\nnormalized to [0, 1]. Audio VA values are provided by dataset annotations and mapped from the original [1, 5] scale to [0, 1]\nThis appendix provides supplementary analysis of mixed-emotion steering behavior across different backbones and datase",
  "2602.02734": "=== METHOD ===\nbalanced scripts. This paper details our methodology for data\ndard modeling paradigms. Without foundational datasets, it\nis impossible to train, evaluate, and adapt models to serve\nevaluating ASR models.\nin African languages remain scarce. [10] detailed a method-\n3. DATA COLLECTION METHODOLOGY\nsuited for training high-quality, single-speaker TTS models.\na foundational resource for building and evaluating models,\nmethodology, which prioritized local expertise and ethical\n\n\n=== EXPERIMENT ===\nscale, openly accessible speech dataset for 21 languages rep-\ntion (ASR) dataset containing approximately 1,250 hours of\nand a Text-to-Speech (TTS) dataset with over 180 hours of\nof the dataset and discuss its potential limitations and eth-\nThe WAXAL datasets are released at\nhttps://huggingface.co/datasets/google/WaxalNLP under the\ndataset\nCorrespondence: waxal-dataset-support@google.com\nthe WAXAL dataset, a new, large-scale resource for 21 Sub-\nFig. 1. Examples of some of the images used as prompts to elicit natural speech for the ASR dataset.\nIn recent years, several significant multilingual datasets have\ndataset [4] contains recordings in over 700 languages. For\nunsupervised and semi-supervised applications, datasets like\nguages, existing public datasets are often limited in scale,\ndatasets derived from radio archives, such as [8] containing\nken in Sub-Saharan Africa. While these datasets are invalu-\nlarge-scale, unified dataset for 21 Sub-Saharan African lan-\nThe WAXAL dataset was acquired through a multi-year effort\nThe ASR dataset was designed to capture natural, sponta-\nFig. 2. WAXAL-ASR: Distribution of the ASR dataset sliced by language.\nThe intentionally collected attributes for the ASR dataset in-\nThe TTS dataset was designed for building high-quality,\nThe collected attributes for the TTS dataset include speaker\n4. DATASET STATISTICS\ndatasets. The ASR dataset spans 14 languages, and the TTS\ndataset spans 10 languages. The total size of the released,\nTable 1. Statistics of the WAXAL ASR and TTS datasets.\nDataset\nFig. 4. WAXAL-TTS: Distribution of the TTS dataset sliced by language.\ncruit diverse speakers, the dataset may not capture the full\n• Use-Case Specificity: The ASR dataset, with its diverse\ntion and all voice actors for the TTS dataset provided in-\n• Voice Actor Rights: The release of the TTS dataset en-\nThe WAXAL dataset represents a significant step toward\nThe WAXAL datasets are being released publicly to the re-\nat https://huggingface.co/datasets/google/WaxalNLP.\ning evaluation of universal representations of speech,”\ndataset,”\ndataset for spoken language recognition,” in 2021 IEEE\n“Yodas: Youtube-oriented dataset for audio and speech,”\nLanguage Resources and Evaluation Conference (LREC\n\n\n=== CONCLUSION ===\n6. CONCLUSION",
  "2602.04160": "=== METHOD ===\nCLONING AND INFERENCE-TIME MODEL FUSION\ndesign combining duration-guided and alignment-free models\nscenarios where most open-source models fail, while requiring\nmatching (FM) architectures, which learn vector fields for fast, high-\n— stability and control in duration-guided models versus fluency\nand naturalness in alignment-free models remain a trade-off; cross-\nalignment-free models, leaving robust use of long prompts largely\nfrom low-rate mel features is underexplored in TTS models.\nDuration-Guided vs. Alignment-Free Models. Many flow-matching\nAn alternative is autoregressive (AR) modeling\nbridge models without explicit alignment and those relying on pre-\ntimbre, prosody, and acoustic content. Models like Voicebox [3] or\nwhich adopts a ResUNet architecture, and the diffusion-based NU-\n1. We introduce a dual-decoder architecture that combines an\nalignment-free (AF) model with a duration-guided (DG)\nmodel through inference-time vector-field fusion, combining\nSection 2 details the proposed method, including the architec-\n2. PROPOSED METHOD\n2.1. Overall architecture\nand an acoustic prompt s. We utilize two TTS models trained inde-\nmodel (DG) and an alignment-free model (AF). Each model has its\nThe overall architecture is illustrated in Fig. 1.\nFLUX block with the same architecture as the flow-matching de-\nthe DG model so both paths operate on the same (F, T) grid.\nWe combine two independently trained TTS models at\nthe fluency benefits of the AF model in the final steps. We ana-\nTo ensure temporal alignment, the AF model uses the total du-\nration T predicted by the DG model, so both fields operate on the\nsame mean/std for both models.\nstride 4 into the Period-Aware Estimator, so that the model synthe-\nArchitecture details. The text encoder of both DG and AF mod-\nTraining details. We trained our models on 4×NVIDIA A100 GPUs\nart baselines and the commercial ElevenLabs Multilingual v2 model,\nAblation studies. We first demonstrate that inference-time model\nfusion improves intelligibility compared to using either model alone.\n(Only for AF model)\n(Only for DG model)\nAlignment-Free (AF) Model\nDuration-Guided (DG) Model\nFig. 1. Architecture of PFluxTTS. Duration-Guided and Alignment-Free models are mixed with schedule α(t) during inference. On the right,\nSpeech Prompt Encoder is shown, which outputs either an embedding sequence for the DG model or fixed embedding for the AF model.\nfused model (α=0.75) reduces CER to 8.6%. The fused model also\nNext, we compare the DG-only model with the fused model and\nper sample, the fused model was preferred with a mean ∆CMOS\np < 0.012), with the fused model winning in 79% of cases.\nour method, p<0.05), and the objective SPK-SIM score increases\nand BigVGANv2 combined with the pretrained AudioSR model [33].\nFig. 2. Effect of inference-time model fusion on intelligibility of\nMethod\nguided flow-matching model with an alignment-free model through\ninference-time vector field fusion. The duration-guided model is\napproach. The system is paired with a modified PeriodWave vocoder\nbaselines and confirm the contributions of model fusion, FLUX-\ngate alternative fusion schedules, and develop methods to control the\n[1] Shivam Mehta et al., “A fast tts architecture with conditional\nguage models are human parity zero-shot text to speech syn-\naging large language models for advanced multilingual text-to-\nof high-quality versatile speech generation models,”\nspeech synthesis with factorized codec and diffusion models,”\nral audio upsampling model for various sampling rates,” arXiv\n[28] Gemma Team, “Gemma 2: Improving open language models\nChen, “Ella-v: Stable neural codec language modeling with\n\n\n=== EXPERIMENT ===\nComprehensive experiments on challenging cross-lingual and\npresents experimental setup and results, and Section 4 concludes.\nAlthough we initially experimented with alternative downsamplers\nexperiments, sequence-level prompt conditioning in AF led to fre-\nIn our experiments, α(t) is piecewise-constant: we set\n3. EXPERIMENTS\nof key components. Complementary datasets are used: VoxLingua-\nfor dubbing-style subjective evaluation across four languages, and\n3.1. Experiment setup\nEvaluation details. For subjective evaluation, we sampled 40 utter-\nCross-lingual setup. In all tests, the synthesized speech is in En-\ndatasets and in monolingual setups (e.g., LibriSpeech, VCTK). In\ncontrast, we specifically design our experiments for cross-lingual,\ndatasets; we restrict evaluation to English-only synthesis, where sys-\nComparison with baselines. The results of subjective evaluation are\npresented in Table 1. The experiment shows that the proposed sys-\nObjective evaluation on VoxLingua-dev.\nachieves the best LSD on both datasets, slightly outperforming\nOur experiments focus on English as the target language with\nmultilingual prompts, using real-world conversational datasets such\nevaluations. Overall, PFluxTTS outperforms most open-source sys-\nIn the future, we plan to scale training to larger datasets, investi-\noriented dataset for audio and speech,” in ASRU, 2023.\n[24] Jörgen Valk and Tanel Alumäe, “VoxLingua107: a dataset for\n“Somos: The samsung open mos dataset for the evaluation of\n\n\n=== RESULTS ===\nsubjective CMOS results (Sec. 3.2).\n3.2. Results\ntems and a leading commercial solution, achieving subjective results\n\n\n=== CONCLUSION ===\n4. CONCLUSION",
  "2602.05443": "=== METHOD ===\nwhich integrates diffusion model and generative adversarial net-\nwork. Furthermore, the proposed method incorporates the following\nthe complexity of waveform modeling from data-driven features,\nMoreover, we showed that the proposed method\nCode and pre-trained models are available from\n(SSL) models [7, 8, 9] has changed the way of conventional speech\nprocessing tasks. SSL models have demonstrated high performance\nsion model [17, 18]-like inference. Due to the combination of the\ntwo generative models, WaveFit can generate high-quality wave-\nFig. 1. Conceptual diagrams and noise examples of each methods.\nfusion models in two ways: 1. initial noise could be sampled from\nfeatures are used as input [12], these methods cannot be used since\na result, the model can focus on more important aspects of waveform\nmodeling, and is thought to mitigate the difficulty of training.\nerations. Moreover, the proposed method can generate natural wave-\nuation showed that the proposed method outperforms the baselines in\nfocus on the latter. This method generates the speech waveform\nmodel, it cannot use hand-crafted noise sampling methods [17, 22],\nlearned by the model as in (2).\n2.2. RestoreGrad: Diffusion model with trainable prior\nRestoreGrad [23] is a method to obtain an informative prior for dif-\nfusion models even when hand-crafted noise sampling methods are\ndifficult. To achieve this, RestoreGrad combines diffusion models\nwith trainable priors, which are modeled by VAEs. Specifically, Re-\nFig. 2. Overview of the proposed model. During training, the poste-\nDuring inference steps of the diffusion model, the model can\nto facilitate waveform generation compared to the naive approach of\ncrafted method [22] and sampling from standard normal distribution.\n3. PROPOSED METHOD\ndifficulty of waveform modeling from data-driven features, enabling\n3.2. Model overview\nFigure 2 shows an overview of the proposed method. The proposed\nmethod introduces a prior encoder and a posterior encoder to achieve\nmain, the proposed method incorporates them in the time-frequency\ndomain to shorten the sequence length and reduce the modeling com-\nThe loss function of the proposed method is as follows:\nModel and training setup. Our model generates waveforms by\nform 2× upsampling. Then, an architecture similar to WaveFit [5]\nwith upsampling scale of {5, 4, 3, 2, 2} performs 240×. The model\ntively. For the encoder architecture, real-valued DCUnet-10 [25]\nComparison methods. To evaluate the effectiveness of the pro-\nposed method, we set up two baselines. In addition to WaveFit [12],\nthe WaveFit architecture itself in waveform generation from SSL\ngenerator was 17.18 M. The model was trained for 1M steps with\nthe same batch size as the proposed method.\nA total of 450 samples per method were evaluated on a five-point\nWavLM-large [7] is an English-specialized SSL model pre-trained\nwith masked language modeling objective. 2. XLS-R-0.3B [8] is\na multilingual SSL model pre-trained with contrastive objective.\n3. Whisper-medium [31] is a multilingual speech-to-text model.\nEvaluation results when using LibriTTS-R test-clean, 8-th layer SSL features, and T = 5. Bold indicates the best method under\nMethods\nmodels\nModels-Layers\nmethod outperformed the baseline on all reference-aware objective\nS-MOS, the proposed method again outperformed baselines for all\ntics. For the N-MOS, the proposed method outperformed the base-\nposed method is sensitive to hyperparameters. RestoreGrad, which\nour method is based on, showed a tendency to be somewhat sen-\nwithin the model [8]. A similar phenomenon may have occurred in\nDuring inference, the real-time factor (RTF) of the proposed method\nmethod. These results suggest that inference starting from initial\nFeatures from shallow layers of SSL models are known to contain\nness of the proposed method when conditioning on features with dif-\nposed method shows better results than the baseline in all reference-\nacoustic information, the proposed method achieved substantial im-\nsuggests that the proposed method is capable of generating wave-\nposed method can generate high-quality waveforms from features\ntive evaluations, we confirmed that the proposed method consistently\nbaseline. Future work includes building models from large datasets\nand extending to multilingual models.\nversion and its challenges: From statistical modeling to deep\ntion model integrating self-supervised speech and text repre-\nrestoration model for million-hour scale data restoration,” in\nwaveform generation model based on generative adversarial\nmodel based neural vocoder with adaptive noise spectral shap-\nnoising diffusion models with data-dependent adaptive prior,”\ning conditional denoising diffusion models with jointly learned\nnovel deep generative model for direct representation of com-\nsupervised speech representation model,” in Proc. of IEEE\n\n\n=== EXPERIMENT ===\nsteps. Through experiments, we showed that WaveTrainerFit can\nObjective evaluations showed that WaveTrainerFit achieves bet-\ndataset creation [19, 20]. However, since WaveFit was originally\nIn experiments [23], RestoreGrad showed superior results in\n4. EXPERIMENTAL EVALUATION\n4.1. Experimental setup\nDatasets. We used the LibriTTS-R corpus [19]. This corpus con-\nEvaluation metrics. We adopted the following objective metrics:\nSpeechBERTScore [26] is an automatic speech evaluation metric\nwaveform energy in all evaluations, all waveforms were aligned to\nTable 2. Layer-focused objective evaluation results. Here, we used\n4.2. Experimental results\n4.2.1. Objective and subjective evaluation results\nTable 1 shows the evaluation results. It can be seen that the proposed\nour experiments, possibly leading to degraded performance. An im-\nferent properties, we conducted evaluations using WavLM features\nextracted from the 2nd, 8th, and 24th layers. In this experiment,\nreference-free UTMOS [35] was added as an evaluation metric that\nSSL features. In evaluation experiments, we showed that the pro-\naware automatic evaluation of speech generation leveraging\nNLP evaluation metrics,” in Proc. of Interspeech, 2024.\n\n\n=== RESULTS ===\nto have the energy close to that of the target waveform. As a result,\nterm of Eq. (9) in the prior encoder. As a result, initial noise contain-\na negative impact. XLS-R shows worse performance compared to\naffect performance.\n4.2.2. Performance at each iteration and processing speed\n4.2.3. Performance for SSL features from different layers\nThe results are shown in Table 2. It can be seen that the pro-\nshowed better results in speaker similarity MOS compared to the\n[28] M. Morise, “Harvest: A high-performance fundamental fre-\n\n\n=== CONCLUSION ===\nportant future work is a detailed analysis of how combinations of\n5. CONCLUSION",
  "2602.05207": "=== METHOD ===\nARCHI-TTS: A FLOW-MATCHING-BASED TEXT-TO-SPEECH MODEL WITH\nficulty of text-speech alignment modeling and the high computa-\naudio prompt, current TTS models can perform zero-shot synthesis\negories: autoregressive (AR) models and non-autoregressive (NAR)\nmodels. AR-based TTS models consecutively predict discrete au-\ntion modeling and diverse sampling strategies, their foundational\nNAR-based TTS models have gained prominence by offering par-\nmodeling. A key catalyst for recent breakthroughs in NAR speech\nrectly models continuous data distributions, for example, Natural-\nEfforts on NAR TTS models are primarily concentrated on two\ncrucial challenges: a) the effective text-speech alignment modeling,\nstrategies. Several approaches relied on explicit guidance, such as\nhighly effective methods have gained traction, such as padding char-\ncient diffusion sampling, with Diffusion Model Distillation (DMD)\nlation methods is the increased training overhead. They necessitate\nnot only a pre-trained teacher model but also extra forward passes\nmodel for high-quality and fast speech synthesis. ARCHI-TTS is\na novel aligner-encoder-decoder architecture trained with a flow-\nmatching objective. First, to model the text-speech alignment, a se-\nperforms state-of-the-art (SOTA) TTS models on the LibriSpeech\nAs a direct benefit of a by-product of separated DiT architecture, the\nmodel can be significantly accelerated at inference time without a\nscribes the experimental setup, including datasets, model configura-\nFig. 1. The overview of the ARCHI-TTS architecture.\nARCHI-TTS is a fully non-autoregressive speech synthesis model\naudio prompt. The architecture is centered on two primary compo-\nfrom Diffusion Transformer (DiT) blocks. The overall architecture\nthis approach is the high temporal redundancy of mel-spectrograms,\na separate, complex vocoder model to invert the representation back\na more direct synthesis approach by utilizing a highly compressed,\nwork [16], a powerful approach for generative modeling. CFM mod-\narchitecture, comprising a condition encoder and a velocity decoder\n[17]. The model is conditioned on a rich set of inputs: a) seman-\nModel\nModel\nmodel—which typically dominates the computational cost of the\n3.2. Model Configuration\nAll VAE latents are pre-extracted prior to training the TTS model.\nOur base flow-matching model is trained for 800k updates with a\n(EMA) model is utilized for sampling. We allocate 18 DiT layers\nSOTA models on selected samples in SeedTTS testset.\nModel\nembeddings using a CAM++ model from 3D-Speaker1. For CFG,\nWe compare ARCHI-TTS with state-of-the-art TTS models, includ-\ning both AR and NAR models. Evaluation is conducted under the\nand the industrial-level CosyVoice2 [3] TTS model. We evaluate 10\nWe report the average performance of our models based on 3 random\nrate VAE latents, our model can sample 10-second audio with a\nwell on multi-lingual scenario within a single model. ARCHI-TTS\nlags behind other SOTA models.\nperform ablation studies using small models trained on LibriTTS and\nbase models trained on LibriHeavy. We conducted studies on adding\n1https://www.modelscope.cn/models/iic/speech_\nTable 4. Ablation studies in model architecture on LibriSpeech-PC\nTab 4 summarizes the results. From the results on small models,\ncrease in WER. Scaling up model and data size without speaker em-\ntents as audio representations. In the base model, removing speaker\noriginal model with a doubled codebook size. This shows that a VQ-\nregularized semantic representation is beneficial to the model.\ntext-to-speech model featuring a novel semantic aligner and a flow-\nTTS achieves competitive performance over the SOTA models on\nation method that preserves speech synthesis quality. Future work\nsynthesis with large language models,”\nspeech generation models,” arXiv preprint arXiv:2406.02430,\nthesis with factorized codec and diffusion models,” in Proceed-\nGustav Eje Henter, “Matcha-tts: A fast tts architecture with\nspeech: Distilled diffusion model surpassing the teacher in\nsis with latent diffusion models,” in Proc. CVPR, 2022, pp.\nNickel, and Matt Le, “Flow matching for generative model-\nance,” in NeurIPS 2021 Workshop on Deep Generative Models\ntoregressive modeling for speech generation,” arXiv preprint\nand capitalization capabilities of end-to-end asr models,” in\n\n\n=== EXPERIMENT ===\ntic understanding. Experimental results demonstrate that ARCHI-\nEvaluations (NFE), for example, teacher-student distillation utilized\ntive denoising. Experiments conducted on the large-scale 100k-hour\nmulti-lingual Emilia dataset [12] demonstrate several key finding: a)\ntive subjective quality: In mean opinion score (MOS) evaluations,\ntions, training details, and evaluation metrics. Section 3.4 presents\nthe results and analysis of our experiments Section 4 concludes the\n3. EXPERIMENTAL SETUP\n3.1. Datasets\nFor training, we primarily utilize the Emilia dataset [12], a large-\nstudies, we also use the 50,000-hour English LibriHeavy dataset [23]\nand the 600-hour English LibriTTS dataset [24]. Our evaluations are\nTable 3. Results of MOS evaluation of ARCHI-TTS against other\nparticipated in the evaluation.\n3.4. Experimental Results\nTrain Dataset\nthat both larger datasets and speaker embedding contribute to im-\nmatching decoder. Experimental results demonstrate that ARCHI-\ndiverse speech dataset for large-scale speech generation,” in\nBenchmark for evaluation of punctuation\n\n\n=== RESULTS ===\ning synthesis without performance degradation. An auxiliary CTC\npressive performance, which is often enhanced by implicit dura-\nCompetitive performance with high efficiency: ARCHI-TTS out-\nwhich results in a high token rate (e.g., 50-100Hz) and necessitates\nTable 1. Results on LibriSpeech-PC test-clean. RTF results are mea-\ndenotes results from the original papers. ♢denotes results from F5-\nTTS paper. † denotes the results from DiTAR paper.\nTable 2. Results on Seed-TTS testset. * denotes results from the\noriginal papers. ♢denotes results from F5-TTS’s paper. † denotes\nresults from DiTAR’s paper.\nfor sampling. ARCHI-TTS demonstrates competitive performance\nWER. The results also indicate that our designs can achieve promis-\ning performance with limited resources. In the results of MOS evalu-\nof speaker embedding, (ii) the performance change introduced by\nfurther boosts SSIM with unchanged WER. These findings suggest\nThe results, shown in Fig. 2, reveal that a high sharing ratio signif-\nicantly accelerates inference, though with some performance degra-\nfor this performance drop. At an NFE of 32, ARCHI-TTS achieves a\n\n\n=== CONCLUSION ===\npaper and discusses future work.\n4. CONCLUSION",
  "2602.05770": "=== METHOD ===\nWe evaluate two non-autoregressive architectures, StyleTTS2\nspeech. Our models utilize flexible duration modeling to im-\nmodel, which significantly outperforms standard Demucs\nof our approach for realistic speech generation.\nmodels that are able to synthesize spontaneous speech, need-\nTraining TTS models on in-the-wild data presents sev-\nble speech for many architectures. Second, the diversity in\nvariable pacing makes duration modeling substantially more\nTo address these challenges, our approach combines: (1)\nautoregressive architectures (F5-TTS [5] and StyleTTS2 [6])\nwith flexible duration modeling, and (3) systematic analysis\nheads) using the findings of [7]. However, this approach un-\nderperformed compared to finetuning strategies. Both models\nment step with the Sidon model [8]. Although TITW-Easy\nmaintain consistency with the pretrained model’s text repre-\nand use the same as the pretrained model.\nTable 1. Effect of audio prompt length on model performance\nModel\nspeech transcription using Nemo conformer large model com-\nembeddings of the model voxcelebs12 rawnet3 from ESP-\nboth models when using the short prompt as seen in Table\nity scores (UTMOS and DNSMOS) in both models, however,\nimproved in both models when using the enhanced reference,\nbe due to the different role of reference audio in each model,\nprompt that the model uses to fill the masked region condi-\n1M steps on TITW (F5-TTStiny in Table 2). This approach\nmodel which is known to have content preservation issues, but\nmodel performance. All models use the long prompt strategy\nModel\ndistinct enhancement effects for each model. F5-TTS with-\nFig. 1. Spectrogram comparison of models with different in-\nfinal submission. This was our best model w.r.t intelligibility\nof the project Desarrollo Modelos ALIA. CEB acknowledges\nends, and off-the-shelf models,” in Interspeech 2024,\nlanguage model-based zero-shot text-to-speech synthe-\n\n\n=== EXPERIMENT ===\nin signal quality. Experimental results show that finetuning\nThe TTS In the Wild (TITW) dataset [4] provide spontaneous\ndatasets.\n2. EXPERIMENTS\nGiven the spontaneous nature of the dataset and the zero-shot\ngeneration for one of the evaluation protocols StyleTTS2 and\nF5-TTS were chosen. We initially experimented with train-\ncompute the evaluation metrics in different dimensions. For\nAs a first experiment, the length of the audio prompt was\nprevious experiment we use the longest audio available per\nthe evaluation metrics. Future work should investigate adap-\nMultilingual Dataset for Speech Research,”\nmultilingual, and diverse speech dataset for large-scale\nlingual speech restoration for large-scale dataset cleans-\n“VERSA: A versatile evaluation toolkit for speech, au-\n\n\n=== RESULTS ===\nshot synthesis performance, demonstrating the effectiveness\n3. RESULTS\nWER is unaffected. Our findings suggest a direct correlation\nity of the audio prompt. Building on the results from the\nFor comparison, we include results for the smaller F5-\nresults in denoising speech prompts, we did not find a degra-\nhigh-frequency energy across the entire utterance, resulting\nto maximize the performance of the benchmark samples in\n\n\n=== CONCLUSION ===\n4. CONCLUSION AND FUTURE WORK"
}