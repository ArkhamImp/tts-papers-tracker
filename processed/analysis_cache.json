{
  "2601.02073": {
    "tldr": "A low-resource tonal language TTS system for Mizo was developed using Tacotron2 and VITS, with VITS outperforming Tacotron2 in both subjective and objective evaluations.",
    "core_contribution": "Demonstrates that a non-autoregressive, end-to-end TTS framework (VITS) can achieve acceptable perceptual quality and intelligibility for a low-resource tonal language without explicit tone markings.",
    "methodology": "Built two TTS models (Tacotron2 and VITS) using 5.18 hours of Mizo speech data, trained with ESPnet2 toolkit, evaluated using DNSMOS, MCD, RMSE F0, F0 correlation, and MOS scores from native speakers.",
    "key_findings": "VITS outperformed Tacotron2 in both subjective (MOS) and objective (DNSMOS, MCD, RMSE F0, F0 correlation) evaluations, with significantly lower tone errors and better overall perceptual quality.",
    "limitations": "Limited training data (5.18 hours), lack of explicit tone markings in the text, and potential variability in tone synthesis due to data diversity.",
    "future_work": "Explore larger datasets, improve tone modeling, and investigate long-term strategies for low-resource tonal language TTS development.",
    "evaluation": "medium",
    "rating": 7
  },
  "2601.03888": {
    "tldr": "IndexTTS 2.5 improves upon its predecessor with semantic codec compression, Zipformer architecture, multilingual extensions, and GRPO optimization, achieving 2.28x faster inference while maintaining quality.",
    "core_contribution": "Introduces four key improvements to zero-shot multilingual TTS: semantic codec compression (50Hz to 25Hz), Zipformer-based S2M architecture, three explicit cross-lingual modeling strategies, and GRPO-based T2S optimization.",
    "methodology": "Replaces U-DiT with Zipformer in S2M module, compresses semantic codec frame rate, implements boundary-aware alignment, token-level concatenation, and instruction-guided generation for multilingual support, and applies GRPO for post-training T2S module.",
    "key_findings": "Achieves 2.28x improvement in real-time factor (RTF) while maintaining comparable WER and speaker similarity to IndexTTS 2; Zipformer architecture preferred over U-DiT in subjective evaluations; supports Mandarin, English, Japanese, and Spanish with emotion transfer in unseen languages.",
    "limitations": "Limited to four languages in evaluation; relies on pre-trained components; emotional similarity evaluation may be subjective; no comparison with non-zero-shot multilingual TTS models.",
    "future_work": "Potential expansion to more languages; exploration of additional cross-lingual modeling strategies; integration with larger language models for improved text comprehension.",
    "evaluation": "strong",
    "rating": 8
  },
  "2601.05911": {
    "tldr": "Pantagruel introduces unified self-supervised encoder models for French text and speech using feature-space prediction, achieving competitive performance across both modalities.",
    "core_contribution": "Unified self-supervised encoder models for French text and speech using feature-space prediction within a joint-embedding predictive architecture (JEPA), trained on large-scale French corpora including a new 100k-hour INA broadcast dataset.",
    "methodology": "Models use data2vec 2.0 architecture with modality-specific encoders predicting contextualized feature representations rather than discrete tokens. Text models trained on Wikipedia, OSCAR, and CroissantLLM; speech models trained on MultilingualLibriSpeech, LeBenchmark, and INA-100k. Incorporates masked language modeling loss for text models. Evaluated on standard French benchmarks including FLUE and LeBenchmark.",
    "key_findings": "Pantagruel models show competitive or superior performance compared to strong French baselines (CamemBERT, FlauBERT, LeBenchmark2.0) across both text and speech tasks. Speech models achieve strong ASR performance, with large models outperforming others. Feature-space prediction proves effective for French representation learning.",
    "limitations": "Some results show Pantagruel models lag slightly behind SOTA models in certain text tasks. The evaluation methodology varies across different tokenizers, making direct comparisons challenging. Some datasets (CroissantLLM) were still in progress during evaluation.",
    "future_work": "Extending to other languages, exploring larger model capacities and more diverse corpora, developing cross-modal applications, and releasing additional model variants trained on different datasets.",
    "evaluation": "medium",
    "rating": 7
  },
  "2601.12480": {
    "tldr": "SpeechEdit introduces a unified neural codec language model for selective editable text-to-speech generation with attribute control.",
    "core_contribution": "A unified neural codec language model that enables selective control over speech attributes like timbre, prosody, and emotion while maintaining naturalness.",
    "methodology": "Trains on LibriEdit dataset with delta pairs for attribute control, using a two-stage AR-NAR architecture with explicit control instructions for selective editing.",
    "key_findings": "Achieves competitive naturalness and robustness in zero-shot TTS while offering flexible attribute control, outperforming baselines in emotion editing tasks.",
    "limitations": "Relies on implicit disentanglement, has speaker modeling granularity issues, and requires fine-grained attribute control for optimal performance.",
    "future_work": "Enhancing emotion editing performance, improving speaker modeling granularity, and exploring more explicit disentanglement methods.",
    "evaluation": "medium",
    "rating": 7
  },
  "2601.05699": {
    "tldr": "Afri-MCQA is the first multimodal cultural question-answering benchmark for 15 African languages, revealing significant performance gaps in LLMs across text and speech modalities.",
    "core_contribution": "Introduction of Afri-MCQA, a 7.5k Q&A multilingual cultural benchmark with text and speech modalities, and comprehensive evaluation of LLMs on African languages.",
    "methodology": "Native speakers created parallel English-African language Q&A pairs across 15 languages; evaluated open/closed-source LLMs on MC-VQA and open-ended VQA tasks in text and audio; included control experiments for linguistic competence; used accuracy and human evaluation metrics.",
    "key_findings": "Open-weight models perform poorly on African languages, especially in open-ended VQA and speech; performance gaps exist between native languages and English; model scaling alone insufficient; speech-first approaches and culturally grounded pretraining needed.",
    "limitations": "Dataset size moderate (7.5k pairs); potential human bias in curation; limited to 15 languages; evaluation focused on specific models and tasks; may not generalize to all African languages or cultural contexts.",
    "future_work": "Speech-first model development, culturally grounded pretraining, cross-lingual cultural transfer, improved LID and ASR for African languages, expansion to more languages and tasks.",
    "evaluation": "strong",
    "rating": 8
  },
  "2601.02444": {
    "tldr": "VocalBridge is a diffusion-based purification framework that effectively removes protective perturbations from speech, enabling successful voice cloning despite existing defenses.",
    "core_contribution": "Proposes a novel diffusion-bridge purification model (VocalBridge) that learns latent mappings in EnCodec space to remove speaker-protective perturbations while preserving acoustic identity for voice cloning.",
    "methodology": "Uses a time-conditioned 1D U-Net denoiser with cosine noise schedule in EnCodec latent space, incorporating Whisper-guided phoneme variants for transcript-free purification, and evaluates against multiple TTS/VC models and defense mechanisms.",
    "key_findings": "VocalBridge outperforms existing purification methods, achieving 37.4% authentication restoration rate (ARR) for TTS and 35.6% for VC models, with MOS values between 2.95-3.27, demonstrating effectiveness against perturbation-based voice defenses.",
    "limitations": "Limited evaluation of real-world deployment scenarios, potential overfitting to specific defense mechanisms, and reliance on controlled experimental conditions rather than adversarial adaptation.",
    "future_work": "Developing more robust protection mechanisms against adaptive purification attacks, exploring cross-domain generalization, and investigating real-time implementation challenges.",
    "evaluation": "strong",
    "rating": 8
  },
  "2601.02914": {
    "tldr": "Modern voice cloning models can easily bypass commercial speaker verification systems, and anti-spoofing detectors struggle to generalize across different audio synthesis methods, revealing critical vulnerabilities in audio-based biometric authentication.",
    "core_contribution": "First systematic empirical evaluation of speaker verification and anti-spoofing systems against diverse deepfake speech synthesis models, revealing two major security vulnerabilities.",
    "methodology": "Large-scale benchmark dataset with 30 speakers (15 male, 15 female) from AISHELL-3 and Celeb datasets; evaluation of commercial speaker verification models, anti-spoofing detectors, and diverse speech synthesis approaches including diffusion-based, flow-based, and prompt-conditioned architectures; cross-lingual evaluation between Mandarin-trained models and English deepfakes.",
    "key_findings": "Voice cloning models trained on very small samples can bypass commercial speaker verification (EER approaching 0.55); anti-spoofing detectors show 30× performance degradation when encountering unseen synthesis methods; strong in-domain performance fails to generalize to out-of-domain attacks; XLS-R + AASIST achieves best deepfake detection performance but still struggles with unseen models.",
    "limitations": "Focus on English and Mandarin languages only; evaluation limited to specific commercial and open-source models; potential bias from dataset selection; may not capture all real-world attack scenarios; performance metrics based on controlled experimental conditions.",
    "future_work": "Development of adaptive defenses and architectural innovations for robust anti-spoofing; exploration of multi-factor authentication; creation of more diverse and representative training corpora; investigation of real-time detection systems; addressing cross-lingual generalization challenges.",
    "evaluation": "strong",
    "rating": 8
  },
  "2601.15621": {
    "tldr": "Qwen3-TTS is a multilingual, controllable, and streaming text-to-speech model family with advanced voice cloning and low-latency capabilities.",
    "core_contribution": "Introduces a dual-track LM architecture with two specialized tokenizers (25Hz and 12Hz) enabling real-time synthesis, 3-second voice cloning, and description-based voice control.",
    "methodology": "Uses a dual-track autoregressive architecture with Multi-Token Prediction (MTP) module, trained on 5M+ hours of speech data across 10 languages, featuring Qwen-TTS-Tokenizer-25Hz for semantic content and Qwen-TTS-Tokenizer-12Hz for ultra-low-latency streaming.",
    "key_findings": "Achieves state-of-the-art performance across multilingual benchmarks, supports zero-shot voice cloning, cross-lingual generation, and long-form speech synthesis with 97ms first-packet emission latency.",
    "limitations": "Limited details on specific benchmark datasets and metrics, potential overfitting concerns with large-scale training data, and no mention of computational efficiency or deployment considerations.",
    "future_work": "Extending multilingual coverage beyond 10 languages, exploring more granular stylistic controls, and further scaling model capabilities.",
    "evaluation": "medium",
    "rating": 8
  },
  "2601.13802": {
    "tldr": "Habibi introduces open-source unified-dialectal Arabic TTS models using curriculum learning from ASR data, outperforming commercial systems.",
    "core_contribution": "First systematic benchmark for multi-dialect Arabic speech synthesis with open-source models supporting 20+ dialects.",
    "methodology": "Curriculum learning approach using existing ASR corpora, two-stage training (MSA then dialect-specific), and specialized vs unified model strategies with in-context learning.",
    "key_findings": "Unified models outperform commercial ElevenLabs v3 on 6 major dialects, with specialized models showing better dialect-specific performance and curriculum approach enabling effective convergence.",
    "limitations": "Limited analysis of neural model behaviors, computational cost concerns, potential cross-dialect recognition bias, and lack of exploration of more tailored model architectures.",
    "future_work": "More effective checkpoint fusion across dialects, incorporating more data, analyzing neural model behaviors, and understanding linguistic distinctions in model representations.",
    "evaluation": "medium",
    "rating": 7
  },
  "2601.23255": {
    "tldr": "A jailbreak attack on large audio-language models using narrative-style TTS delivery achieves 98.26% success rate, significantly exceeding text-only baselines.",
    "core_contribution": "Demonstrates that prosodic and paralinguistic features in synthetic speech can bypass safety mechanisms in audio-language models, revealing a new attack vector.",
    "methodology": "Uses an instruction-following TTS model to embed disallowed directives within narrative audio streams, exploiting structural and acoustic properties to circumvent safety mechanisms calibrated for text.",
    "key_findings": "Achieves 98.26% success rate on Gemini 2.0 Flash and other models, outperforming text-only baselines; vocal tone variation alone can substantially shift model behavior even without adversarial content.",
    "limitations": "Relies on synthetic speech from specific TTS models; limited human evaluation due to sample size; effectiveness may vary across accents and real-world conditions.",
    "future_work": "Need for safety frameworks that jointly reason over linguistic and paralinguistic representations; cross-accent evaluations; exploring real-world deployment scenarios.",
    "evaluation": "medium",
    "rating": 7
  },
  "2601.16023": {
    "tldr": "DS2ST-LM introduces a multilingual LLM-based direct speech-to-speech translation framework that outperforms cascaded baselines while preserving speaker identity.",
    "core_contribution": "A scalable single-stage direct S2ST framework leveraging a multilingual LLM with timbre-aware speech synthesis, achieving state-of-the-art performance across multiple language pairs.",
    "methodology": "Integrates Whisper speech encoder, learnable projection module, Qwen2-0.5B LLM, and timbre-controlled vocoder; uses GigaS2S-1000 synthetic dataset; evaluates speech-derived vs text-derived semantic tokens and three projection architectures.",
    "key_findings": "Outperforms cascaded and ST+TTS baselines on BLEU, METEOR, BLEURT, and COMET metrics; preserves speaker identity better than prior direct S2ST systems; simple Linear projection achieves highest performance despite faster convergence of higher-capacity projectors.",
    "limitations": "Relies on synthetic data which may not fully capture real-world diversity; simple Linear projection may lack explicit modeling capabilities for certain tasks; scalability to alternative LLM architectures not fully explored.",
    "future_work": "Scaling to alternative LLM architectures, jointly modeling speech understanding and generation, and extending to more language pairs.",
    "evaluation": "strong",
    "rating": 8
  },
  "2601.10629": {
    "tldr": "VoiceSculptor is an open-source unified system that enables fine-grained, instruction-following voice design and high-fidelity cloning through chain-of-thought reasoning and attribute modeling.",
    "core_contribution": "A unified open-source framework integrating instruction-based voice design with high-fidelity cloning, achieving state-of-the-art performance on InstructTTSEval-Zh while maintaining full reproducibility.",
    "methodology": "Uses LLaSA-3B for voice design with chain-of-thought fine-grained attribute modeling, cross-validated emotion labeling, and retrieval-augmented generation; generates prompt waveforms fed into CosyVoice2 for cloning; trained with joint text-token and attribute-token CE loss.",
    "key_findings": "Achieves SOTA among open-source models on InstructTTSEval-Zh; scaling studies show consistent gains from larger model size (1B→3B) and richer training data; CoT-based attribute modeling improves controllability without architectural changes.",
    "limitations": "Evaluated primarily in Chinese; relies on additional synthesis stage introducing latency; evaluation uses LLM-based metrics which may have bias; some components depend on proprietary models for comparison.",
    "future_work": "Extending to multilingual support beyond Chinese; exploring more efficient architectures to reduce synthesis latency; improving robustness to out-of-domain instructions.",
    "evaluation": "medium",
    "rating": 8
  },
  "2601.12966": {
    "tldr": "A controllable TTS system that synthesizes Lombard speech for any speaker without requiring explicit Lombard data during training by manipulating style embeddings.",
    "core_contribution": "Introduces a method to generate Lombard speech for any speaker by shifting PCA components of style embeddings learned from a large, prosodically diverse dataset, enabling fine-grained control over Lombardness levels.",
    "methodology": "Leverages F5-TTS as the base model, injects speaker information using a TDNN architecture, and manipulates style embeddings derived from PCA analysis of Lombard and articulation datasets to control Lombardness during inference.",
    "key_findings": "The method preserves naturalness and speaker identity, enhances intelligibility under noise, and provides fine-grained control over prosody. Evaluations show improved Lombardness and intelligibility compared to baseline F5-TTS, with lower WER and higher SSIM scores across different SNR levels.",
    "limitations": "Requires a large, prosodically diverse dataset for training style embeddings, and the effectiveness depends on the quality of the PCA analysis and manipulation of style embeddings.",
    "future_work": "Explore more advanced methods for style embedding manipulation and extend the approach to other speech styles beyond Lombard speech.",
    "evaluation": "medium",
    "rating": 7
  },
  "2601.02944": {
    "tldr": "XLSR-MamBo is a hybrid Mamba-Attention architecture that scales backbone depth to improve audio deepfake detection performance.",
    "core_contribution": "Proposes a modular framework integrating XLSR front-end with hybrid Mamba-Attention backbones, demonstrating that scaling backbone depth mitigates performance variance and improves robustness against unseen deepfake synthesis methods.",
    "methodology": "Systematically evaluates four topological designs using advanced SSM variants (Mamba, Mamba2, Hydra, Gated DeltaNet) in hybrid architectures, with comprehensive experiments on ASVspoof 2021 LA, DF, ITW, and DFADD datasets.",
    "key_findings": "MamBo-3-Hydra-N3 achieves competitive SOTA performance, with Hydra's native bidirectional modeling providing advantages over heuristic dual-branch strategies; deeper architectures consistently improve performance and reduce variance compared to shallower models.",
    "limitations": "Evaluation relies exclusively on the ASV19LA training dataset, real-world applicability remains unverified against deepfakes in-the-wild, and privacy-preserving architectures like on-device detection are not explored.",
    "future_work": "Advancing hybrid architectures to keep pace with evolving deepfake techniques, exploring privacy-preserving architectures, and investigating the theoretical upper bound of model robustness.",
    "evaluation": "strong",
    "rating": 8
  },
  "2601.11329": {
    "tldr": "F-Actor is an open, instruction-following full-duplex conversational speech model trained efficiently with just 2,000 hours of data by freezing the audio encoder and fine-tuning only the language model.",
    "core_contribution": "First open, instruction-following full-duplex conversational speech model that can be trained efficiently under academic resource constraints, enabling controllable conversational behaviors like backchanneling and interruptions.",
    "methodology": "Freezes audio encoder and fine-tunes only the language model (Llama3.2-1B-Instruct) with 2,000 hours of data, uses single-stage training protocol, and enables explicit instruction control over speaker voice, conversation topic, conversational behavior, and dialogue initiation.",
    "key_findings": "Achieves over 99% accuracy in instruction following when combining audio and text streams with word-level alignment, produces coherent conversations with controllable backchanneling and interruptions, and demonstrates efficient training requiring minimal data compared to prior approaches.",
    "limitations": "Restricted to a small fixed pool of speakers, trained exclusively on text-to-speech data, limited to English, and may not generalize well to diverse conversational scenarios or spontaneous speech.",
    "future_work": "Extending the model to handle more diverse speakers and languages, improving generalization to spontaneous speech, and exploring more sophisticated modeling of conversational behaviors like turn-taking and overlapping speech.",
    "evaluation": "medium",
    "rating": 7
  },
  "2601.11141": {
    "tldr": "FlashLabs Chroma 1.0 is the first open-source, real-time end-to-end spoken dialogue model with personalized voice cloning, achieving sub-second latency and high speaker similarity.",
    "core_contribution": "Introduces an interleaved text-audio token schedule (1:2) for streaming generation, enabling real-time end-to-end spoken dialogue with personalized voice cloning.",
    "methodology": "Uses a dual-stream (Thinker-Talker) architecture with a 4B parameter model, trained on 100K steps with a balanced objective for text and audio tokens, leveraging speaker verification fine-tuning for voice cloning.",
    "key_findings": "Achieves 10.96% relative improvement in speaker similarity over human baseline, with Real-Time Factor (RTF) of 0.43, and competitive performance across understanding, reasoning, and dialogue tasks.",
    "limitations": "Current architecture does not support multi-speaker scenarios, and the model is trained on limited datasets, potentially affecting generalization.",
    "future_work": "Integrating advanced methods for multi-speaker support and improving dialogue capabilities, as well as exploring encoder-decoder architectures for potential benefits.",
    "evaluation": "strong",
    "rating": 8
  },
  "2601.09239": {
    "tldr": "DSA-Tokenizer introduces a disentangled semantic-acoustic tokenization approach using flow matching-based hierarchical fusion for speech modeling.",
    "core_contribution": "Proposes a novel tokenizer that explicitly separates speech into discrete semantic and acoustic tokens with distinct optimization constraints, enabling better disentanglement than existing methods.",
    "methodology": "Uses ASR supervision for semantic tokens and mel-spectrogram restoration for acoustic tokens, with a hierarchical Flow-Matching decoder to eliminate rigid length constraints and enable flexible recombination.",
    "key_findings": "Achieves superior balance between reconstruction quality and ASR performance compared to baseline models, with robust disentanglement enabling controllable speech generation in LLMs.",
    "limitations": "Limited evaluation details provided, no comparison with state-of-the-art models on downstream tasks, and code/model availability pending after acceptance.",
    "future_work": "Extending DSA-Tokenizer to general audio modeling and improving performance for natural human-AI interaction.",
    "evaluation": "medium",
    "rating": 7
  },
  "2601.02753": {
    "tldr": "Vclip introduces a face-voice association learning method using CLIP to generate speaker embeddings for face-based TTS, achieving strong cross-modal matching performance.",
    "core_contribution": "Proposes Vclip, a CLIP-based face-voice association model trained on noisy audio-visual data to efficiently learn cross-modal embeddings, enabling retrieval-based speaker generation for TTS.",
    "methodology": "Uses CLIP’s semantic knowledge to learn face-voice associations from VoxCeleb, then applies a retrieval-based GMM speaker generation module to produce probable speaker embeddings for downstream TTS, with TTS feedback to improve matching.",
    "key_findings": "Achieves 89.63% cross-modal verification AUC on VoxCeleb testset, outperforms Self-Lifting baseline, and demonstrates that retrieval + TTS feedback improves face-voice matching in subjective evaluations.",
    "limitations": "Relies on noisy audio-visual data, retrieval-based approach may not scale well, and generated voices still lag behind true voice cloning in naturalness.",
    "future_work": "Improving retrieval efficiency, scaling to larger datasets, and enhancing naturalness of generated voices for better face-voice alignment.",
    "evaluation": "medium",
    "rating": 7
  },
  "2601.06560": {
    "tldr": "A lightweight audio deepfake detection model using cross-scale attention and consistency learning achieves near-perfect performance across multiple challenging datasets.",
    "core_contribution": "Introduces a resolution-aware framework that explicitly models and aligns multi-resolution spectral representations through cross-scale attention and consistency learning, achieving strong performance while remaining computationally efficient.",
    "methodology": "The approach processes low, mid, and high-resolution spectrograms through separate feature extractors, uses cross-scale attention to dynamically emphasize resolution-invariant features, and applies consistency learning to enforce agreement across scales. The model uses 159k parameters and <1 GFLOP per inference.",
    "key_findings": "Achieves near-perfect EER of 0.16% on ASVspoof LA, 5.09% on ASVspoof PA, 4.54% on FoR rerecorded audio, and 4.81% EER with 0.98 AUC on in-the-wild deepfakes. Ablation studies confirm cross-scale attention and consistency learning are critical, and the model remains lightweight and efficient.",
    "limitations": "Limited to speaker-disjoint protocols, may not generalize to all future generative speech models, and relies on standardized datasets that may not capture all real-world scenarios.",
    "future_work": "Exploring generalization to new generative speech models, extending to other modalities, and improving robustness to unseen attack types.",
    "evaluation": "strong",
    "rating": 8
  },
  "2601.19786": {
    "tldr": "This paper systematically investigates how accent information is encoded in Discrete Speech Representation Tokens (DSRTs) and proposes new accent-aware token designs for controllable accent generation.",
    "core_contribution": "First systematic investigation of accent information in DSRTs, proposing a unified evaluation framework combining Accent ABX task and cross-accent Voice Conversion resynthesis, along with new content-only and content-accent DSRT designs that outperform existing approaches.",
    "methodology": "The authors propose a unified evaluation framework measuring accent accessibility via a novel Accent ABX task and recoverability via cross-accent Voice Conversion (VC) resynthesis. They analyze DSRTs from various speech encoders, finding that ASR supervision reduces accent information but naive codebook size reduction cannot effectively disentangle accent from phonetic and speaker information.",
    "key_findings": "Accent information is substantially reduced when ASR supervision is used to fine-tune encoders, but cannot be effectively disentangled from phonetic and speaker information through naive codebook size reduction. The proposed content-only and content-accent DSRTs significantly outperform existing designs in controllable accent generation.",
    "limitations": "Experiments are limited to English SSL models and the VCTK dataset, lacking subjective evaluation and testing with L2 English accents or zero-shot TTS scenarios. The study doesn't include large-scale pretraining data or models.",
    "future_work": "Extending evaluation to zero-shot TTS, additional large-scale model training, subjective listening tests, and testing with L2 English accents and datasets involving non-native speakers.",
    "evaluation": "medium",
    "rating": 7
  },
  "2601.20319": {
    "tldr": "This paper investigates how emotional speech and generative strategies affect ASR performance, introducing two fine-tuning approaches that improve emotion-aware ASR without degrading clean speech recognition.",
    "core_contribution": "Introduces two generative strategies (transcription correctness and emotional salience) for constructing fine-tuning subsets that improve ASR performance on emotional speech while maintaining neutral speech recognition.",
    "methodology": "Analyzes speech synthesized from three emotional TTS models (EmoVoice, MaskGCT, CosyVoice2), identifies substitution errors as dominant issue, and proposes two generative strategies for fine-tuning Qwen2-audio-7B model using supervised fine-tuning with LoRA adapters.",
    "key_findings": "Fine-tuning with generative strategies achieves consistent WER improvements on real emotional datasets (MSP Podcast Test1/2, IEMOCAP) without degradation on clean LibriSpeech; combined TTS-EMO-G strategy shows strongest gains, particularly for expressive speech.",
    "limitations": "Limited to three emotional TTS models, evaluation focused on English datasets, potential domain mismatch between synthetic training data and real emotional speech, and reliance on well-trained emotional TTS systems.",
    "future_work": "Extending approach to multilingual and low-resource scenarios, exploring additional emotional dimensions, and investigating more sophisticated generative strategies.",
    "evaluation": "medium",
    "rating": 7
  },
  "2602.00560": {
    "tldr": "A novel text-based speech editing method that decouples content editing from acoustic reconstruction using semantic tokens and Flow Matching, achieving superior imperceptibility and robustness.",
    "core_contribution": "Introduces a decoupled architecture that separates semantic editing from acoustic reconstruction, using a pre-trained TTS model as a consistency critic and GRPO for alignment, outperforming existing AR and NAR baselines.",
    "methodology": "Uses a decoder-only transformer as a policy model to edit semantic tokens, Flow Matching decoder for acoustic reconstruction, and Self-Consistency Rewards with GRPO to align edited tokens with original context using a pre-trained TTS model as implicit critic.",
    "key_findings": "Significantly outperforms state-of-the-art AR and NAR baselines in intelligibility (WER), speaker similarity, and perceptual quality (DNSMOS, Subjective MOS), with robust performance across varying edit durations and contexts.",
    "limitations": "Relies on pre-trained TTS models and ASR for evaluation, which may introduce dependency on model quality; GRPO alignment has negligible effect on speaker similarity; potential computational overhead from decoupled architecture.",
    "future_work": "Not explicitly mentioned, but implied directions include extending to multilingual settings, improving alignment methods, and exploring real-time applications.",
    "evaluation": "strong",
    "rating": 8
  },
  "2601.22873": {
    "tldr": "EmoShift introduces a lightweight activation-steering framework for emotion-aware TTS that learns emotion-specific steering vectors to enhance expressiveness while preserving naturalness and speaker similarity.",
    "core_contribution": "Proposes EmoShift, a parameter-efficient activation-steering method with an EmoSteer layer that learns emotion-specific steering vectors to control emotional expression in TTS without full fine-tuning.",
    "methodology": "EmoShift inserts an EmoSteer layer into an LLM-based TTS model to learn steering vectors for each emotion, capturing latent offsets in the output embedding space. The model is trained with only 10M parameters (less than 1/30 of full fine-tuning) using negative log-likelihood minimization.",
    "key_findings": "EmoShift outperforms zero-shot and fully fine-tuned baselines in both objective (Word Error Rate, Speaker Similarity, Emotion Recognition Accuracy) and subjective (Mean Opinion Score, Emotion MOS) evaluations while maintaining naturalness and speaker similarity.",
    "limitations": "The paper mentions limitations but doesn't elaborate extensively; potential issues include scalability to more emotional categories and generalization across different speakers and languages.",
    "future_work": "Extend EmoShift to more emotional categories and explore controllable emotional intensity in speech synthesis.",
    "evaluation": "medium",
    "rating": 7
  },
  "2602.01908": {
    "tldr": "LipSody improves lip-to-speech synthesis by explicitly modeling prosody consistency using speaker identity, linguistic content, and emotional context from facial video.",
    "core_contribution": "Introduces a novel visual-only prosody estimation method and integrates it into a diffusion-based lip-to-speech framework to enhance prosody consistency while maintaining intelligibility.",
    "methodology": "Builds on diffusion-based LipVoicer architecture, adds explicit prosody modeling using speaker identity, linguistic content, and emotional context from face video; uses conformer-based lip-reading and HiFi-GAN vocoder; trained with DDPM and CFG guidance.",
    "key_findings": "Significantly improves prosody-related metrics (pitch deviation, energy consistency, speaker similarity) over LipVoicer while maintaining high intelligibility (WER); subjective tests show better naturalness and preference.",
    "limitations": "Focuses on prosody consistency but may still face challenges in extreme emotional or speaker variability; evaluation limited to LRS3 dataset; some results omitted due to space constraints.",
    "future_work": "Potential to explore richer emotional modeling, multi-speaker adaptation, and real-time deployment; further validation on diverse datasets.",
    "evaluation": "medium",
    "rating": 7
  },
  "2601.03403": {
    "tldr": "A systematic formalization of Tigrinya number verbalization rules with open-source implementation, revealing significant gaps in LLM capabilities for this low-resource language.",
    "core_contribution": "First comprehensive documentation of Tigrinya cardinal and ordinal number verbalization rules, including a formal algorithm and open-source implementation.",
    "methodology": "Developed formal rules for Tigrinya number verbalization covering cardinals, ordinals, currency, dates, times, and phone numbers; implemented algorithm; evaluated six frontier LLMs (GPT-5 Mini, Opus 4.5, Gemini 3 Flash) on 100 test examples across six categories.",
    "key_findings": "LLMs show substantial deficiencies in Tigrinya number verbalization, with best model (Opus 4.5) achieving only 65% accuracy overall; performance degrades significantly for non-cardinal categories; GPT-5 Mini failed on most cases due to token limits.",
    "limitations": "LLM evaluation assumes basic Tigrinya support without official model claims; limited test set; regional dialect variations not fully addressed; GPT-5 Mini token budget constraints affected results.",
    "future_work": "Expand LLM evaluation with larger test sets as models improve; address regional dialect variations; apply rules to language modeling, speech synthesis, and accessibility applications.",
    "evaluation": "medium",
    "rating": 7
  },
  "2601.05329": {
    "tldr": "CosyEdit is an end-to-end speech editing model adapted from a zero-shot TTS model, achieving high-quality speech editing without external alignment modules.",
    "core_contribution": "Introduces CosyEdit, an end-to-end speech editing model fine-tuned from CosyVoice, which eliminates the need for external alignment modules and achieves performance comparable to state-of-the-art cascade systems.",
    "methodology": "Fine-tuned a 400M-parameter CosyVoice model on a curated 250-hour GigaEdit dataset, using a combination of autoregressive and non-autoregressive components, with optimized inference procedures to internalize speech-text alignment.",
    "key_findings": "CosyEdit outperforms several billion-parameter language model baselines and matches state-of-the-art cascade approaches on the RealEdit benchmark, demonstrating robust and efficient speech editing capabilities.",
    "limitations": "Limited to 250 hours of supervised data, potential challenges in handling highly complex or out-of-domain editing tasks, and reliance on the quality of the underlying TTS model.",
    "future_work": "Exploring applications in watermarking and speech forgery detection, and further improving the model's ability to handle diverse and complex editing scenarios.",
    "evaluation": "strong",
    "rating": 8
  },
  "2601.07064": {
    "tldr": "SIGNAL unifies graph-based learning and open-set detection to attribute synthetic speech to its source and detect unseen generators.",
    "core_contribution": "A hybrid GNN-KNN framework that combines relational modeling with instance-level classification for synthetic speech attribution and open-set detection.",
    "methodology": "SIGNAL uses speech foundation models (SFMs) to extract utterance embeddings, then applies parallel GNN and KNN branches. The GNN models class-level relationships via a query-conditioned graph over generator prototypes, while the KNN branch enables open-set detection through confidence thresholding. The framework is evaluated using DiffSSD and SingFake datasets.",
    "key_findings": "SIGNAL consistently improves performance across both source attribution and open-set detection tasks. Mamba-based embeddings delivered the strongest results, with the hybrid GNN+KNN architecture achieving ACC: 83.27%, F1: 82.63%, and EER: 14.78% on DiffSSD. The framework shows robust generalization to unseen generators.",
    "limitations": "Evaluation is limited to two public datasets (DiffSSD and SingFake), and the study focuses on diffusion-based TTS systems without exploring other generative model types. The framework's performance on real-world, diverse audio environments is not assessed.",
    "future_work": "Exploring the framework's application to other generative model types, improving robustness in real-world scenarios, and extending the approach to broader audio forensics tasks are suggested future directions.",
    "evaluation": "medium",
    "rating": 7
  },
  "2601.07367": {
    "tldr": "FOCAL is a benchmarking framework for evaluating multi-modal voice agents with novel reasoning and semantic metrics.",
    "core_contribution": "Introduces FOCAL, a framework for end-to-end evaluation of multi-modal agents with component-wise error analysis and two novel metrics (Reasoning and Semantic scores).",
    "methodology": "Proposes a cascading pipeline architecture for voice agents, automated and human-assisted testing, and evaluates ASR/TTS performance alongside reasoning and semantic metrics.",
    "key_findings": "Demonstrates evaluation of a RAG-based shopping agent under different modalities, highlighting error propagation and incomplete responses in certain cases.",
    "limitations": "Limited details on experimental setup, no specific quantitative results or comparisons with baselines provided.",
    "future_work": "Not explicitly mentioned in the provided excerpts.",
    "evaluation": "weak",
    "rating": 4
  },
  "2601.08450": {
    "tldr": "Decoding order in autoregressive speech synthesis significantly impacts quality, with adaptive strategies outperforming fixed left-to-right approaches.",
    "core_contribution": "Demonstrates that fixed decoding orders like left-to-right are suboptimal for speech synthesis, and adaptive decoding strategies yield better performance.",
    "methodology": "Uses masked diffusion framework to investigate arbitrary decoding orders, interpolating between identity and random permutations, comparing fixed strategies (l2r, r2l) with adaptive ones (Top-K), and quantizing acoustic representations to 1-bit.",
    "key_findings": "Adaptive decoding orders outperform fixed ones, with Top-K showing best results; randomness in decoding order affects speech quality; even 1-bit quantization supports high-quality speech.",
    "limitations": "Limited to LJSpeech dataset; quantisation may not capture all frequency correlations; subjective evaluations show discrepancies with automatic metrics.",
    "future_work": "Exploring reduced frequency correlations in quantization; integrating diffusion and autoregressive models more effectively; extending to other datasets and languages.",
    "evaluation": "medium",
    "rating": 7
  },
  "2601.05554": {
    "tldr": "SPAM is a new automatic metric for evaluating style prompt adherence in TTS that is both plausible and faithful by aligning acoustic attributes with style prompts.",
    "core_contribution": "Introduces SPAM, a Style Prompt Adherence Metric that explicitly satisfies plausibility and faithfulness for prompt-based TTS evaluation.",
    "methodology": "Factorizes speech into acoustic attributes and aligns them with style prompts using a CLAP-inspired approach with supervised contrastive loss; trained with RA-CLAP teacher model and evaluated on TextrolSpeech and LibriTTS-P datasets.",
    "key_findings": "SPAM achieved strong correlation with MOS (0.520 and 0.429) and demonstrated faithful evaluation by discriminating different semantics of prompts across datasets.",
    "limitations": "Limited to two datasets, potential dataset bias, and reliance on existing models like RA-CLAP for training.",
    "future_work": "Not explicitly mentioned in the provided text.",
    "evaluation": "medium",
    "rating": 7
  },
  "2601.12254": {
    "tldr": "A confidence-based filtering method using discrete token log-probabilities effectively detects hallucination errors in generative speech enhancement models, improving TTS dataset quality.",
    "core_contribution": "Proposes a non-intrusive filtering method that leverages confidence scores from discrete token-based generative speech enhancement models to detect hallucination errors missed by conventional quality metrics.",
    "methodology": "Uses log-probabilities of generated discrete tokens as confidence scores to filter hallucination errors from GSE models. Employs Genhancer as the backbone model with DAC audio tokenizer and WavLM features. Trains TTS models (Matcha-TTS) on curated datasets and evaluates using MOS, DNSMOS, and ASR metrics.",
    "key_findings": "Confidence scores strongly correlate with intrusive SE metrics and effectively detect GSE-specific hallucination errors. Curating in-the-wild datasets with this method improves TTS model performance compared to unfiltered enhanced data or conventional filtering methods.",
    "limitations": "Currently limited to discrete token-based GSE models; does not address continuous latent space models. Trade-off exists between data quantity and quality when applying filtering thresholds.",
    "future_work": "Extending the approach to GSE models operating in continuous latent space and exploring additional confidence measures for broader applicability.",
    "evaluation": "medium",
    "rating": 7
  },
  "2601.14472": {
    "tldr": "A neural vocoder that jointly models magnitude and phase using prosody-guided harmonic attention, achieving improved pitch accuracy and naturalness over state-of-the-art models.",
    "core_contribution": "Introduces prosody-guided harmonic attention to enhance voiced segment encoding and directly predicts complex spectral components for waveform synthesis, ensuring phase coherence and improved pitch fidelity.",
    "methodology": "The approach uses prosody-guided harmonic attention to condition spectral modeling on F0, jointly modeling magnitude and phase within a unified architecture. It employs a multi-objective training strategy integrating adversarial, spectral, and phase-aware losses, and generates waveforms via inverse STFT.",
    "key_findings": "Experiments on LJSpeech and VCTK datasets show F0 RMSE reduced by 22%, voiced/unvoiced error lowered by 18%, and MOS scores improved by 0.15 over HiFi-GAN and AutoVocoder, demonstrating more natural, pitch-accurate, and robust synthetic speech.",
    "limitations": "The paper does not explicitly discuss limitations, but potential issues include reliance on accurate F0 extraction, possible computational overhead from complex spectrum modeling, and the need for further validation on diverse, real-world datasets.",
    "future_work": "Future directions include broadening the applicability of the architecture to more expressive TTS pipelines and further improving robustness and naturalness in diverse acoustic conditions.",
    "evaluation": "medium",
    "rating": 7
  },
  "2601.17761": {
    "tldr": "AR-Omni is a unified autoregressive model that generates text, images, and speech using a single Transformer decoder without external expert components.",
    "core_contribution": "Presents the first unified autoregressive model for any-to-any generation across text, images, and speech using a single decoder architecture.",
    "methodology": "Uses a single Transformer decoder with interleaved multimodal token streams, task-aware loss reweighting for modality imbalance, token-level perceptual alignment loss for visual fidelity, and finite-state decoding for stability-creativity trade-offs.",
    "key_findings": "Achieves competitive performance across all three modalities while maintaining real-time speech synthesis (0.88 RTF) and diffusion-free image generation, demonstrating the viability of unified autoregressive modeling.",
    "limitations": "Still faces challenges with modality imbalance and visual fidelity compared to specialized models, and the unified approach may sacrifice some performance compared to modality-specific expert systems.",
    "future_work": "Enhancing the quality of generated outputs across all modalities and exploring larger-scale pretraining for improved performance.",
    "evaluation": "medium",
    "rating": 7
  },
  "2601.20230": {
    "tldr": "A semi-cascaded full-duplex dialogue system using multimodal LLMs and minimal conversational units achieves strong performance without training.",
    "core_contribution": "Introduces a unit-based agent framework that decomposes dialogue into minimal conversational units for independent processing and prediction of transitions.",
    "methodology": "Implements a semi-cascaded full-duplex dialogue system built around a multimodal large language model, supported by auxiliary modules like voice activity detection (VAD) and text-to-speech (TTS) synthesis, operating in a train-free, plug-and-play manner.",
    "key_findings": "Achieves second place among all teams on the HumDial test set of the Human-like Spoken Dialogue Systems Challenge (Track 2: Full-Duplex Interaction), demonstrating effectiveness of the framework.",
    "limitations": "Limited technical details provided in excerpts, unclear specific metrics and performance numbers, potential lack of robustness testing across diverse dialogue scenarios.",
    "future_work": "Not explicitly mentioned in provided excerpts, but potential directions could include expanding to more complex dialogue scenarios and improving transition prediction accuracy.",
    "evaluation": "medium",
    "rating": 7
  },
  "2602.01170": {
    "tldr": "EmoAra is an end-to-end pipeline that preserves emotional context when translating English speech to Arabic speech for banking customer service.",
    "core_contribution": "Integration of emotion-preserving components (SER, ASR, MT, TTS) into a unified cross-lingual spoken communication system.",
    "methodology": "Uses CNN for emotion detection, Whisper for ASR, fine-tuned MarianMT for translation, and MMS-TTS-Ara for speech synthesis; employs transfer learning and domain-specific fine-tuning.",
    "key_findings": "Achieved 94% F1-score for emotion classification, BLEU 56 and BERTScore F1 88.7% for translation, and 81% human evaluation score on banking-domain translations.",
    "limitations": "Limited dataset size for training, challenges with very long sentences, overlapping acoustic features for certain emotions, and reliance on pre-trained models.",
    "future_work": "Improving handling of long sentences, expanding dataset size, and exploring alternative architectures for better emotion classification.",
    "evaluation": "medium",
    "rating": 7
  },
  "2601.03632": {
    "tldr": "ReStyle-TTS introduces a framework for continuous and reference-relative style control in zero-shot TTS by decoupling text and reference guidance while preserving speaker timbre.",
    "core_contribution": "Proposes Decoupled Classifier-Free Guidance (DCFG) to reduce model dependency on reference style, combined with style-specific LoRAs and Orthogonal LoRA Fusion for continuous multi-attribute control, plus Timbre Consistency Optimization to prevent timbre drift.",
    "methodology": "The approach fine-tunes a base zero-shot TTS model with DCFG to independently control text and reference guidance, applies style-specific LoRAs with orthogonal fusion for disentangled attribute control, and adds TCO to maintain speaker timbre consistency during style manipulation.",
    "key_findings": "Experiments show ReStyle-TTS achieves superior performance in continuous pitch, energy, and emotion control while maintaining speaker timbre and intelligibility, outperforming baselines in both matched and mismatched reference-target style scenarios.",
    "limitations": "The method requires training separate LoRA modules for different style attributes, which may increase computational overhead, and the evaluation focuses primarily on controlled datasets rather than real-world diverse scenarios.",
    "future_work": "Potential extensions include applying the framework to more diverse emotional datasets, exploring additional style attributes, and investigating more efficient ways to combine multiple LoRA modules.",
    "evaluation": "medium",
    "rating": 7
  },
  "2601.03170": {
    "tldr": "A training-free method for fine-grained intra-utterance emotion and duration control in TTS using segment-aware conditioning and LLM-based prompt generation.",
    "core_contribution": "Introduces a segment-aware emotion conditioning strategy with causal masking and monotonic stream alignment, combined with duration steering, enabling smooth intra-utterance emotional shifts and local duration adjustments without retraining.",
    "methodology": "Proposes segment-aware emotion conditioning using causal masking and monotonic stream alignment filtering, plus segment-aware duration steering combining local duration embedding steering with global EOS logit modulation. Constructs a 30,000-sample multi-emotion and duration-annotated text dataset for LLM-based automatic prompt generation.",
    "key_findings": "Achieves state-of-the-art intra-utterance consistency in multi-emotion and duration control while maintaining baseline-level speech quality. Demonstrates robust emotional fidelity and smooth emotion transitions, outperforming comparative methods in both objective and subjective evaluations.",
    "limitations": "Relies on a constructed dataset for LLM-based prompt generation, which may limit generalizability. The framework does not explicitly model gradual emotion evolution, potentially affecting continuous emotion transitions.",
    "future_work": "Improving continuous emotion evolution modeling and duration control, exploring better model continuous emotion evolution and duration fine-tuning, and enhancing model evaluation protocols.",
    "evaluation": "strong",
    "rating": 8
  },
  "2601.21886": {
    "tldr": "The paper proposes using utterance-level speech quality predictors with segment-based consistency constraints to improve frame-level interpretability and localization of TTS synthesis artifacts.",
    "core_contribution": "Demonstrates that utterance-level SQA models can be regularized with segment-based consistency constraints to reduce frame-level stochasticity, enabling better localization of synthesis artifacts.",
    "methodology": "Trains SQA models (including WavLM-based architectures) with consistency losses on utterance-level data, then applies them to detect low-quality segments in TTS outputs. Uses intersection-based evaluation and listening tests to validate findings.",
    "key_findings": "Consistency regularization improves frame-level predictions. Models successfully detect synthesis artifacts with higher precision than random control sets. Listening tests confirm listeners rate detected segments as lower quality.",
    "limitations": "Frame-level predictions remain stochastic despite regularization. Detection performance drops with long contexts. Model may produce false positives for non-verbal vocalizations.",
    "future_work": "Investigate reducing false positives, improving context handling, and exploring alternative regularization strategies for frame-level predictions.",
    "evaluation": "medium",
    "rating": 7
  },
  "2601.20094": {
    "tldr": "T-Mimi replaces Mimi's convolutional decoder with a transformer-only architecture to achieve 9x faster on-device TTS with minimal quality loss.",
    "core_contribution": "A purely transformer-based decoder for the Mimi codec that reduces on-device TTS latency from 42.1ms to 4.4ms while maintaining audio quality.",
    "methodology": "Replaces convolutional components in Mimi's decoder with transformer layers inspired by TS3-Codec, employs quantization-aware training with mixed precision (8-bit for most layers, 32-bit for final layers), and optimizes for mobile CPU efficiency using XNNPACK.",
    "key_findings": "Achieves 9x latency reduction (42.1ms → 4.4ms), maintains comparable audio quality (PESQ scores near baseline), 8-bit quantization reduces storage by 75% with minimal quality loss, final two transformer layers require 32-bit precision for quality preservation.",
    "limitations": "Limited evaluation on in-house dataset only, quantization sensitivity in final layers restricts compression benefits, no comparison with other real-time TTS systems beyond Mimi baseline.",
    "future_work": "Extending approach to other codecs beyond Mimi, exploring adaptive quantization strategies, and applying methodology to broader range of acoustic models and streaming applications.",
    "evaluation": "medium",
    "rating": 7
  },
  "2601.20510": {
    "tldr": "A multi-view detection approach combining semantic, structural, and signal-level analysis outperforms single-paradigm detectors against diverse TTS architectures.",
    "core_contribution": "First empirical characterization of how detection architectures perform against three state-of-the-art TTS models (Dia2, Maya1, MeloTTS), demonstrating that integrated detection strategies are necessary for robust audio deepfake detection.",
    "methodology": "Generated 12,000 synthetic audio samples using three TTS models (Dia2, Maya1, MeloTTS) on Daily-Dialog dataset, evaluated against four detection frameworks (Whisper-MesoNet, SSL-AASIST, XLS-R-SLS, proprietary UncovAI model) across semantic, structural, and signal-level approaches.",
    "key_findings": "Single-paradigm detectors show significant variability in performance across TTS architectures, with proprietary model achieving near-perfect separation (EER < 1%) across all models, while multi-view approach demonstrates robust performance across all evaluated models.",
    "limitations": "Proprietary detection model results may not be reproducible, limited to three specific TTS architectures, evaluation focused on synthetic speech rather than real-world deepfakes, no comparison with human detection capabilities.",
    "future_work": "Extend evaluation to additional TTS architectures, explore real-world deepfake scenarios, investigate optimal feature fusion strategies, develop adaptive detection systems that can evolve with emerging TTS technologies.",
    "evaluation": "strong",
    "rating": 8
  },
  "2601.20481": {
    "tldr": "TruS introduces a training-free speaker unlearning method for zero-shot TTS that suppresses specific speaker identities at inference time without retraining.",
    "core_contribution": "The first training-free speaker unlearning framework that generalizes to both seen and unseen opt-out speakers by steering identity-specific hidden activations in TTS models.",
    "methodology": "TruS works by steering internal representations in TTS models to suppress identity-related activations using an ID-prototype computed from intermediate features and a pretrained speaker verification model, operating entirely at inference time on pretrained models like F5-TTS.",
    "key_findings": "TruS effectively prevents voice generation of both seen and unseen opt-out speakers while preserving other attributes like prosody and emotion, achieving comparable unlearning performance to retraining-based methods without the computational cost.",
    "limitations": "The method requires careful layer selection and ID-prototype pool size tuning, and performance may degrade if too many layers are selected or if the retain speaker set is too small.",
    "future_work": "Extending the approach to other TTS architectures beyond DiT-based models and exploring more sophisticated steering mechanisms for better identity suppression.",
    "evaluation": "medium",
    "rating": 7
  },
  "2601.19781": {
    "tldr": "Phonological Tokenizer fine-tunes phonetic tokens via differentiable k-means with multi-task ASR and speech resynthesis objectives to better capture linguistic and prosodic information while discarding speaker identity.",
    "core_contribution": "Introduces a phonological tokenizer that effectively incorporates prosodic information through multi-objective fine-tuning of SSL models, achieving superior performance in prosody-sensitive tasks compared to existing acoustic and phonetic tokenizers.",
    "methodology": "Fine-tunes WavLM-large SSL features using differentiable k-means clustering with weighted ASR and speech resynthesis losses, trained on LibriSpeech-100h subset, evaluated across discriminative (ASR, ER, SID), generative (reconstruction, VC), and speechLM tasks.",
    "key_findings": "Outperforms baselines in prosody-sensitive tasks (ER, VC), achieves competitive ASR performance, demonstrates strong speechLM capabilities (GenPPL, UTMOS), and shows optimal performance with balanced α weighting between ASR and reconstruction objectives.",
    "limitations": "Limited to 30-hour training subset, requires careful hyperparameter tuning (α weight), evaluation primarily on English datasets, potential scalability concerns for larger models or languages.",
    "future_work": "Scaling up training with more data, enabling inference-time adaptation, exploring multilingual applications, and investigating larger model architectures.",
    "evaluation": "medium",
    "rating": 7
  },
  "2601.13801": {
    "tldr": "HoverAI is an embodied aerial agent that combines drone mobility, visual projection, and conversational AI to enable natural human-drone interaction with adaptive avatars.",
    "core_contribution": "Integration of drone mobility, infrastructure-independent visual projection, and real-time conversational AI into a unified platform for socially responsive human-drone interaction.",
    "methodology": "The system uses a MEMS laser projector and onboard semi-rigid screen for visual output, RGB camera for vision, Whisper for ASR, LLM-based intent classification, RAG for dialogue, face analysis for personalization, and XTTS v2 for voice synthesis, all integrated into a multimodal pipeline.",
    "key_findings": "High accuracy in command recognition (F1: 0.90), demographic estimation (gender F1: 0.89, age MAE: 5.14 years), and speech transcription (WER: 0.181) across 12 participants in controlled experiments.",
    "limitations": "Evaluation conducted in controlled environments only, potential for users to overestimate system capabilities, and technical limitations of current approaches for real-world deployment.",
    "future_work": "Include real-world deployment testing, address risks of users overestimating system capabilities, and improve robustness for noisy environments.",
    "evaluation": "medium",
    "rating": 7
  },
  "2601.22661": {
    "tldr": "A paper introducing Mean Continuation Log-Probability (MCLP) as both an evaluation metric and reward signal for improving expressive role-play TTS using Large Audio Language Models.",
    "core_contribution": "Proposes MCLP to quantify stylistic consistency in role-play TTS and uses it as a reinforcement learning reward to enhance style alignment with instructions.",
    "methodology": "Leverages in-context learning of pre-trained LALMs to compute MCLP via continuation log-probability prediction, measuring likelihood of ground-truth speech conditioned on generated speech. Employs MCLP as RL reward using GRPO to align RP-TTS model with role-play instructions.",
    "key_findings": "Significant improvements over strong LALM baselines on both objective (WER, CER, Pinyin Error Rate) and subjective metrics (MOS approaching ground truth at 3.646), with exceptionally low CER and Pinyin error rates.",
    "limitations": "Dataset construction relies on filtering from existing corpora, potential reward hacking concerns, and evaluation focused on Chinese language only.",
    "future_work": "Open-sourcing the constructed RP-TTS dataset, extending to more languages, and exploring more sophisticated reward mechanisms.",
    "evaluation": "strong",
    "rating": 8
  },
  "2601.12289": {
    "tldr": "ParaMETA is a unified framework for learning disentangled paralinguistic speaking style representations from speech, enabling both recognition and controllable TTS generation.",
    "core_contribution": "Introduces a projection-based approach to learn task-specific embeddings for multiple speaking styles (emotion, gender, age, language) in a single model, reducing inter-task interference and enabling fine-grained style control in TTS.",
    "methodology": "Uses dedicated subspaces for each style type to project speech embeddings, avoiding cross-task suppression. Supports both speech- and text-based prompting for TTS. Trained on a multilingual multi-style dataset with subject-independent evaluation.",
    "key_findings": "Outperforms strong baselines in classification accuracy (best in 12/16 backbone-task combinations) and generates more natural, expressive speech. Achieves 70% reduction in CUDA memory usage compared to CLAP-based methods.",
    "limitations": "Not universally beneficial for all tasks; some negative transfer still observed. Requires careful hyperparameter tuning (e.g., update rate m=0.99).",
    "future_work": "Further exploration of the approach's benefits across more tasks and datasets; potential improvements in handling cross-task interference.",
    "evaluation": "strong",
    "rating": 8
  },
  "2601.07303": {
    "tldr": "A challenge and dataset for detecting component-level audio deepfakes where speech and environmental sounds can be manipulated independently",
    "core_contribution": "Introduction of CompSpoofV2 dataset and ESDD2 challenge for component-level audio anti-spoofing with separation-enhanced joint learning framework",
    "methodology": "Separation-enhanced joint learning framework that processes speech and environmental sounds independently, with five-class classification (genuine speech, genuine sound, spoofed speech, spoofed sound, both spoofed) and evaluation using EER and Macro-F1 score",
    "key_findings": "CompSpoofV2 contains over 250k audio samples (283 hours) with five categories; baseline model performance evaluated on validation, evaluation, and test sets using CodaBench platform with Macro-F1 as primary metric",
    "limitations": "Component-level manipulation detection is inherently more challenging than whole audio deepfake detection; limited information on baseline model architecture and performance metrics in the provided text",
    "future_work": "Extension of dataset and framework for more realistic detection scenarios; potential integration with additional public datasets (subject to approval)",
    "evaluation": "medium",
    "rating": 7
  },
  "2601.17880": {
    "tldr": "QURAN-MD is a comprehensive multimodal dataset of the Quran integrating Arabic text, English translations, transliterations, and audio recordings from 32 reciters at both verse and word levels.",
    "core_contribution": "A fine-grained multilingual multimodal dataset enabling computational analysis of Quranic recitation, supporting applications in NLP, speech recognition, TTS, tajweed detection, and multimodal embeddings.",
    "methodology": "Aggregated and harmonized three public sources: Kaggle verse-level speech-to-text recordings from 30 reciters, Quranic Arabic Corpus for linguistic annotations, and additional audio data, organized hierarchically in JSON with verse and word-level alignments.",
    "key_findings": "Dataset enables fine-grained analysis of pronunciation, phonology, and semantic context; supports ASR, tajweed error detection, Quranic TTS, style transfer, prosody modeling, and personalized tutoring systems.",
    "limitations": "No explicit quantitative evaluation metrics provided; dataset construction relies on existing public sources without novel alignment or annotation methods; potential quality issues from source data not addressed.",
    "future_work": "Applications in multimodal embeddings, semantic retrieval, style transfer, personalized tutoring, and further development of ASR and tajweed detection models.",
    "evaluation": "weak",
    "rating": 6
  },
  "2602.01030": {
    "tldr": "This paper systematically investigates speech bias in multilingual MLLMs across linguistic, demographic, and positional variations, revealing high sensitivity to language and option order but relative robustness to demographic factors.",
    "core_contribution": "The first systematic investigation of speech bias in multilingual MLLMs, introducing the BiasInEar dataset and a unified framework for assessing fairness and robustness in speech-integrated LLMs.",
    "methodology": "Constructed BiasInEar dataset (70.8 hours, 11,200 questions) spanning English, Chinese, and Korean with balanced gender and accent; evaluated nine representative models using four metrics (accuracy, entropy, APES, Fleiss' κ) under linguistic, demographic, and structural perturbations.",
    "key_findings": "MLLMs are relatively robust to demographic factors but highly sensitive to language and option order; architectural design and reasoning strategy substantially affect robustness across languages; larger models show higher consistency but still exhibit structural biases.",
    "limitations": "Dataset derived from Global-MMLU-Lite may not fully capture natural speech variations; some open-source models excluded due to API constraints; findings may not generalize to all speech tasks or domains.",
    "future_work": "Broaden evaluation to standardized interdisciplinary benchmarks; investigate natural speech variations; explore mitigation strategies for structural biases in speech-integrated LLMs.",
    "evaluation": "strong",
    "rating": 8
  },
  "2601.04656": {
    "tldr": "FlexiVoice is a zero-shot TTS system that enables flexible style control through natural language instructions and speech references, using progressive post-training techniques.",
    "core_contribution": "Introduces a novel Progressive Post-Training (PPT) scheme with Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO) to achieve accurate instruction following and factor disentanglement in zero-shot TTS.",
    "methodology": "Built on a pre-trained LLM core with auto-regressive generation and flow matching; employs DPO to align emotional output with instructions, GRPO to disentangle style, timbre, and content, and Instruction GRPO for advanced instruction following using audio-language model rewards.",
    "key_findings": "Outperforms baselines and commercial models on naturalness, controllability, and robustness; achieves strong decoupling of control factors with human evaluation confirming performance; demonstrates effective zero-shot voice cloning and style control.",
    "limitations": "Requires large-scale annotated datasets; computational cost of training with multiple reward models; potential challenges in handling highly complex or conflicting instructions.",
    "future_work": "Expanding to more languages and styles; improving efficiency of reward model inference; exploring applications in more diverse real-world scenarios.",
    "evaluation": "strong",
    "rating": 8
  },
  "2601.18438": {
    "tldr": "UrgentMOS is a unified speech quality assessment framework that jointly learns from multiple quality metrics and pairwise preferences to achieve state-of-the-art performance across diverse speech quality datasets.",
    "core_contribution": "A unified multi-metric and preference learning framework that can handle heterogeneous datasets with incomplete annotations and supports both absolute and comparative evaluation.",
    "methodology": "Multi-branch architecture with absolute MOS prediction and pairwise preference modeling using cross-attention; trained on diverse datasets with both metric-based and preference-based supervision; handles missing metric annotations through unified learning.",
    "key_findings": "Achieves state-of-the-art performance in both absolute (LCC/SRCC) and comparative (preference accuracy) evaluation across TTS, speech enhancement, and other speech quality datasets; demonstrates robustness to dataset heterogeneity and missing annotations.",
    "limitations": "Performance can degrade when using too many metrics; some dataset-specific biases remain; requires careful threshold tuning for preference evaluation.",
    "future_work": "Exploring more efficient architectures; extending to additional speech quality dimensions; improving cross-dataset generalization.",
    "evaluation": "strong",
    "rating": 8
  },
  "2601.11027": {
    "tldr": "WenetSpeech-Wu introduces the first large-scale, multi-dimensionally annotated open-source speech corpus and benchmark for the Wu dialect, along with strong open-source models across multiple speech tasks.",
    "core_contribution": "The first large-scale, multi-dimensionally annotated open-source speech corpus (WenetSpeech-Wu) for the Wu dialect, a standardized benchmark (WenetSpeech-Wu-Bench), and a suite of strong open-source models across ASR, TTS, speech understanding, and instruction-following TTS.",
    "methodology": "The authors created a 8,000-hour multi-dimensionally annotated speech corpus for the Wu dialect, established a standardized benchmark covering ASR, Wu-to-Mandarin translation, speaker attribute prediction, speech emotion recognition, TTS synthesis, and instruction-following TTS, and trained multiple strong open-source models including Conformer-U2pp-Wu, Whisper-Medium-Wu, and Step-Audio2-Wu-ASR for ASR, as well as TTS and instruct TTS models.",
    "key_findings": "The proposed models achieved competitive performance across multiple tasks, with the Step-Audio2-Wu-ASR model showing state-of-the-art performance across all model scales. The instruct TTS models substantially outperformed existing approaches, and the unified speech understanding models demonstrated strong performance on the WenetSpeech-Wu corpus.",
    "limitations": "The paper acknowledges that limitations remain to be addressed in future work, though specific limitations are not detailed in the provided excerpts. The manual labeling process may affect model generalization, and the baseline models are trained on in-house data which may not be as accessible.",
    "future_work": "Future work may further improve performance through more specialized modeling and training strategies, and address the remaining limitations of the current approach.",
    "evaluation": "medium",
    "rating": 8
  },
  "2601.04029": {
    "tldr": "Large Audio-Language Models struggle to reliably judge speaker consistency in multi-turn dialogues, often prioritizing text over acoustic cues.",
    "core_contribution": "SpeakerSleuth benchmark evaluates LALMs' ability to assess speaker consistency across multi-turn conversations with 1,818 human-verified instances across four diverse datasets.",
    "methodology": "Three tasks: Consistency (detect if any turn disrupts speaker identity), Localization (identify which turn is problematic), and Discrimination (choose best-matching audio among variants). Evaluated nine LALMs and three speaker embedding methods across four datasets with controlled acoustic difficulty.",
    "key_findings": "LALMs perform poorly on consistency and localization tasks (most scoring below 50%), dramatically degrade when other interlocutors' turns are included, but excel at discrimination tasks (82.6% accuracy). Models prioritize textual coherence over acoustic cues, failing to detect obvious gender switches.",
    "limitations": "Limited analysis of model biases, did not explicitly analyze acoustic feature utilization, benchmark may not cover all real-world scenarios, evaluation focused on specific model architectures and datasets.",
    "future_work": "Developing methods to better leverage dialogue context, addressing modality imbalances between text and acoustics, improving model calibration for speaker consistency evaluation, exploring more robust evaluation frameworks.",
    "evaluation": "strong",
    "rating": 8
  },
  "2601.13910": {
    "tldr": "A comprehensive survey of deep-learning-based singing voice synthesis approaches, categorizing systems and analyzing core technologies.",
    "core_contribution": "Systematic categorization of SVS models into cascaded and end-to-end paradigms, with in-depth analysis of singing modeling, control techniques, and relevant resources.",
    "methodology": "Literature review approach organizing existing SVS systems by task type and architecture, analyzing core technologies including singing-voice modeling, control techniques, and reviewing datasets, annotation tools, and evaluation benchmarks.",
    "key_findings": "Identification of two major SVS paradigms (cascaded and end-to-end), analysis of three core technologies (singing-voice modeling, control techniques, and evaluation methods), and comprehensive overview of datasets and resources.",
    "limitations": "This is a survey paper rather than original research, so it doesn't present new experimental results or metrics, and relies on existing literature for its analysis.",
    "future_work": "Potential for MLLM-like architectures in SVS, exploration of sparse architectures, and development of more sophisticated evaluation benchmarks and datasets.",
    "evaluation": "weak",
    "rating": 7
  },
  "2601.17086": {
    "tldr": "SonoEdit introduces a null-space constrained model editing technique for correcting pronunciation errors in LLM-based TTS systems without retraining.",
    "core_contribution": "A one-shot pronunciation correction method that modifies specific word pronunciations while provably preserving all other model behavior through null-space constrained editing.",
    "methodology": "The approach combines acoustic causal tracing to identify relevant Transformer layers (layers 15-21) with null-space constrained editing to compute closed-form weight updates that correct target pronunciations while remaining orthogonal to the subspace governing general speech generation.",
    "key_findings": "SonoEdit achieves 2.8% Target-WER for pronunciation correction while maintaining 3.15% Global-WER, outperforming unconstrained methods and demonstrating minimal degradation in speech quality metrics like WER and prosodic structure preservation.",
    "limitations": "The method requires careful layer identification through causal tracing, may be limited to specific types of pronunciation errors, and the evaluation dataset (HardNoun-300) focuses on proper nouns rather than general pronunciation issues.",
    "future_work": "Potential extensions include applying the technique to broader pronunciation correction tasks, exploring applications to other speech generation models, and investigating the method's effectiveness across different languages and dialects.",
    "evaluation": "medium",
    "rating": 7
  },
  "2601.15596": {
    "tldr": "DeepASMR introduces the first zero-shot TTS system for ASMR speech generation using a two-stage LLM-based approach that can synthesize high-fidelity ASMR from a single snippet of normal speech.",
    "core_contribution": "The first framework for zero-shot ASMR speech generation that can create ASMR-style speech from any speaker using only normal speech as input, eliminating the need for whispered training data.",
    "methodology": "A two-stage architecture: (1) LLM-based text-to-semantic encoder that disentangles content from style using discrete speech tokens, and (2) flow-matching acoustic decoder for timbre reconstruction. The system leverages a novel dataset (DeepASMR-DB) and employs virtual pool retrieval for style modeling.",
    "key_findings": "DeepASMR outperforms baseline zero-shot TTS models (CosyVoice2, F5TTS) and commercial models in naturalness and style fidelity for ASMR generation, achieving high performance in both intra and cross-style synthesis scenarios while maintaining competitive performance on normal speech synthesis.",
    "limitations": "The paper doesn't explicitly discuss limitations, but potential issues include dependency on the quality of the input speech snippet, possible challenges with extreme voice characteristics, and the need for specialized evaluation protocols for ASMR-specific qualities.",
    "future_work": "The paper doesn't explicitly mention future work directions, but potential areas could include extending to more languages, improving handling of extreme voice characteristics, and developing more sophisticated evaluation metrics for ASMR-specific qualities.",
    "evaluation": "strong",
    "rating": 8
  }
}