# TTS Papers - 2023-04-24

Total: 1

## Zero-shot text-to-speech synthesis conditioned using self-supervised speech representation model
- **Authors**: Kenichi Fujita et.al.
- **arXiv**: [2304.11976](https://arxiv.org/abs/2304.11976)
- **Tags**: zero-shot, synthesis
- **Abstract**: Recent advances in text-to-speech have significantly improved the expressiveness of synthesized speech. However, it is still challenging to generate speech with contextually appropriate and coherent speaking style for multi-sentence text in audiobooks. In this paper, we propose a context-aware coherent speaking style prediction method for audiobook speech synthesis. To predict the style embedding of the current utterance, a hierarchical transformer-based context-aware style predictor with a mixture attention mask is designed, considering both text-side context information and speech-side style information of previous speeches. Based on this, we can generate long-form speech with coherent style and prosody sentence by sentence. Objective and subjective evaluations on a Mandarin audiobook dataset demonstrate that our proposed model can generate speech with more expressive and coherent speaking style than baselines, for both single-sentence and multi-sentence test.

