# TTS Papers - 2024-05-16

Total: 3

## Faces that Speak: Jointly Synthesising Talking Face and Speech from Text
- **Authors**: Youngjoon Jang et.al.
- **arXiv**: [2405.10272](https://arxiv.org/abs/2405.10272)
- **Tags**: other
- **Abstract**: Recent advances in deep learning have enabled the creation of natural-sounding synthesised speech. However, attackers have also utilised these tech-nologies to conduct attacks such as phishing. Numerous public datasets have been created to facilitate the development of effective detection models. How-ever, available datasets contain only entirely fake audio; therefore, detection models may miss attacks that replace a short section of the real audio with fake audio. In recognition of this problem, the current paper presents the RFP da-taset, which comprises five distinct audio types: partial fake (PF), audio with noise, voice conversion (VC), text-to-speech (TTS), and real. The data are then used to evaluate several detection models, revealing that the available detec-tion models incur a markedly higher equal error rate (EER) when detecting PF audio instead of entirely fake audio. The lowest EER recorded was 25.42%. Therefore, we believe that creators of detection models must seriously consid-er using datasets like RFP that include PF and other types of fake audio.

## Building a Luganda Text-to-Speech Model From Crowdsourced Data
- **Authors**: Sulaiman Kagumire et.al.
- **arXiv**: [2405.10211](https://arxiv.org/abs/2405.10211)
- **Tags**: synthesis
- **Abstract**: Automatic Speech Understanding (ASU) aims at human-like speech interpretation, providing nuanced intent, emotion, sentiment, and content understanding from speech and language (text) content conveyed in speech. Typically, training a robust ASU model relies heavily on acquiring large-scale, high-quality speech and associated transcriptions. However, it is often challenging to collect or use speech data for training ASU due to concerns such as privacy. To approach this setting of enabling ASU when speech (audio) modality is missing, we propose TI-ASU, using a pre-trained text-to-speech model to impute the missing speech. We report extensive experiments evaluating TI-ASU on various missing scales, both multi- and single-modality settings, and the use of LLMs. Our findings show that TI-ASU yields substantial benefits to improve ASU in scenarios where even up to 95% of training speech is missing. Moreover, we show that TI-ASU is adaptive to dropout training, improving model robustness in addressing missing speech during inference.

## Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model
- **Authors**: Siyang Wang et.al.
- **arXiv**: [2405.09768](https://arxiv.org/abs/2405.09768)
- **Tags**: multilingual, codec, llm-based, synthesis

