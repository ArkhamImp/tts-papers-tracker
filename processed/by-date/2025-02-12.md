# TTS Papers - 2025-02-12

Total: 1

## Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment
- **Authors**: Zuyan Liu et.al.
- **arXiv**: [2502.04328](https://arxiv.org/abs/2502.04328)
- **Tags**: multilingual
- **Abstract**: Text-to-Speech (TTS) synthesis faces the inherent challenge of producing multiple speech outputs with varying prosody given a single text input. While previous research has addressed this by predicting prosodic information from both text and speech, additional contextual information, such as video, remains under-utilized despite being available in many applications. This paper investigates the potential of integrating visual context to enhance prosody prediction. We propose a novel model, VisualSpeech, which incorporates visual and textual information for improving prosody generation in TTS. Empirical results indicate that incorporating visual features improves prosodic modeling, enhancing the expressiveness of the synthesized speech. Audio samples are available at https://ariameetgit.github.io/VISUALSPEECH-SAMPLES/.

