# TTS Papers - 2023-03-01

Total: 2

## DTW-SiameseNet: Dynamic Time Warped Siamese Network for Mispronunciation Detection and Correction
- **Authors**: Raviteja Anantha et.al.
- **arXiv**: [2303.00171](https://arxiv.org/abs/2303.00171)
- **Tags**: other
- **Abstract**: Recent expressive text to speech (TTS) models focus on synthesizing emotional speech, but some fine-grained styles such as intonation are neglected. In this paper, we propose QI-TTS which aims to better transfer and control intonation to further deliver the speaker's questioning intention while transferring emotion from reference speech. We propose a multi-style extractor to extract style embedding from two different levels. While the sentence level represents emotion, the final syllable level represents intonation. For fine-grained intonation control, we use relative attributes to represent intonation intensity at the syllable level.Experiments have validated the effectiveness of QI-TTS for improving intonation expressiveness in emotional speech synthesis.

## UzbekTagger: The rule-based POS tagger for Uzbek language
- **Authors**: Maksud Sharipov et.al.
- **arXiv**: [2301.12711](https://arxiv.org/abs/2301.12711)
- **Tags**: multilingual
- **Abstract**: In expressive speech synthesis it is widely adopted to use latent prosody representations to deal with variability of the data during training. Same text may correspond to various acoustic realizations, which is known as a one-to-many mapping problem in text-to-speech. Utterance, word, or phoneme-level representations are extracted from target signal in an auto-encoding setup, to complement phonetic input and simplify that mapping. This paper compares prosodic embeddings at different levels of granularity and examines their prediction from text. We show that utterance-level embeddings have insufficient capacity and phoneme-level tend to introduce instabilities when predicted from text. Word-level representations impose balance between capacity and predictability. As a result, we close the gap in naturalness by 90% between synthetic speech and recordings on LibriTTS dataset, without sacrificing intelligibility.

