# TTS Papers - 2025-06-14

Total: 3

## StreamMel: Real-Time Zero-shot Text-to-Speech via Interleaved Continuous Autoregressive Modeling
- **Authors**: Hui Wang et.al.
- **arXiv**: [2506.12570](https://arxiv.org/abs/2506.12570)
- **Tags**: zero-shot, streaming, synthesis

## Speech-Language Models with Decoupled Tokenizers and Multi-Token Prediction
- **Authors**: Xiaoran Fan et.al.
- **arXiv**: [2506.12537](https://arxiv.org/abs/2506.12537)
- **Tags**: multilingual
- **Abstract**: We develop a task-oriented spoken dialogue system (SDS) that regulates emotional speech based on contextual cues to enable more empathetic news conversations. Despite advancements in emotional text-to-speech (TTS) techniques, task-oriented emotional SDSs remain underexplored due to the compartmentalized nature of SDS and emotional TTS research, as well as the lack of standardized evaluation metrics for social goals. We address these challenges by developing an emotional SDS for news conversations that utilizes a large language model (LLM)-based sentiment analyzer to identify appropriate emotions and PromptTTS to synthesize context-appropriate emotional speech. We also propose subjective evaluation scale for emotional SDSs and judge the emotion regulation performance of the proposed and baseline systems. Experiments showed that our emotional SDS outperformed a baseline system in terms of the emotion regulation and engagement. These results suggest the critical role of speech emotion for more engaging conversations. All our source code is open-sourced at https://github.com/dhatchi711/espnet-emotional-news/tree/emo-sds/egs2/emo_news_sds/sds1

## Phonikud: Hebrew Grapheme-to-Phoneme Conversion for Real-Time Text-to-Speech
- **Authors**: Yakov Kolani et.al.
- **arXiv**: [2506.12311](https://arxiv.org/abs/2506.12311)
- **Tags**: streaming, synthesis
- **Abstract**: We present the first text-to-speech (TTS) system tailored to second language (L2) speakers. We use duration differences between American English tense (longer) and lax (shorter) vowels to create a "clarity mode" for Matcha-TTS. Our perception studies showed that French-L1, English-L2 listeners had fewer (at least 9.15%) transcription errors when using our clarity mode, and found it more encouraging and respectful than overall slowed down speech. Remarkably, listeners were not aware of these effects: despite the decreased word error rate in clarity mode, listeners still believed that slowing all target words was the most intelligible, suggesting that actual intelligibility does not correlate with perceived intelligibility. Additionally, we found that Whisper-ASR did not use the same cues as L2 speakers to differentiate difficult vowels and is not sufficient to assess the intelligibility of TTS systems for these individuals.

