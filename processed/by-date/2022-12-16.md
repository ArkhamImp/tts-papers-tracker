# TTS Papers - 2022-12-16

Total: 3

## Speech Aware Dialog System Technology Challenge (DSTC11)
- **Authors**: Hagen Soltau et.al.
- **arXiv**: [2212.08704](https://arxiv.org/abs/2212.08704)
- **Tags**: other
- **Abstract**: Text-to-speech synthesis (TTS) is a task to convert texts into speech. Two of the factors that have been driving TTS are the advancements of probabilistic models and latent representation learning. We propose a TTS method based on latent variable conversion using a diffusion probabilistic model and the variational autoencoder (VAE). In our TTS method, we use a waveform model based on VAE, a diffusion model that predicts the distribution of latent variables in the waveform model from texts, and an alignment model that learns alignments between the text and speech latent sequences. Our method integrates diffusion with VAE by modeling both mean and variance parameters with diffusion, where the target distribution is determined by approximation from VAE. This latent variable conversion framework potentially enables us to flexibly incorporate various latent feature extractors. Our experiments show that our method is robust to linguistic labels with poor orthography and alignment errors.

## Text-to-speech synthesis based on latent variable conversion using diffusion probabilistic model and variational autoencoder
- **Authors**: Yusuke Yasuda et.al.
- **arXiv**: [2212.08329](https://arxiv.org/abs/2212.08329)
- **Tags**: synthesis
- **Abstract**: We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to "reading") and from semantic tokens to low-level acoustic tokens ("speaking"). Decoupling these two tasks enables training of the "speaking" module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the "reading" component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker-id labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in terms of naturalness and acoustic quality, as measured in subjective tests.

## Investigation of Japanese PnG BERT language model in text-to-speech synthesis for pitch accent language
- **Authors**: Yusuke Yasuda et.al.
- **arXiv**: [2212.08321](https://arxiv.org/abs/2212.08321)
- **Tags**: multilingual, synthesis

